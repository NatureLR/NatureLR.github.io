[{"title":"ansible基本使用","url":"/2020/12/16/ansible%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"Ansible是一个自动化运维工具，可以实现批量配置，部署，命令等功能\n\n\n安装yum 安装\nyum -y install ansible\n\npip安装\npip install ansible\n\n配置路径默认读取/etc/ansible/目录下的主机清单和规则\n主机清单\n记录ansible需要执行操作的目标机器文件，默认读取/etc/ansible/hosts，一般通过 -i参数指定,也可以分类写到一个文件夹下\n\n\n#开头为注释\n忽略空行\n组由[组名]定义\n主机名和域名都可以\n一个ip或域名可以是组的成员\n没有分组的主机写在任意的一个组的前面\n\n连续IP# 等价于 192.168.1.1 192.168.1.2 192.168.1.2 192.168.1.3 192.168.1.4等等\n192.168.1.[1:4]\n\n# 等价于 server1.example.com server2.example.com server3.example.com等等\nserver[1:3].example.com\n\n参数192.168.1.1 ansible_ssh_user=root ansible_ssh_pass=root\n\n常用参数\nansible_ssh_host              目标主机地址\nansible_ssh_port              目标主机端口，默认22\nansible_ssh_user              目标主机用户\nansible_ssh_pass              目标主机ssh密码\nansible_sudo_pass             sudo密码\nansible_sudo_exe\nansible_connection            与主机的连接类型，比如：local,ssh或者paramiko\nansible_ssh_private_key_file  私钥地址\nansible_shell_type            目标系统的shell类型\nansible_python_interpreter    python版本\n\n别名test1 ansible_ssh_port=22 ansible_ssh_host=192.168.1.1 ansible_ssh_user=root  　　# 别名test1\n\n主机组[foo]\n192.168.1.1\n192.168.2.1\n\n主机组嵌套[db]\n192.168.1.1\n\n[server]\n192.168.2.1\n\n[all:children]\ndb\nserver\n\n主机组参数[test]\nname1 ansible_ssh_host=192.168.1.[1:3]\n\n[test:vars]\nansible_ssh_user=root\nansible_ssh_pass=\"root\"\ntestvar=\"test\"\n\n模块\nansible的功能都是通过模块来完成的 ansible-doc -s &lt;模块名&gt;查看模块的参数 ansible-doc -l 查看所有模块\n\n常用模块\n参数中的free_form是各个模块的命令或args，并不是一个存在的参数\n\ncommand\n在目标主机上执行命令\n\n\n参数：\nfree_form              必选表在目标机器上执行的命令\nchdir                  在目标主机的哪里执行命令\ncreates                文件存在时就不执行此命令\nremoves                和creates相反存在时就执行\n\n\n例子：ansible test -m command -a “chdir&#x3D;&#x2F;var&#x2F;log removes&#x3D;kern.log cat kern.log” &#x2F;var&#x2F;log下kern.log存在就查看kern.log\n\nshell\n和command一样不过command不支持重定向等管道操作，shell会调用/bin/sh执行\n\n\n参数：  \nfree_form:             执行的命令\nchdir:                 改变运行执行的目录\ncreates:               文件存在则不就不执行命令\nexecutable:            改变命令说用的shell解释器，默认为&#x2F;bin&#x2F;sh\nremoves:               和creates相反存在时就执行\n\n\n例子：ansible  test -m shell -a “cat &#x2F;etc&#x2F;hosts”\n\nscript\n在目标主机上执行本地主机的脚本\n\n\n参数：\nfree_form:             需要执行的脚本路径\nchdir:                 执行脚本的目录\ncreates:               目标机器的文件存在则不执行\nremoves:               目标机器的文件存在则不执行\n\n\n例子： ansible test -m script -a “test.sh”\n\ncopy\n复制本地文件或文件夹到目标主机上\n\n\n参数：\nsrc：                   指定需要copy的文件或目录\ndest：                  文件将被拷贝到目标主机的哪个目录中，dest为必须参数\ncontent                 不适用src时用此参数写入内容\nforce:                  目标主机路径已经有文件且内容不相同时是否覆盖\nbackup:                 目标主机已经有文件且内容不同时是否备份\nowner:                  拷贝到目标主机后的所有者\ngroup:                  拷贝到目标主机后的属组\nmode:                   拷贝到目标主机后的权限\n\n\n例子： ansible test -m copy -a “src&#x3D;&#x2F;root&#x2F;test.sh dest&#x3D;&#x2F;tmp”\n\nfile\n对目标主机的文件管理\n\n\n参数：\npath：                  指定目标目录或文件\nstate：\ndirectory：           如果目录不存在，创建目录\nfile：                即使文件不存在，也不会被创建\nlink：                创建软连接\nhard：                创建硬连接\ntouch：               如果文件不存在，则会创建一个新的文件，如果文件或目录已存在，则更新其最后修改时\nabsent：              删除目录、文件或者取消链接文\n\n\nsrc：                   当state设置为link或者hard时，需要操作的源文件\nforce:                  需要在两种情况下强制创建软连接，一种是源文件不存在但之后会建立的情况下；另一种是目标连接已存在，需要先取消之前的软连接，有两个选项：yes|no\nowner：                 指定被操作文件的所有者，\ngroup：                 指定被操作文件的所属组\nmode：                  权限\nrecurse：               文件为目录时，递归目录\n\n\n例子：\n设置权限为777所属组为minikube所有者为：ansible test -m file -a “path&#x3D;&#x2F;tmp&#x2F;test.sh  mode&#x3D;777 owner&#x3D;minikube group&#x3D;minikube”\n创建/etc/hosts的软连接到home目录：ansible test -m file -a “path&#x3D;&#x2F;root&#x2F;hosts  src&#x3D;&#x2F;etc&#x2F;hosts state&#x3D;link”\n\n\n\nblockinfile\n在指定的文件里修改一段文本\n\n\n参数：\npath：                 必须参数，指定要操作的文件\nblock：                指定要操作的一段文本\nmarker：               ansibel默认修改时会添加一个以#开头标记，可以改为自定义\nstate:                 present为插入或者更新;absent删除\ninsertafter：          默认会将文本插入到指定的位置的后面\ninsertbefore：         默认会将文本插入到指定的位置的前面\nbackup：               是否在修改文件之前对文件进行备份。\ncreate：               当要操作的文件并不存在时，是否创建对应的文件。\n\n\n例子：\n在目标主机的&#x2F;tmp&#x2F;test文件中插入ansible-test且标记内容为teststart：ansible localhost -m blockinfile -a “path&#x3D;&#x2F;tmp&#x2F;test block&#x3D;ansible-test marker&#x3D;’#{mark}teststart’”\n\n\n\nlineinfile\n和blockinfile相似不过是一行还可以使用正则表达式\n\n\n参数：\npath：                  必须参数，指定要操作的文件\nline:                   要指定的文本内容\nregexp：                正则匹配对应的行，当替换文本时，如果有多行文本都能被匹配，则只有最后面被匹配到的那行文本才会被替换，当删除文本时，如果有多行文本都能被匹配，这么这些行都会被删除\nstate：                 absent为删除，state的默认值为present\nbackrefs：              在使用正则匹配时如果没有匹配到默认会在文件的末尾插入要替换的文本，设置为yes则不会\ninsertafter：           默认会将文本插入到指定的位置的后面\ninsertbefore：          默认会将文本插入到指定的位置的前面\nbackup：                是否在修改文件之前对文件进行备份\ncreate：                当要操作的文件并不存在时，是否创建对应的文件-例子：\n将&#x2F;tmp&#x2F;test的文件中#kern开头行换成kern.* &#x2F;var&#x2F;log&#x2F;kern.log:ansible localhost -m lineinfile -a ‘path&#x3D;&#x2F;tmp&#x2F;test regexp&#x3D;”^#kern” line&#x3D;”kern.* &#x2F;var&#x2F;log&#x2F;kern.log”‘\n\n\n\nreplace\n文本替换模块\n\n\n参数：\npath：                 必须参数，指定要操作的文件，2.3版本之前，只能使用dest, destfile, name指定要操作的文件，2.4版本中，仍然可以使用这些参数名，这些参数名作为path参数的别名使用。\nregexp:                必须参数，指定一个python正则表达式，文件中与正则匹配的字符串将会被替换。\nreplace：              指定最终要替换成的字符串。\nbackup：               是否在修改文件之前对文件进行备份，最好设置为yes。\n\n\n例子：将&#x2F;etc&#x2F;test文件中所有的localhost换成FOO: ansible localhost -m replace -a ‘path&#x3D;&#x2F;tmp&#x2F;test  regexp&#x3D;”localhost” replace&#x3D;foo’\n\nsystemd\n运行systemd相关的命令\n\n\n参数：\nenabled:               是否设置为开机启动\nname:                  systemd模块名字\nstate:                 想要设置的状态，比如restartd重启started启动、stopped停止、reloaded重新加载\ndaemon_reload:         运行daemon-reload命令\ndaemon_reexec:         运行daemon_reexec命令\n\n\n例子：ansible test -m systemd -a “name&#x3D;rsyslog state&#x3D;restarted”\n\nyum\nyum包管理\n\n\n参数：\naction: yum\nconf_file              yum的配置文件\ndisable_gpg_check      关闭gpg_check\ndisablerepo            不启用某个源\nenablerepo             启用某个源\nname                   指定要安装的包，如果有多个版本需要指定版本，否则安装最新的包\nstate                  安装:present，安装最新版:latest，卸载程序包:absent\n\n\n例子: 安装最新版psree命令：ansible localhost -m yum -a “name&#x3D;psmisc state&#x3D;latest”\n\ncron\n定时模块\n\n\n参数：\nbackup                 如果设置，创建一个crontab备份\ncron_file              如果指定, 使用这个文件cron.d，而不是单个用户crontab\nday                    日应该运行的工作( 1-31, *, *&#x2F;2, etc )\nhour                   小时 ( 0-23, *, *&#x2F;2, etc )\njob                    指明运行的命令是什么\nminute                 分钟( 0-59, *, *&#x2F;2, etc )\nmonth                  月( 1-12, *, *&#x2F;2, etc )\nname                   定时任务描述\nreboot                 任务在重启时运行，不建议使用，建议使用special_time\nspecial_time           特殊的时间范围，参数：reboot（重启时）,annually（每年）,monthly（每月）,weekly（每周）,daily（每天）,hourly（每小时）\nstate                  指定状态，默认prsent添加定时任务，absent删除定时任务\nuser                   以哪个用户的身份执行\nweekday                周 ( 0-6 for Sunday-Saturday, *, etc )\n\n\n例子：\n每天8点半执行cat &#x2F;etc&#x2F;hosts这个命令：ansible localhost -m cron -a “name&#x3D;test minute&#x3D;30 hour&#x3D;8 day&#x3D;* job&#x3D;’cat &#x2F;etc&#x2F;hosts’”\n删除test这个cronjob：ansible localhost -m cron -a “name&#x3D;test state&#x3D;absent”\n重启时rm -rf &#x2F;tmp命令： ansible test -m cron -a ‘name&#x3D;”test” special_time&#x3D;reboot job&#x3D;”rm -rf &#x2F;tmp”‘\n\n\n\n执行命令# ansible &lt;主机> -m &lt;模块> -a &lt;模块参数>\nansible &lt;主机> -m shell -a \"cat /etc/hosts\"\n\n指定某些机器执行ansbile &lt;主机组&gt; -m &lt;模块&gt; -a &lt;参数&gt; –limit &lt;主机&gt;  指定执行主机ansbile &lt;主机组&gt; -m &lt;模块&gt; -a &lt;参数&gt; –limit &lt;!主机&gt; 排除执行的主机ansbile &lt;主机组&gt; -m &lt;模块&gt; -a &lt;参数&gt; –limit &lt;主机1：主机2&gt; 只在主机1和主机2中执行\n一步一步的执行且确认在执行 剧本的时候加上 –step，每执行一个任务就询问一次\nansible-playbook  -i inventories test.yaml --step\n\nplaybook\n剧本就是一系列ansible命令组合类似shell脚本和shell命令\n\n一个将内核日志输出到&#x2F;var&#x2F;log&#x2F;kern.log的剧本\n---\n- hosts: all                 # 要执行的主机组\n  tasks:\n  - name: 修改rsyslog配置文件 # 任务名字\n    tags: rsyslog            # 任务标签\n    lineinfile:              # 任务模块\n       dest: /etc/rsyslog.conf\n       regexp: \"&#123;&#123; item.regexp &#125;&#125;\"\n       line: \"&#123;&#123; item.line &#125;&#125;\"\n    with_items:              # 循环执行\n     - &#123; regexp: '^#kern',line: 'kern.* /var/log/kern.log' &#125;\n     - &#123; regexp: '^#\\$ModLoad imklog',line: '$ModLoad imklog' &#125;\n     - &#123; regexp: '^#\\$ModLoad imjournal',line: '$ModLoad imjournal' &#125;\n  - name: 修改logrotate的syslog配置\n    shell: sed -i '1i\\\\/var\\/log\\/kern.log' /etc/logrotate.d/syslog\n    tags: logrotate\n  - name: 重启rsyslog服务\n    tags: rsyslog\n    systemd:\n      name: rsyslog\n      state: restarted\n      enabled: yes\n\ntags\n标签可以灵活的选择执行那些task或其他的对象\n\n特殊的标签：\n\nalways 当把标签设置为always即使使用–tags指定tags任务也会执行，可以使用–skip-tags always跳过\nnever  和always相反即使用–tags指定也不会执行\ntagged 只执行有标签的任务\nuntagged 只执行未打标签的含有always也会执行\nall 所有都执行不用指定\n\n---\n- hosts: test\n  remote_user: root\n  tasks:\n    - name: 创建文件test1\n      tags: test1\n      file:\n        path: /tmp/test1\n        state: touch\n    - name: 创建文件test2\n      tags: always\n      file:\n        path: /tmp/test2\n        state: touch\n    - name: 创建文件test3\n      file:\n        path: /tmp/test3\n        state: touch\n\n变量\n变量非常常用\n\n---\n- hosts: test\n  remote_user: root\n  vars:\n    path: /tmp/\n  tasks:\n    - name: 创建文件test1\n      tags: test1\n      file:\n        path: \"&#123;&#123; path &#125;&#125;test1\"\n        state: touch\n\nDEBUG\n调试打印\n\n---\n- hosts: test\n  remote_user: root\n  vars:\n    path: /tmp/\n  tasks:\n    - name: 创建文件test1\n      tags: test1\n      file:\n        path: \"&#123;&#123; path &#125;&#125;test1\"\n        state: touch\n    - name: print var\n      debug:\n        var: path\n    - name: msg\n      debug:\n        msg: this is debug info,The test file has been touched\n\n注册变量\n将模块运行的返回值进行赋值\n\n---\n- hosts: test\n  remote_user: root\n  tasks:\n  - name: test shell\n    shell: \"cat /etc/hosts\"\n    register: testvar\n  - name: shell 模块返回值\n    debug:\n      var: testvar\n  - name: 指定shell模块的返回\n    debug:\n      msg: \"&#123;&#123;testvar.stdout&#125;&#125;\"\n\n交互\n命令行交互输入的变脸类似c语言的scan函数\n\n---\n- hosts: test\n  remote_user: root\n  vars_prompt:\n    - name: \"user\"\n      prompt: \"请选择帐号 \\n\n      root \\n\n      poweruser \\n\n      test \\n\n      \"\n      private: no # 输入的字符显示出来\n      default: root # 默认值\n    - name: \"passwd\"\n      prompt: \"请输入密码\"\n  tasks:\n   - name: 输出变量\n     debug:\n      msg: 你的帐号：&#123;&#123;user&#125;&#125;;你的密码：&#123;&#123;passwd&#125;&#125;\n\n命令行传值\n通过命令行输入变量的值\n\n---\n- hosts: test\n  remote_user: root\n  vars:\n    var2: default\n  tasks:\n  - name: \"通过命令行传值\"\n    debug:\n      msg: var的值是：&#123;&#123;var&#125;&#125;;var2的值： &#123;&#123;var2&#125;&#125;\n\n将上面的yaml保存为test.yaml然后执行 ansible-playbook -i inventories test.yaml --extra-vars &quot;var=test&quot; -e &quot;var2=test2&quot;,-e是–extra-vars的缩写，命令行的值会覆盖默认值还可以使用@传变量文件\n主机(组)变量\n为每个主机（组）设置的变量，当使用次主机时变量生效\n\n[test]\n\nlocalhost hostvar1=test_host_var hostvar2=host_var_test\n\n[test:vars]\ngroupvar=testgroupvar\n\n---\n- hosts: test\n  remote_user: root\n  tasks:\n  - name: \"主机变量\"\n    debug:\n      msg: hostvar1的值是：&#123;&#123;hostvar1&#125;&#125;;hostvar2的值： &#123;&#123;hostvar2&#125;&#125;\n  - name: \"主机组变量\"\n    debug:\n      msg: test组的变量是：&#123;&#123;groupvar&#125;&#125;\n\nset_fact定义变量\n通过set_fact模块配置变量\n\n---\n- hosts: test\n  remote_user: root\n  tasks:\n  - set_fact:\n      testvasr: testvas\n  - name: 打印值\n    debug:\n      msg: \"&#123;&#123;testvasr&#125;&#125;\"\n\n内置变量\nansible有一些保留变量\n\n\nansible_version\nhostvars\ninventory_hostname\ninventory_hostname_short\nplay_hosts\ngroups\ngroup_names\n\n循环with_items\n以条目为单位循环with_items下的的元素\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name: 循环打印变量\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_items:\n     - 1\n     - 2\n     - 3\n  - name: 循环打印kv变量\n    debug:\n      msg: \"k的值是：&#123;&#123;item.k&#125;&#125;:v的值是：&#123;&#123;item.v&#125;&#125;\"\n    with_items:\n      - &#123; k: 1, v: 2 &#125;\n      - &#123; k: 3, v: 4 &#125;\n\nwith_list\n以list为单位循环元素,也即是一次性答应出一个list而不是list里面的元素\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name: 循环打印变量\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_items:\n      - [1,2,3]\n      - [a,b]\n  - name : 循环打印list\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_items:\n      - [1,2,3]\n      - [a,b]\n\nwith_flattened\n和with_item很像将item一个元素一个元素的打印出来\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name: 循环打印item\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_items:\n      - [1,2,3]\n      - [a,b,c]\n  - name : 循环打印list\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_list:\n      - [1,2,3]\n      - [a,b,c]\n  - name : 循环打印flattened\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_flattened:\n      - [1,2,3]\n      - [a,b,c]\n\nwith_together\n将list的元素纵排打印\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name: 循环打印item\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_items:\n      - [1,2,3]\n      - [a,b,c]\n  - name : 循环打印together\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_together:\n      - [1,2,3]\n      - [a,b,c]\n\n\nwith_cartesian\n将list的元素交叉打印出来\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name : 循环打印with_cartesian\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_cartesian:\n      - [1,2,3]\n      - [a,b,c]\n\nwith_indexed_items\n将所有list变成一个大的list然后将这个大的list按顺序从0开始添加索引和值\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name : 循环打印with_indexed_items\n    debug:\n      msg: \"&#123;&#123;item&#125;&#125;\"\n    with_indexed_items:\n      - [1,2,3]\n      - [a,b,c]\n\nwith_sequence\n类似golang序言的for循环，定义步长开始值，结束值\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name : with_sequence\n    debug: \n      msg: \"&#123;&#123; item &#125;&#125;\"\n    with_sequence: start=1 end=10 stride=2\n\nwith_dict\n顾名思义是循环处理字典的\n\n- hosts: test\n  gather_facts: false\n  vars:\n    account:\n      user: root\n      passwd: 123456\n  tasks:\n  - name : with_dict\n    debug: \n      msg: \"键是：&#123;&#123; item.key &#125;&#125; ；值是：&#123;&#123; item.value &#125;&#125;\"\n    with_dict: \"&#123;&#123;account&#125;&#125;\"\n\nwith_subelements\n也是对字典镜像操作，制定的字段作为key把其他字段作为value\n\n- hosts: test\n  gather_facts: false\n  vars:\n    account:\n      root:\n        user: root\n        passwd: 123456\n        open: \n          - tmp\n          - server\n      test:\n        user: test\n        passwd: abc\n        open: \n          - hosts\n          - log\n  tasks:\n  - name : with_sequence\n    debug: \n      msg: \"&#123;&#123; item &#125;&#125;\"\n    with_subelements: \n      - \"&#123;&#123;account&#125;&#125;\"\n      - open\n\nwith_file\n按行读取一个文件的内容\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name : with_file\n    debug: \n      msg: \"&#123;&#123; item &#125;&#125;\"\n    with_file: \n      - /etc/hosts\n\nwith_fileglob\n读取文件名字\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name : with_fileglob\n    debug: \n      msg: \"&#123;&#123; item &#125;&#125;\"\n    with_fileglob: \n      - /etc/*\n\n判断比较符\n&#x3D;&#x3D; 相等\n!&#x3D; 不等\n&gt; 大于\n&lt;  小于\n&gt;&#x3D;小于等于\n&lt;&#x3D;大于等于\n\n逻辑运算符\nand 与\nor 或\nnot 非\n() 组合\n\n- hosts: test\n  gather_facts: false\n  tasks:\n  - name : 判断\n    debug: \n      msg: \"&#123;&#123; item &#125;&#125;\"\n    with_items:\n      - 1\n      - 2\n      - 3\n    when: item > 2\n\n文件判断\nis exists 如果文件存在则为真\nis not exists  如果文件不存在则为假\nnot &lt;path&gt; is exists 和is not相同\n\n- hosts: test\n  #gather_facts: false\n  vars:\n    testpath: /tmp/test\n  tasks:\n  - name: 判断/tmp/test文件是否存在\n    debug: \n      msg: 是centos\n    when:  testpath is not exists\n\n变量判断\nis defined 定义则为真\nis undefined 没定义则为真\nis none 为空则为真\n\n- hosts: test\n  #gather_facts: false\n  vars:\n    testpath: /tmp/test\n  tasks:\n  - name: 判断testpath是否存在\n    debug: \n      msg: testpath文件存在\n    when: testpath is defined\n\n执行结果判断\nsuccess 或 succeeded 执行成功则返回真\nfailure 或 failed    执行失败则返回真\nchange 或 changed    执行状态为changed则返回真\nskip 或 skipped      没有满足条件，而被跳过执行时，则返回真\n\n- hosts: test\n  #gather_facts: false\n  tasks:\n  - shell: cat /etc/hosts\n    register: ret\n  - debug:\n      msg: \"执行成功\"\n    when: ret is success\n\n文件类型判断\nfile 是文件则为真\ndirectory 是目录则为真\nlink 软连接则为真\nmount 挂载点则为真\nexists 存在则为真\n\n- hosts: test\n  #gather_facts: false\n  vars:\n    path: /etc/hosts\n  tasks:\n  - debug:\n      msg: \"&#123;&#123; path &#125;&#125; 是个文件\" \n    when: path is file\n\n字符串判断\nlower 字符为小写则为真\nupper 字符为大写则为真\n\n- hosts: test\n  #gather_facts: false\n  vars:\n    path: TEST\n  tasks:\n  - debug:\n      msg: \"&#123;&#123; path &#125;&#125; 是大写\" \n    when: path is upper\n\n整除判断\neven  偶数为真\nodd  奇数为真\ndivisibleby(num) 整除则为真\n\n- hosts: test\n  gather_facts: false\n  vars:\n    X: 5\n    Y: 6\n    Z: 66\n  tasks:\n  - debug:\n      msg: \"&#123;&#123; X &#125;&#125; 是奇数\" \n    when: X is odd\n  - debug:\n      msg: \"&#123;&#123; Y &#125;&#125; 是偶数\" \n    when: Y is even\n  - debug:\n      msg: \"&#123;&#123; Z &#125;&#125; 能被66整除\" \n    when: Z is divisibleby(66)\n\n其他判断\nversion 判断版本大小\nstring 是字符则为真\nnumber 是数字则为真\nsubset 一个list是另一个list的子集则为真\nsuperset 一个list不是另一个list的子集则为真\n\n- hosts: test\n  gather_facts: false\n  vars:\n    Versions: 1.2.4\n    A:\n    - 2\n    - 3\n    B: [1,2,3,4,5]\n    str: test\n    str2: 2\n  tasks:\n  - debug:\n      msg: \"&#123;&#123; Versions &#125;&#125; 版本是否大于1.2.1\" \n    when: Versions is version(\"1.2.1\",\">\")\n  - debug:\n      msg: b是a的子集\n    when: B is subset(A)\n  - debug:\n      msg: str是字符串\n    when: str is string\n  - debug:\n      msg: str2 是数字\n    when: str2 is number\n\nhandler\n在上面的例子中无论前面修改配置文件是否修改都会执行rsyslog重启，这样有些不妥 handler的执行顺序与被notify的顺序无关\n\n# 这样只有配置文件真正被修改了才会执行重启\n---\n- hosts: test\n  remote_user: root\n  tasks:\n    - name: 修改rsyslog配置文件\n      tags: rsyslog\n      lineinfile:\n         dest: /etc/rsyslog.conf\n         regexp: ^#kern\n         line: kern.* /var/log/kern.log\n      notify:                # 引用handlers\n        重启rsyslog服务\n  handlers:                  # 和tasks同级\n    - name: 重启rsyslog服务\n      systemd:\n         name: rsyslog\n         state: restarted\n         enabled: yes\n\n\nmeta关键字可以让notify之后立刻执行handlers\n\n---\n- hosts: test\n  remote_user: root\n  tasks:\n    - name: 修改rsyslog配置文件\n      tags: rsyslog\n      lineinfile:\n         dest: /etc/rsyslog.conf\n         regexp: ^#kern\n         line: kern.* /var/log/kern.log\n      notify:\n        重启rsyslog服务\n    - meta: flush_handlers\n    \n    - name: 查看配置文件状态\n      shell: cat /etc/rsyslog.conf |grep \"kern.\\*\"\n      register: ps\n    - debug: msg=&#123;&#123; ps.stdout &#125;&#125;\n\n  handlers:\n    - name: 重启rsyslog服务\n      systemd:\n         name: rsyslog\n         state: restarted\n         enabled: yes\n\n\nlisten handlers组\n\n---\n- hosts: test\n  remote_user: root\n  tasks:\n    - name: 修改rsyslog配置文件\n      tags: rsyslog\n      lineinfile:\n         dest: /etc/rsyslog.conf\n         regexp: ^#kern\n         line: kern.* /var/log/kern.log\n      notify:\n         handler group1 # 通知了handler group1\n    - meta: flush_handlers \n\n    - name: 查看配置文件状态\n      shell: cat /etc/rsyslog.conf |grep \"kern.\\*\"\n      register: ps\n    - debug: msg=&#123;&#123; ps.stdout &#125;&#125;\n\n  handlers:\n    - name: 重启rsyslog服务\n      listen: handler group1\n      systemd:\n        name: rsyslog\n        state: restarted\n        enabled: yes\n    - name: 创建测试文件\n      listen: handler group1\n      file:\n        path: /tmp/test\n        state: touch\n\ninclude &amp;&amp; import tasks\n当task越来越多的时候如果都在一个文件不是很好管理，将一些相关性很强的写到一个文件然后引用另外的yaml文件 import_tasks静态的，在playbook解析阶段将所有文件中的变量读取加载 include_tasks动态则是在执行playbook之前才会加载自己变量\n\n---\n- hosts: test\n  remote_user: root\n  tasks:\n    - name: 修改rsyslog配置文件\n      tags: rsyslog\n      lineinfile:\n         dest: /etc/rsyslog.conf\n         regexp: ^#kern\n         line: kern.* /var/log/kern.log\n    - name: 查看配置文件状态\n      import_tasks: config.yaml\n---\n# config.yaml\n- name: 查看配置文件状态\n  shell: cat /etc/rsyslog.conf |grep \"kern.\\*\"\n  register: ps\n- debug: msg=&#123;&#123; ps.stdout &#125;&#125;\n\n模版\n有些时候应用的配置文件会根据部署的机器调整一些参数，但是大部分参数不需要调整额这个时候就需要模版来处理 ansibel的template模块使用python的jinja2模版引擎\n\n\n参数:\nowner  在目标主机上通过模版生成的文件的所属者\ngroup  在目标主机上通过模版生成的文件的所属组\nmode   在目标主机上通过模版生成的文件的权限\nforce  目标主机上已经有了文件是否强制覆盖\nbackup 目标主机上已经有了文件是否覆盖\n\n\n\n占位符\n&lt;!–swig￼52–&gt; 表达式，比如变量、运算表达式、比较表达式等写法参考比较符\n&lt;!–swig￼53–&gt; 控制语句,如if控制结构，for循环控制结构\n注释，模板文件被渲染后，注释不会包含在最终生成的文件中\n\n例子模版文件：\n基本变量替换\n&#123;&#123; testvar &#125;&#125;\n\n逻辑计算\n\n&#123;&#123; number &gt; 1 &#125;&#125;\n\n&#123;&#123; number is defined &#125;&#125;\n\n&#123;&#123; &#39;&#x2F;tmp&#39; is exists  &#125;&#125;\n\n数据结构取值\n&#123;&#123; account.user &#125;&#125;\n&#123;&#123; numbers.1 &#125;&#125;\n\n&#123;# 这个是注释 #&#125;\n&#123;# \n这个是多行注释，\n不会在生成的末班中显示\n#&#125;\n\n判断\n&#123;% if numbers.1&gt;1 %&#125;\n嘿嘿嘿\n&#123;% endif %&#125;\n\n循环\n&#123;% for i in [3,14,15,9,26] -%&#125;\n&#123;&#123; i &#125;&#125; &#123;&#123;&#39; &#39;&#125;&#125;\n&#123;%- endfor %&#125;\n\nplaybook如下：\n- hosts: test\n  gather_facts: false\n  vars:\n    testvar: footest\n    number: 3\n    account:\n        user: root\n        passwd: 123456\n    numbers:\n      - 1\n      - 2\n      - 3\n  tasks:\n  - debug:\n      msg: \"test\" \n  - template:\n      src: test.j2\n      dest: /tmp/test\n\n执行 ansible-playbook  -i inventories test.yaml\n角色\n当任务越来越多时一个文件放这么多有些不一样配合include和import分门别类的存放ansible资源文件\n\n目录结构\n每个目录下都有an.yaml用语导入此目录其他的yaml\n\n.\n├── inventories\n├── roles\n│   ├── defaults\n│   ├── files       # 文件的目录\n│   ├── handlers    # handler目录\n│   ├── meta        # 特殊设定及其依赖关系\n│   ├── tasks       # 存放task的目录\n│   ├── templates   # 存放模版\n│   └── vars        # 存放变量\n└── test.yaml       # 剧本文件\n\n角色例子在和roles文件同级目录创建剧本\n---\n- hosts: test\n  gather_facts: false\n  roles:\n    - role: roles\n\n\n在roles文件夹的task中创建task\n---\n- debug: \n    msg: hahaha\n\n\n运行ansible-playbook  -i inventories test.yaml\n过滤器\n对一些数据进行处理\n\n---\n- hosts: test\n  remote_user: root\n  vars:\n    testvar: haha\n  tasks:\n  - debug:\n      msg: \"&#123;&#123; testvar | upper &#125;&#125;\"    # 将全部字母转换成大写\n\n基本格式就像上面一样其他功能只需要将upper替换为其他的字段即可\n常用过滤器\n\nupper 字符转换为大写\nlower 字符转换为小写\nindent 设置缩进\njson_query 将字符串作为json\ndirname 路径字符串的路径名\nhash() 进行hash处理\npassword_hash 密码专用的hash\nchecksum  计算md5\nipaddr() 需要安装netaddr，针对ip地址处理\n\nlookup\n上面的过滤器和lookup其实都是插件具体介绍:https://docs.ansible.com/ansible/latest/plugins/plugins.html\n\n---\n- name: 渲染模版到变量\n  set_fact:\n    yaml: '&#123;&#123; lookup(\"template\", \"test.j2\") &#125;&#125;'\n- name: 读取文件到变量\n  set_fact:\n    file: '&#123;&#123; lookup(\"file\", \"/etc/hosts\") &#125;&#125;'\n\n在本地&amp;&amp;只执行一次\n有些时候一些剧本在本地执行，就像本地执行shell一样,由于在本地执行所以需要搭配run_once\n\n- hosts: localhost\n  connection: local\n  remote_user: root\n  run_once: true\n  gather_facts: no\n  tasks:\n     - shell: echo \"test\" > /tmp/test\n\n并行执行\n在机器比较多时一台一台执行太慢serial可以指定并行执行的数量\n\n- hosts: test\n  remote_user: root\n  serial: 1\n  #serial: 20% 按百分比\n  gather_facts: no\n  tasks:\n     - shell: echo \"test\" > /tmp/test\n\n参考资料http://www.zsythink.net/archives/category/%e8%bf%90%e7%bb%b4%e7%9b%b8%e5%85%b3/ansible/https://docs.ansible.com/http://www.ansible.com.cn/\n","tags":["ansible"]},{"title":"Apache Guacamole","url":"/2020/10/29/apache-Guacamole/","content":"Apache Guacamole是一个基于web的远程终端支持ssh,vnc,rdp等协议\n\n\n架构图如下官网地址：http://guacamole.apache.org\n\n\n从图中可看出分为guacamole服务和guacd服务，guacd服务负责连接远程的vpc，rdp，ssh等服务器\n\n安装部署这里使用k8s部署，注意本安装仅用于测试使用，由于mysql没做持久化重启之后数据会丢失\n部署guacamolecat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guacamole\nspec:\n  selector:\n    matchLabels:\n      app: guacamole\n  template:\n    metadata:\n      labels:\n        app: guacamole\n    spec:\n      containers:\n      - env:\n        - name: GUACD_HOSTNAME # guacd地址\n          value: guacamole-guacd\n        - name: MYSQL_DATABASE # mysql数据库\n          value: guacamole\n        - name: MYSQL_HOSTNAME # mysql地址\n          value: guacamole-mysql\n        - name: MYSQL_PASSWORD # mysql密码\n          value: root\n        - name: MYSQL_USER # mysql用户\n          value: root\n        image: guacamole/guacamole:latest # 这里使用了最新版\n        name: guacamole\n        ports:\n        - containerPort: 8080\n          name: 8080tcp02\n          protocol: TCP\n        resources: &#123;&#125;\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: guacamole\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: guacamole\n  type: NodePort # 使用nodeport进行访问，也可以用ingress\nEOF\n\n部署guacdcat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guacamole-guacd\nspec:\n  selector:\n    matchLabels:\n      app: guacamole-guacd\n  template:\n    metadata:\n      labels:\n        app: guacamole-guacd\n    spec:\n      containers:\n      - name: guacamole-guacd\n        image: guacamole/guacd:latest\n        resources: &#123;&#125;\n        ports:\n        - containerPort: 4822\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: guacamole-guacd\nspec:\n  selector:\n    app: guacamole-guacd\n  ports:\n  - port: 4822\n    targetPort: 4822\nEOF\n\n部署mysql\nmysql可以使用已经有的,且以下资源未做持久化重启之后数据会丢失不要用于生产！！！\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guacamole-mysql\nspec:\n  selector:\n    matchLabels:\n      app: guacamole-mysql\n  template:\n    metadata:\n      labels:\n        app: guacamole-mysql\n    spec:\n      containers:\n      - name: guacamole-mysql\n        image: mysql:latest\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: root\n        resources: &#123;&#125;\n        ports:\n        - containerPort: 3306\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: guacamole-mysql\nspec:\n  selector:\n    app: guacamole-mysql\n  ports:\n  - port: 3306\n    targetPort: 3306\nEOF\n\n初始化mysql\n将guacamole的Entrypoint改为sleep 1h以方便进入容器\n\n容器里执行/opt/guacamole/bin/initdb.sh --mysql &gt; initdb.sql 导出mysql的表结构\n\napt update &amp;&amp; apt install mysql-client安装mysql客户端\n\nmysql -h guacamole-mysql -uroot -proot登录mysql数据库\n\n如果出现 ERROR 2059 (HY000): Authentication plugin ‘caching_sha2_password’ cannot be loaded 错误则需要在guacamole-mysql容器里登录数据库执行 ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;root&#39;;\n\n\ncreate database guacamole; 创建数据库\n\nuse guacamole; 进入数据库， source initdb.sql导入表结构\n\n\n登录\n因为是nodeport所有可以使用 &lt;nodePort&gt;&#x2F;guacamole\n默认账号密码为guacadmin&#x2F;guacadmin\n\n添加链接\n进入配置界面配置根据目标主机的情况填写\n\n这个时候首页就出现了可以连接的机器，点击即可连接此机器\n\nCtrl + Alt + Shift 可以打开控制面板，复制文件也可以直接拖进去\n","tags":["终端"]},{"title":"argocd","url":"/2022/03/04/argocd/","content":"argocd 是个有可视化界面的git-ops工具\n\n\n\nargocd是一个gitops工具，可以将git上的文件同步到k8s集群，且支持多集群，这样我只需要修改git上的内容就可以完成发布\n\n安装\n单节点安装\n\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.2.5/manifests/install.yaml\n\n\n高可用安装\n\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.2.5/manifests/ha/install.yaml\n\n登录界面argocd默认安装没有用nodePort，我们需要手动将argocd-server改为nodePort\nkubectl patch svc argocd-server -p '&#123;\"spec\":&#123;\"type\":\"NodePort\"&#125;&#125;'\n\n这样就可以通过NodePort来访问了，账号为admin\n获取初始密码\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"&#123;.data.password&#125;\" | base64 -d\n\n接下来就可以在界面上点点点了\n参考资料https://argo-cd.readthedocs.io/en/stable/\n","tags":["gitops"]},{"title":"cdi","url":"/2024/07/09/cdi/","content":"CDI(container device interface)是一个容器运行时支持第三方设备的一个规范，类似CNI一样对添加设备进行抽象\n\n\n现有的情况下添加如gpu的一些设备则需要使用nvidia的runtime来替代默认的runtime，这样只能支持nvidia的gpu方法不通用\n设备由完全限定名称唯一指定，该名称由供应商 ID、设备类别以及每个供应商 ID-设备类别对唯一的名称构成\nvendor.com/class=unique_name\n\ncdi的目录在/etc/cdi和/var/run/cdi\nnvidia\nnvidia的ctk\n\nnvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n\ncontainerd[plugins.\"io.containerd.grpc.v1.cri\"]\n  enable_cdi = true\n  cdi_spec_dirs = [\"/etc/cdi\", \"/var/run/cdi\"]\n\ndocker\n/etc/docker/daemon.json中配置开启cdi,随后重启\n\n&#123;\n  \"features\": &#123;\n    \"cdi\": true\n  &#125;\n&#125;\n\n参考资料https://github.com/cncf-tags/container-device-interfacehttps://developer.aliyun.com/article/1180698\n","tags":["docker","k8s","gpu"]},{"title":"chattr","url":"/2022/05/21/chattr/","content":"chattr是一个可以修改文件属性的命令\n\n\n\nlinux是一个多用户系统，防止一个用户删除了另一个用户的文件，有些病毒入侵服务器之后就会修改此属性让管理员无法删除和修改文件\n\n基本参数说明\n格式为 chattr [-pRVf] [-+&#x3D;aAcCdDeijPsStTu] [-v version] files…\n\n选项:\n\nR：用于递归显示目录的列表属性及其内容。\nV：它将显示程序的版本。\na：用于列出目录的所有文件，其中还包括名称以句点（’.’）开头的文件。\nd：此选项会将目录列为常规文件，而不是列出其内容。\nv：用于显示文件的版本\n\n操作符:\n\n-：删除文件一个属性\n+：添加文件一个属性\n&#x3D;：使选定的属性成为文件所具有的唯一属性\n\n操作属性:\n\na：让文件或目录仅供附加用途。\nb：不更新文件或目录的最后存取时间。\nc：将文件或目录压缩后存放。\nd：将文件或目录排除在倾倒操作之外。\ne: 此属性表示文件正在使用扩展数据块映射磁盘上的块。不能使用chattr修改e属性。\ni：不得任意更动文件或目录。\ns：保密性删除文件或目录。\nS：即时更新文件或目录。\nu：预防意外删除。\n\n查看文件属性lsattr列出文件属性\nlsattr file\n\n只显示了”e”属性\n--------------e---- file\n\n例子添加”i”属性\nsudo chattr +i file\n\n查看文件属性\nlsattr file\n\n增加了”e”属性\n----i---------e---- file\n\n这个时候写入文件时\nsudo echo \"test\" > file\n# zsh: operation not permitted: file\n\n删除文件\nsudo rm -rf file\nrm: cannot remove 'file': Operation not permitted\n\n删除”i”属性\nsudo chattr -i file\n\n这个时候就可以写入和删除操作了\n添加唯一属性\nsudo chattr \"=i\" file\n\n再查看只有一个”i”属性\nlsattr file\n----i-------------- file\n\n参考资料https://www.runoob.com/linux/linux-comm-chattr.htmlhttps://www.geeksforgeeks.org/chattr-command-in-linux-with-examples\n","tags":["lsattr"]},{"title":"Centos8 web console(cockpit)","url":"/2020/08/03/centos8-web-console/","content":"有些时候我们不想登录上centos的服务器执行一些操作这个时候就需要一个图形化界面\n\n\n安装dnf -y install cockpit\n\n启动systemctl start cockpit\n\n开机自动启动systemctl enable cockpit\n\n访问在浏览器中输入&lt;服务器的IP:9090&gt;即可登录到web界面\n输入账号密码后进去如类似下界面\n\n\n端口号可以在 /usr/lib/systemd/system/cockpit.socket中修改\n\n","tags":["linux"]},{"title":"cilium安装部署","url":"/2023/06/21/cilium%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","content":"clium是一个使用ebpf实现的cni\n\n\n安装ebpf需要高版本内核支持,建议5.0以上\n\n\n\n下载二进制文件\n\nCILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/master/stable.txt)\nCLI_ARCH=amd64\nif [ \"$(uname -m)\" = \"aarch64\" ]; then CLI_ARCH=arm64; fi\ncurl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/$&#123;CILIUM_CLI_VERSION&#125;/cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz&#123;,.sha256sum&#125;\nsha256sum --check cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz.sha256sum\nsudo tar xzvfC cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz /usr/local/bin\nrm cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz&#123;,.sha256sum&#125;\n\n\n安装cilium\n\ncilium install\n\ncilium status\n\ncilium hubble enable\n\n\n开启hubble可观测性界面\n\n# 下载二进制文件\nwget https://github.com/cilium/hubble/releases/download/v0.10.0/hubble-linux-amd64.tar.gz\n\n# 开启hubble界面\ncilium hubble enable --ui\n\n# 打开hubble界面\ncilium hubble ui\n\n参考资料https://docs.cilium.io/en/stable/\n","tags":["k8s","cni","网络"]},{"title":"contaInerd源码-diff","url":"/2023/11/03/contaInerd%E6%BA%90%E7%A0%81-diff/","content":"diff主要负责解压缩过程\n\n\n代码版本为v.17.5\n接口定义\n接口比较少只有2个\n\n// diff/diff.go\ntype Applier interface &#123;\n  // Apply applies the content referred to by the given descriptor to\n  // the provided mount. The method of applying is based on the\n  // implementation and content descriptor. For example, in the common\n  // case the descriptor is a file system difference in tar format,\n  // that tar would be applied on top of the mounts.\n  Apply(ctx context.Context, desc ocispec.Descriptor, mount []mount.Mount, opts ...ApplyOpt) (ocispec.Descriptor, error)\n&#125;\n\ntype Comparer interface &#123;\n  // Compare computes the difference between two mounts and returns a\n  // descriptor for the computed diff. The options can provide\n  // a ref which can be used to track the content creation of the diff.\n  // The media type which is used to determine the format of the created\n  // content can also be provided as an option.\n  Compare(ctx context.Context, lower, upper []mount.Mount, opts ...Opt) (ocispec.Descriptor, error)\n&#125;\n\ndiff grpc类型\n常规的注册,从serivce类型中拿到diffservice一个实例\n\n// services/diff/service.go\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.GRPCPlugin,\n    ID:   \"diff\",\n    Requires: []plugin.Type&#123;\n      plugin.ServicePlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      plugins, err := ic.GetByType(plugin.ServicePlugin)\n\n      p, ok := plugins[services.DiffService]\n \n      i, err := p.Instance()\n\n      return &amp;service&#123;local: i.(diffapi.DiffClient)&#125;, nil\n    &#125;,\n  &#125;)\n&#125;\n\n\n同样实现了接口直接调用了service的apply\n\nfunc (s *service) Apply(ctx context.Context, er *diffapi.ApplyRequest) (*diffapi.ApplyResponse, error) &#123;\n  return s.local.Apply(ctx, er)\n&#125;\n\ndiff service类型\n这里注册的时候添加了一个config,获取更下面的DiffPlugin一个实例\n\n// services/diff/local.go\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.ServicePlugin,\n    ID:   services.DiffService,\n    Requires: []plugin.Type&#123;\n      plugin.DiffPlugin,\n    &#125;,\n    Config: defaultDifferConfig,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      differs, err := ic.GetByType(plugin.DiffPlugin)\n  \n      orderedNames := ic.Config.(*config).Order\n      ordered := make([]differ, len(orderedNames))\n      for i, n := range orderedNames &#123;\n        differp, ok := differs[n]\n    \n        d, err := differp.Instance()\n   \n        ordered[i], ok = d.(differ)\n\n      &#125;\n      return &amp;local&#123;\n        differs: ordered,\n      &#125;, nil\n    &#125;,\n  &#125;)\n&#125;\n\n\n组合好opt然后传入到Apply\n\n// services/diff/local.go\nfunc (l *local) Apply(ctx context.Context, er *diffapi.ApplyRequest, _ ...grpc.CallOption) (*diffapi.ApplyResponse, error) &#123;\n  var (\n    ocidesc ocispec.Descriptor\n    err     error\n    desc    = toDescriptor(er.Diff)\n    mounts  = toMounts(er.Mounts)\n  )\n\n  var opts []diff.ApplyOpt\n  if er.Payloads != nil &#123;\n    opts = append(opts, diff.WithPayloads(er.Payloads))\n  &#125;\n\n  for _, differ := range l.differs &#123;\n    ocidesc, err = differ.Apply(ctx, desc, mounts, opts...)\n    if !errdefs.IsNotImplemented(err) &#123;\n      break\n    &#125;\n  &#125;\n  return &amp;diffapi.ApplyResponse&#123;\n    Applied: fromDescriptor(ocidesc),\n  &#125;, nil\n\n&#125;\n\ndiff类型\n这里注册一个DiffPlugin,这里不一样的是从插件里拿的是MetadataPlugin,然后获取metadata的ContentStore()并传值给Comparer和Applier\n\n// diff/walking/plugin/plugin.go\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.DiffPlugin,\n    ID:   \"walking\",\n    Requires: []plugin.Type&#123;\n      plugin.MetadataPlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      md, err := ic.Get(plugin.MetadataPlugin)\n\n      ic.Meta.Platforms = append(ic.Meta.Platforms, platforms.DefaultSpec())\n      cs := md.(*metadata.DB).ContentStore()\n\n      return diffPlugin&#123;\n        Comparer: walking.NewWalkingDiff(cs),\n        Applier:  apply.NewFileSystemApplier(cs),\n      &#125;, nil\n    &#125;,\n  &#125;)\n&#125;\n\n\nfsApplier只有个store\n\n// diff/apply/apply.go\n\n// NewFileSystemApplier returns an applier which simply mounts\n// and applies diff onto the mounted filesystem.\nfunc NewFileSystemApplier(cs content.Provider) diff.Applier &#123;\n  return &amp;fsApplier&#123;\n    store: cs,\n  &#125;\n&#125;\n\n\n这里开始从content里读取blob\n然后申明一个processor,processor主要和解压有关如gz等\n从配置里获取一个processor并赋值\n随后processor赋值到readCounter中\nra传递给apply()进行下一步处理\n\n// diff/apply/apply.go\n\nfunc (s *fsApplier) Apply(ctx context.Context, desc ocispec.Descriptor, mounts []mount.Mount, opts ...diff.ApplyOpt) (d ocispec.Descriptor, err error) &#123;\n  // 从content读取\n  ra, err := s.store.ReaderAt(ctx, desc)\n  defer ra.Close()\n\n  var processors []diff.StreamProcessor\n  processor := diff.NewProcessorChain(desc.MediaType, content.NewReader(ra))\n  processors = append(processors, processor)\n  for &#123;\n    if processor, err = diff.GetProcessor(ctx, processor, config.ProcessorPayloads); err != nil &#123;\n      return emptyDesc, errors.Wrapf(err, \"failed to get stream processor for %s\", desc.MediaType)\n    &#125;\n    processors = append(processors, processor)\n    if processor.MediaType() == ocispec.MediaTypeImageLayer &#123;\n      break\n    &#125;\n  &#125;\n  defer processor.Close()\n\n  digester := digest.Canonical.Digester()\n  rc := &amp;readCounter&#123;\n    r: io.TeeReader(processor, digester.Hash()),\n  &#125;\n\n  //真正开始apply\n  if err := apply(ctx, mounts, rc); err != nil &#123;\n    return emptyDesc, err\n  &#125;\n\n  // Read any trailing data\n  if _, err := io.Copy(io.Discard, rc); err != nil &#123;\n    return emptyDesc, err\n  &#125;\n\n  for _, p := range processors &#123;\n    if ep, ok := p.(interface&#123; Err() error &#125;); ok &#123;\n      if err := ep.Err(); err != nil &#123;\n        return emptyDesc, err\n      &#125;\n    &#125;\n  &#125;\n  return ocispec.Descriptor&#123;\n    MediaType: ocispec.MediaTypeImageLayer,\n    Size:      rc.c,\n    Digest:    digester.Digest(),\n  &#125;, nil\n&#125;\n\n\napply()首先通过mouonts的长度和类型判断是否是临时挂载和使用哪个驱动\n一般在解压是需要mount.WithTempMount()挂载\n需要注意的是apply有各个平台的实现\n\n// diff/apply/apply_linux.go\n\nfunc apply(ctx context.Context, mounts []mount.Mount, r io.Reader) error &#123;\n  switch &#123;\n  case len(mounts) == 1 &amp;&amp; mounts[0].Type == \"overlay\":\n    // OverlayConvertWhiteout (mknod c 0 0) doesn't work in userns.\n    // https://github.com/containerd/containerd/issues/3762\n    if userns.RunningInUserNS() &#123;\n      break\n    &#125;\n    path, parents, err := getOverlayPath(mounts[0].Options)\n    if err != nil &#123;\n      if errdefs.IsInvalidArgument(err) &#123;\n        break\n      &#125;\n      return err\n    &#125;\n    opts := []archive.ApplyOpt&#123;\n      archive.WithConvertWhiteout(archive.OverlayConvertWhiteout),\n    &#125;\n    if len(parents) > 0 &#123;\n      opts = append(opts, archive.WithParents(parents))\n    &#125;\n    _, err = archive.Apply(ctx, path, r, opts...)\n    return err\n  case len(mounts) == 1 &amp;&amp; mounts[0].Type == \"aufs\":\n    path, parents, err := getAufsPath(mounts[0].Options)\n    if err != nil &#123;\n      if errdefs.IsInvalidArgument(err) &#123;\n        break\n      &#125;\n      return err\n    &#125;\n    opts := []archive.ApplyOpt&#123;\n      archive.WithConvertWhiteout(archive.AufsConvertWhiteout),\n    &#125;\n    if len(parents) > 0 &#123;\n      opts = append(opts, archive.WithParents(parents))\n    &#125;\n    _, err = archive.Apply(ctx, path, r, opts...)\n    return err\n  &#125;\n  return mount.WithTempMount(ctx, mounts, func(root string) error &#123;\n    _, err := archive.Apply(ctx, root, r)\n    return err\n  &#125;)\n&#125;\n\n\n这里开始执行bind挂载\n\n// mount/temp.go\n\n// WithTempMount mounts the provided mounts to a temp dir, and pass the temp dir to f.\n// The mounts are valid during the call to the f.\n// Finally we will unmount and remove the temp dir regardless of the result of f.\nfunc WithTempMount(ctx context.Context, mounts []Mount, f func(root string) error) (err error) &#123;\n  root, uerr := ioutil.TempDir(tempMountLocation, \"containerd-mount\")\n  if uerr != nil &#123;\n    return errors.Wrapf(uerr, \"failed to create temp dir\")\n  &#125;\n  // We use Remove here instead of RemoveAll.\n  // The RemoveAll will delete the temp dir and all children it contains.\n  // When the Unmount fails, RemoveAll will incorrectly delete data from\n  // the mounted dir. However, if we use Remove, even though we won't\n  // successfully delete the temp dir and it may leak, we won't loss data\n  // from the mounted dir.\n  // For details, please refer to #1868 #1785.\n  defer func() &#123;\n    if uerr = os.Remove(root); uerr != nil &#123;\n      log.G(ctx).WithError(uerr).WithField(\"dir\", root).Errorf(\"failed to remove mount temp dir\")\n    &#125;\n  &#125;()\n\n  // We should do defer first, if not we will not do Unmount when only a part of Mounts are failed.\n  defer func() &#123;\n    if uerr = UnmountAll(root, 0); uerr != nil &#123;\n      uerr = errors.Wrapf(uerr, \"failed to unmount %s\", root)\n      if err == nil &#123;\n        err = uerr\n      &#125; else &#123;\n        err = errors.Wrap(err, uerr.Error())\n      &#125;\n    &#125;\n  &#125;()\n\n  // [&#123;bind /root/snapshotter/snapshots/1/fs [rw rbind]&#125;] /var/lib/containerd/tmpmounts/containerd-mount4278343774\n  if uerr = All(mounts, root); uerr != nil &#123;\n    return errors.Wrapf(uerr, \"failed to mount %s\", root)\n  &#125;\n  return errors.Wrapf(f(root), \"mount callback failed on %s\", root)\n&#125;\n\n\nAll()遍历所有mouts并执行挂载\n\n// mount/mount.go\n\n// All mounts all the provided mounts to the provided target\nfunc All(mounts []Mount, target string) error &#123;\n  for _, m := range mounts &#123;\n    if err := m.Mount(target); err != nil &#123;\n      return err\n    &#125;\n  &#125;\n  return nil\n&#125;\n\n// mount/mount_linux.go\nfunc (m *Mount) Mount(target string) (err error) &#123;\n  for _, helperBinary := range allowedHelperBinaries &#123;\n    // helperBinary = \"mount.fuse\", typePrefix = \"fuse.\"\n    typePrefix := strings.TrimPrefix(helperBinary, \"mount.\") + \".\"\n    if strings.HasPrefix(m.Type, typePrefix) &#123;\n      return m.mountWithHelper(helperBinary, typePrefix, target)\n    &#125;\n  &#125;\n  var (\n    chdir   string\n    options = m.Options\n  )\n\n  // avoid hitting one page limit of mount argument buffer\n  //\n  // NOTE: 512 is a buffer during pagesize check.\n  if m.Type == \"overlay\" &amp;&amp; optionsSize(options) >= pagesize-512 &#123;\n    chdir, options = compactLowerdirOption(options)\n  &#125;\n\n  flags, data, losetup := parseMountOptions(options)\n  if len(data) > pagesize &#123;\n    return errors.Errorf(\"mount options is too long\")\n  &#125;\n\n  // propagation types.\n  const ptypes = unix.MS_SHARED | unix.MS_PRIVATE | unix.MS_SLAVE | unix.MS_UNBINDABLE\n\n  // Ensure propagation type change flags aren't included in other calls.\n  oflags := flags &amp;^ ptypes\n\n  // In the case of remounting with changed data (data != \"\"), need to call mount (moby/moby#34077).\n  if flags&amp;unix.MS_REMOUNT == 0 || data != \"\" &#123;\n    // Initial call applying all non-propagation flags for mount\n    // or remount with changed data\n    source := m.Source\n    if losetup &#123;\n      loFile, err := setupLoop(m.Source, LoopParams&#123;\n        Readonly:  oflags&amp;unix.MS_RDONLY == unix.MS_RDONLY,\n        Autoclear: true&#125;)\n      if err != nil &#123;\n        return err\n      &#125;\n      defer loFile.Close()\n\n      // Mount the loop device instead\n      source = loFile.Name()\n    &#125;\n    // 执行mount系统调用\n    if err := mountAt(chdir, source, target, m.Type, uintptr(oflags), data); err != nil &#123;\n      return err\n    &#125;\n  &#125;\n\n\n看完bind挂载在看下普通的Apply()\n根据applyFunc参数来确定使用哪个apply，没有则默认使用applyFunc\n\n// archive/tar.go\n\n// Apply applies a tar stream of an OCI style diff tar.\n// See https://github.com/opencontainers/image-spec/blob/master/layer.md#applying-changesets\nfunc Apply(ctx context.Context, root string, r io.Reader, opts ...ApplyOpt) (int64, error) &#123;\n  root = filepath.Clean(root)\n\n  var options ApplyOptions\n  for _, opt := range opts &#123;\n    if err := opt(&amp;options); err != nil &#123;\n      return 0, errors.Wrap(err, \"failed to apply option\")\n    &#125;\n  &#125;\n  if options.Filter == nil &#123;\n    options.Filter = all\n  &#125;\n  if options.applyFunc == nil &#123;\n    options.applyFunc = applyFunc // 这里调用了applyNaive\n  &#125;\n\n  return options.applyFunc(ctx, root, r, options)\n&#125;\n\n\napplyNaive负责将tar文件解压到指定目录中(和snap绑定的临时目录tmpmounts)\n\n// archive/tar.go\n\n// applyNaive applies a tar stream of an OCI style diff tar to a directory\n// applying each file as either a whole file or whiteout.\n// See https://github.com/opencontainers/image-spec/blob/master/layer.md#applying-changesets\nfunc applyNaive(ctx context.Context, root string, r io.Reader, options ApplyOptions) (size int64, err error) &#123;\n  var (\n    dirs []*tar.Header\n\n    tr = tar.NewReader(r)\n\n    // Used for handling opaque directory markers which\n    // may occur out of order\n    unpackedPaths = make(map[string]struct&#123;&#125;)\n\n    convertWhiteout = options.ConvertWhiteout\n  )\n\n  if convertWhiteout == nil &#123;\n    // handle whiteouts by removing the target files\n    convertWhiteout = func(hdr *tar.Header, path string) (bool, error) &#123;\n      base := filepath.Base(path)\n      dir := filepath.Dir(path)\n      if base == whiteoutOpaqueDir &#123;\n        _, err := os.Lstat(dir)\n        if err != nil &#123;\n          return false, err\n        &#125;\n        err = filepath.Walk(dir, func(path string, info os.FileInfo, err error) error &#123;\n          if err != nil &#123;\n            if os.IsNotExist(err) &#123;\n              err = nil // parent was deleted\n            &#125;\n            return err\n          &#125;\n          if path == dir &#123;\n            return nil\n          &#125;\n          if _, exists := unpackedPaths[path]; !exists &#123;\n            err := os.RemoveAll(path)\n            return err\n          &#125;\n          return nil\n        &#125;)\n        return false, err\n      &#125;\n\n      if strings.HasPrefix(base, whiteoutPrefix) &#123;\n        originalBase := base[len(whiteoutPrefix):]\n        originalPath := filepath.Join(dir, originalBase)\n\n        return false, os.RemoveAll(originalPath)\n      &#125;\n\n      return true, nil\n    &#125;\n  &#125;\n\n  // Iterate through the files in the archive.\n  for &#123;\n    select &#123;\n    case &lt;-ctx.Done():\n      return 0, ctx.Err()\n    default:\n    &#125;\n\n    hdr, err := tr.Next()\n    if err == io.EOF &#123;\n      // end of tar archive\n      break\n    &#125;\n    if err != nil &#123;\n      return 0, err\n    &#125;\n\n    size += hdr.Size\n\n    // Normalize name, for safety and for a simple is-root check\n    hdr.Name = filepath.Clean(hdr.Name)\n\n    accept, err := options.Filter(hdr)\n    if err != nil &#123;\n      return 0, err\n    &#125;\n    if !accept &#123;\n      continue\n    &#125;\n\n    if skipFile(hdr) &#123;\n      log.G(ctx).Warnf(\"file %q ignored: archive may not be supported on system\", hdr.Name)\n      continue\n    &#125;\n\n    // Split name and resolve symlinks for root directory.\n    ppath, base := filepath.Split(hdr.Name)\n    ppath, err = fs.RootPath(root, ppath)\n    if err != nil &#123;\n      return 0, errors.Wrap(err, \"failed to get root path\")\n    &#125;\n\n    // Join to root before joining to parent path to ensure relative links are\n    // already resolved based on the root before adding to parent.\n    path := filepath.Join(ppath, filepath.Join(\"/\", base))\n    if path == root &#123;\n      log.G(ctx).Debugf(\"file %q ignored: resolved to root\", hdr.Name)\n      continue\n    &#125;\n\n    // If file is not directly under root, ensure parent directory\n    // exists or is created.\n    if ppath != root &#123;\n      parentPath := ppath\n      if base == \"\" &#123;\n        parentPath = filepath.Dir(path)\n      &#125;\n      if err := mkparent(ctx, parentPath, root, options.Parents); err != nil &#123;\n        return 0, err\n      &#125;\n    &#125;\n\n    // Naive whiteout convert function which handles whiteout files by\n    // removing the target files.\n    if err := validateWhiteout(path); err != nil &#123;\n      return 0, err\n    &#125;\n    writeFile, err := convertWhiteout(hdr, path)\n    if err != nil &#123;\n      return 0, errors.Wrapf(err, \"failed to convert whiteout file %q\", hdr.Name)\n    &#125;\n    if !writeFile &#123;\n      continue\n    &#125;\n    // If path exits we almost always just want to remove and replace it.\n    // The only exception is when it is a directory *and* the file from\n    // the layer is also a directory. Then we want to merge them (i.e.\n    // just apply the metadata from the layer).\n    if fi, err := os.Lstat(path); err == nil &#123;\n      if !(fi.IsDir() &amp;&amp; hdr.Typeflag == tar.TypeDir) &#123;\n        if err := os.RemoveAll(path); err != nil &#123;\n          return 0, err\n        &#125;\n      &#125;\n    &#125;\n\n    srcData := io.Reader(tr)\n    srcHdr := hdr\n\n    if err := createTarFile(ctx, path, root, srcHdr, srcData); err != nil &#123;\n      return 0, err\n    &#125;\n\n    // Directory mtimes must be handled at the end to avoid further\n    // file creation in them to modify the directory mtime\n    if hdr.Typeflag == tar.TypeDir &#123;\n      dirs = append(dirs, hdr)\n    &#125;\n    unpackedPaths[path] = struct&#123;&#125;&#123;&#125;\n  &#125;\n\n  for _, hdr := range dirs &#123;\n    path, err := fs.RootPath(root, hdr.Name)\n    if err != nil &#123;\n      return 0, err\n    &#125;\n    if err := chtimes(path, boundTime(latestTime(hdr.AccessTime, hdr.ModTime)), boundTime(hdr.ModTime)); err != nil &#123;\n      return 0, err\n    &#125;\n  &#125;\n\n  return size, nil\n&#125;\n\nProcessor\nprocessor主要负责解压缩相关比如gz等\n在apply这个函数中获取了processor\n\n// diff/apply/apply.go\nfunc (s *fsApplier) Apply(ctx context.Context, desc ocispec.Descriptor, mounts []mount.Mount, opts ...diff.ApplyOpt) (d ocispec.Descriptor, err error) &#123;\n  // 从content读取\n  ra, err := s.store.ReaderAt(ctx, desc)\n  defer ra.Close()\n\n  var processors []diff.StreamProcessor\n  processor := diff.NewProcessorChain(desc.MediaType, content.NewReader(ra))\n  processors = append(processors, processor)\n  for &#123;\n    if processor, err = diff.GetProcessor(ctx, processor, config.ProcessorPayloads); err != nil &#123;\n      return emptyDesc, errors.Wrapf(err, \"failed to get stream processor for %s\", desc.MediaType)\n    &#125;\n    processors = append(processors, processor)\n    if processor.MediaType() == ocispec.MediaTypeImageLayer &#123;\n      break\n    &#125;\n  &#125;\n  defer processor.Close()\n\n  digester := digest.Canonical.Digester()\n  rc := &amp;readCounter&#123;\n    r: io.TeeReader(processor, digester.Hash()),\n  &#125;\n\n\n注册在这里，从配置文件遍历然后注册\n\n// services/server/server.go\n\n// New creates and initializes a new containerd server\nfunc New(ctx context.Context, config *srvconfig.Config) (*Server, error) &#123;\n  // ...\n\n  for id, p := range config.StreamProcessors &#123;\n    diff.RegisterProcessor(diff.BinaryHandler(id, p.Returns, p.Accepts, p.Path, p.Args, p.Env)) // 注册 processor\n  &#125;\n// ...\n\n\n从注释来看是根据配置配置的MediaType,来选择二进制解压\n\n// diff/stream.go\n// BinaryHandler creates a new stream processor handler which calls out to the given binary.\n// The id is used to identify the stream processor and allows the caller to send\n// payloads specific for that stream processor (i.e. decryption keys for decrypt stream processor).\n// The binary will be called for the provided mediaTypes and return the given media type.\nfunc BinaryHandler(id, returnsMediaType string, mediaTypes []string, path string, args, env []string) Handler &#123;\n  set := make(map[string]struct&#123;&#125;, len(mediaTypes))\n  for _, m := range mediaTypes &#123;\n    set[m] = struct&#123;&#125;&#123;&#125;\n  &#125;\n  return func(_ context.Context, mediaType string) (StreamProcessorInit, bool) &#123;\n    if _, ok := set[mediaType]; ok &#123;\n      return func(ctx context.Context, stream StreamProcessor, payloads map[string]*types.Any) (StreamProcessor, error) &#123;\n        payload := payloads[id]\n        return NewBinaryProcessor(ctx, mediaType, returnsMediaType, stream, path, args, env, payload)\n      &#125;, true\n    &#125;\n    return nil, false\n  &#125;\n&#125;\n\n\n而默认情况下是compressedHandler()\n\nfunc init() &#123;\n  // register the default compression handler\n  RegisterProcessor(compressedHandler)\n&#125;\n\nfunc compressedHandler(ctx context.Context, mediaType string) (StreamProcessorInit, bool) &#123;\n  compressed, err := images.DiffCompression(ctx, mediaType)\n\n  if compressed != \"\" &#123;\n    return func(ctx context.Context, stream StreamProcessor, payloads map[string]*types.Any) (StreamProcessor, error) &#123;\n      ds, err := compression.DecompressStream(stream)\n\n      return &amp;compressedProcessor&#123;\n        rc: ds,\n      &#125;, nil\n    &#125;, true\n  &#125;\n  return func(ctx context.Context, stream StreamProcessor, payloads map[string]*types.Any) (StreamProcessor, error) &#123;\n    return &amp;stdProcessor&#123;\n      rc: stream,\n    &#125;, nil\n  &#125;, true\n&#125;\n\n\nDecompress()就负责读取压缩格式\n\n// DecompressStream decompresses the archive and returns a ReaderCloser with the decompressed archive.\nfunc DecompressStream(archive io.Reader) (DecompressReadCloser, error) &#123;\n  buf := newBufferedReader(archive)\n  bs, err := buf.Peek(10)\n  if err != nil &amp;&amp; err != io.EOF &#123;\n    // Note: we'll ignore any io.EOF error because there are some odd\n    // cases where the layer.tar file will be empty (zero bytes) and\n    // that results in an io.EOF from the Peek() call. So, in those\n    // cases we'll just treat it as a non-compressed stream and\n    // that means just create an empty layer.\n    // See Issue docker/docker#18170\n    return nil, err\n  &#125;\n\n  switch compression := DetectCompression(bs); compression &#123;\n  case Uncompressed:\n    return &amp;readCloserWrapper&#123;\n      Reader:      buf,\n      compression: compression,\n    &#125;, nil\n  case Gzip:\n    ctx, cancel := context.WithCancel(context.Background())\n    gzReader, err := gzipDecompress(ctx, buf)\n    if err != nil &#123;\n      cancel()\n      return nil, err\n    &#125;\n\n    return &amp;readCloserWrapper&#123;\n      Reader:      gzReader,\n      compression: compression,\n      closer: func() error &#123;\n        cancel()\n        return gzReader.Close()\n      &#125;,\n    &#125;, nil\n  case Zstd:\n    zstdReader, err := zstd.NewReader(buf)\n    if err != nil &#123;\n      return nil, err\n    &#125;\n    return &amp;readCloserWrapper&#123;\n      Reader:      zstdReader,\n      compression: compression,\n      closer: func() error &#123;\n        zstdReader.Close()\n        return nil\n      &#125;,\n    &#125;, nil\n\n  default:\n    return nil, fmt.Errorf(\"unsupported compression format %s\", (&amp;compression).Extension())\n  &#125;\n&#125;\n","tags":["k8s","containerd"]},{"title":"containerd基本使用","url":"/2023/10/24/containerd%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"containerd是一个容器运行时标准,也是k8s目前主流的运行时，本文介绍历史，存储方式，和oci\n\n\ncontainerd历史containerd是一个运行时，最开始集成在docker中拆解出来捐赠给cncf，同样被拆解出来的还有runc，以及根据docker镜像v2制定的oci镜像规范\n\n1.24版本之前k8s的kubelet内置了dokcer-shim插件负责和docker通讯，之后则只能通过cri\n\nflowchart LR\n  kubelet(kubelet)--&gt;docker-shim(docker-shim)\n  docker-shim(docker-shim)--&gt;docker(docker)\n  docker(docker)--&gt;containers([containers])\n\n\n最开始cri是一个单独的进程,然后发现效率太差\n\nflowchart LR\n  kubelet(kubelet)--&quot;grpc&quot;--&gt;cri-plugin(cri-plugin)\n\n  cri-plugin(cri-plugin)--&quot;grpc&quot;--&gt;containerd(containerd)\n\n  containerd(containerd)--&quot;exec&quot;--&gt;containerd-shim-runc(containerd-shim-runc)\n  containerd-shim-runc(containerd-shim-runc)--&quot;exec&quot;--&gt;runc(runc)\n  runc(runc)--&quot;exec&quot;--&gt;containers([containers])\n\n\n最后将cri已插件的形式集成到containerd中\n\nflowchart LR\n  kubelet(kubelet)--&quot;grpc&quot;--&gt;cri-plugin(cri-plugin)\n\n  subgraph containerd组件\n  cri-plugin(cri-plugin)--&gt;containerd(containerd)\n  end\n\n  containerd(containerd)--&quot;exec&quot;--&gt;containerd-shim-runc(containerd-shim-runc)\n  containerd-shim-runc(containerd-shim-runc)--&quot;exec&quot;--&gt;runc(runc)\n  runc(runc)--&quot;exec&quot;--&gt;containers([containers])\n\n架构\n全局架构\n\n\n\ncontainerd与kubelet\n\n\n\n内部插件架构\n\n\nflowchart LR\n  kubectl(kubctl)&lt;--&gt;api-server(api-server)\n\n  subgraph master\n  api-server(api-server)&lt;--&gt;etcd[(etcd)]\n  api-server(api-server)&lt;--&gt;scheduler(scheduler)\n  api-server(api-server)&lt;--&gt;controller-manage(controller-manage)\n  end\n\n  api-server(api-server)&lt;--&gt;kubelet(kubelet)\n  kubelet(kubelet)&lt;--&quot;grpc&quot;--&gt;containerd(containerd)\n\n  subgraph containerd组件\n  containerd(containerd)&lt;--&quot;exec&quot;--&gt;containerd-shim-runc(containerd-shim-runc)\n  containerd-shim-runc(containerd-shim-runc)&lt;--&quot;exec&quot;--&gt;runc(runc)\n  runc(runc)&lt;--&quot;exec&quot;--&gt;containers(containers)\n  end\n\n  api-server(api-server)&lt;--&gt;kube-proxy(kube-proxy)\n  kube-proxy(kube-proxy)&lt;--&gt;ipt(iptables&#x2F;ipvs)\n\ncontainerd下载安装\ndocker官方打包的rpm地址\n\n官方仓库下载https://github.com/containerd/containerd/releases\n\n一般安装docker的时候自动安装\n\n\nctr使用\ncontainerd可以使用的客户端有很多比如crictl，nerdctl命令\n\n\n–address value, -a value 指定 containerd’s GRPC server，默认 &#x2F;run&#x2F;containerd&#x2F;containerd.sock\n\n查看K8S命名空间下的镜像,-n指定namespace\n\n\nctr -n k8s.io images ls\n\n\n查看所有namespace\n\nctr ns ls\n\n\n下载镜像\n\nctr images pull docker.io/library/alpine:3.18.3\n\n\n创建 container\n\nctr c create docker.io/library/alpine:3.18.3 alpine\n\n\n查看\n\nctr c ls\n\n\n后台启动\n\nctr t start -d alpine\n\n\n查看container\n\nctr t ls\n\n\n查看 k8s 中正在运行的容器\n\nctr -n k8s.io task ls\n\n\n打tag\n\nctr images tag docker.io/library/alpine:3.18.3 &lt;仓库>/alpine:3.18.3\n\n\npush 上传镜像\n\nctr images push &lt;镜像>\n\n\n查看插件\n\nctr plugin ls\n\n\n打印containerd默认配置\n\ncontainerd config default\n\nOCI镜像格式\n各个类型的关系\n\noci镜像格式的mediaType参考这里\n\n类型mediaType\n所有例子基于alpine:3.18.3\n\nindex\n最顶级的类型主要存放各个操作系统和平台对应的manifest类型\n\n\n\n例子\n\n&#123;\n    \"manifests\": [\n        &#123;\n            \"digest\": \"sha256:c5c5fda71656f28e49ac9c5416b3643eaa6a108a8093151d6d1afc9463be8e33\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"platform\": &#123;\n                \"architecture\": \"amd64\",\n                \"os\": \"linux\"\n            &#125;,\n            \"size\": 528\n        &#125;,\n        &#123;\n            \"digest\": \"sha256:f748290eb66ad6f938e25dd348acfb3527a422e280b7547b1cdfaf38d4492c4b\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"platform\": &#123;\n                \"architecture\": \"arm\",\n                \"os\": \"linux\",\n                \"variant\": \"v6\"\n            &#125;,\n            \"size\": 528\n        &#125;,\n        &#123;\n            \"digest\": \"sha256:16e86b2388774982fbdf230101a72201691b1f97cb0066c2099abf30dd7e6d59\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"platform\": &#123;\n                \"architecture\": \"arm\",\n                \"os\": \"linux\",\n                \"variant\": \"v7\"\n            &#125;,\n            \"size\": 528\n        &#125;,\n        &#123;\n            \"digest\": \"sha256:b312e4b0e2c665d634602411fcb7c2699ba748c36f59324457bc17de485f36f6\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"platform\": &#123;\n                \"architecture\": \"arm64\",\n                \"os\": \"linux\",\n                \"variant\": \"v8\"\n            &#125;,\n            \"size\": 528\n        &#125;,\n        &#123;\n            \"digest\": \"sha256:1fd62556954250bac80d601a196bb7fd480ceba7c10e94dd8fd4c6d1c08783d5\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"platform\": &#123;\n                \"architecture\": \"386\",\n                \"os\": \"linux\"\n            &#125;,\n            \"size\": 528\n        &#125;,\n        &#123;\n            \"digest\": \"sha256:c75ede79e457d6454bca6fc51967a247a4b9daff9f31197cfbef69b1a651cada\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"platform\": &#123;\n                \"architecture\": \"ppc64le\",\n                \"os\": \"linux\"\n            &#125;,\n            \"size\": 528\n        &#125;,\n        &#123;\n            \"digest\": \"sha256:5febc00b4d2a84af2a077bc34ea90659b6570110a54253f19c5dca8164b1dbf6\",\n            \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n            \"platform\": &#123;\n                \"architecture\": \"s390x\",\n                \"os\": \"linux\"\n            &#125;,\n            \"size\": 528\n        &#125;\n    ],\n    \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n    \"schemaVersion\": 2\n&#125;\n\n\n\nmanifest\n主要定义镜像相关的清单，主要包含config类型和layers类型\n\n&#123;\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n  \"config\": &#123;\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n    \"size\": 1471,\n    \"digest\": \"sha256:7e01a0d0a1dcd9e539f8e9bbd80106d59efbdf97293b3d38f5d7a34501526cdb\"\n  &#125;,\n  \"layers\": [\n    &#123;\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 3401613,\n      \"digest\": \"sha256:7264a8db6415046d36d16ba98b79778e18accee6ffa71850405994cffa9be7de\"\n    &#125;\n  ]\n&#125;\n\nconfig\n记录镜像的历史，启动参数环境变量以及最重要的解压后的层等信息\n\n\n\n例子\n\n&#123;\n    \"architecture\": \"amd64\",\n    \"config\": &#123;\n        \"Hostname\": \"\",\n        \"Domainname\": \"\",\n        \"User\": \"\",\n        \"AttachStdin\": false,\n        \"AttachStdout\": false,\n        \"AttachStderr\": false,\n        \"Tty\": false,\n        \"OpenStdin\": false,\n        \"StdinOnce\": false,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"Cmd\": [\n            \"/bin/sh\"\n        ],\n        \"Image\": \"sha256:39dfd593e04b939e16d3a426af525cad29b8fc7410b06f4dbad8528b45e1e5a9\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\",\n        \"Entrypoint\": null,\n        \"OnBuild\": null,\n        \"Labels\": null\n    &#125;,\n    \"container\": \"ba09fe2c8f99faad95871d467a22c96f4bc8166bd01ce0a7c28dd5472697bfd1\",\n    \"container_config\": &#123;\n        \"Hostname\": \"ba09fe2c8f99\",\n        \"Domainname\": \"\",\n        \"User\": \"\",\n        \"AttachStdin\": false,\n        \"AttachStdout\": false,\n        \"AttachStderr\": false,\n        \"Tty\": false,\n        \"OpenStdin\": false,\n        \"StdinOnce\": false,\n        \"Env\": [\n            \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n        ],\n        \"Cmd\": [\n            \"/bin/sh\",\n            \"-c\",\n            \"#(nop) \",\n            \"CMD [\\\"/bin/sh\\\"]\"\n        ],\n        \"Image\": \"sha256:39dfd593e04b939e16d3a426af525cad29b8fc7410b06f4dbad8528b45e1e5a9\",\n        \"Volumes\": null,\n        \"WorkingDir\": \"\",\n        \"Entrypoint\": null,\n        \"OnBuild\": null,\n        \"Labels\": &#123;&#125;\n    &#125;,\n    \"created\": \"2023-08-07T19:20:20.894140623Z\",\n    \"docker_version\": \"20.10.23\",\n    \"history\": [\n        &#123;\n            \"created\": \"2023-08-07T19:20:20.71894984Z\",\n            \"created_by\": \"/bin/sh -c #(nop) ADD file:32ff5e7a78b890996ee4681cc0a26185d3e9acdb4eb1e2aaccb2411f922fed6b in / \"\n        &#125;,\n        &#123;\n            \"created\": \"2023-08-07T19:20:20.894140623Z\",\n            \"created_by\": \"/bin/sh -c #(nop)  CMD [\\\"/bin/sh\\\"]\",\n            \"empty_layer\": true\n        &#125;\n    ],\n    \"os\": \"linux\",\n    \"rootfs\": &#123;\n        \"type\": \"layers\",\n        \"diff_ids\": [\n            \"sha256:4693057ce2364720d39e57e85a5b8e0bd9ac3573716237736d6470ec5b7b7230\"\n        ]\n    &#125;\n&#125;\n\n\n\nlayer\n记录压缩过后的层sha256信息,是真正层的内容一般采用tar.gz压缩\n\n下载保存oci镜像格式使用skopeo可以和方便的保存到本地，之前介绍过使用skopeo同步docker镜像\nskopeo copy docker://docker.io/alpine:3.18.3 oci:alpine-oci --override-os linux --override-arch amd64\n\n❯ tree                                                                   \n.\n├── blobs\n│   └── sha256\n│       ├── 7264a8db6415046d36d16ba98b79778e18accee6ffa71850405994cffa9be7de\n│       ├── 913cf3a39d377faf89ed388ad913a318a390488c9f34c46e43424795cdabffe8\n│       └── cf4e5bc0709f07284518b287f570c47bdb2afc14b8ae4f14077e9ff810a0120b\n├── index.json\n└── oci-layout\n\n3 directories, 5 files\n\n\nContainerd存储方式目录结构\n其目录命名格式以类型.id的格式\n\n有些目录根据插件并没有显示\n\n默认数据存放在/var/lib/containerd/中\n\n\ntree -L 2\n.\n├── io.containerd.content.v1.content # 存放从hub上下载的源文件\n│   ├── blobs                        # 存放下载完毕的文件\n│   └── ingest                       # 存放下载未完成的文件\n├── io.containerd.grpc.v1.cri        # cri插件存放的文件\n│   ├── containers                   # cri创建的容器\n│   └── sandboxes\n├── io.containerd.metadata.v1.bolt   # 存放containerd的数据文件\n│   └── meta.db           \n├── io.containerd.runtime.v1.linux\n├── io.containerd.runtime.v2.task    # 运行的容器\n│   └── k8s.io                       # namespace\n├── io.containerd.snapshotter.v1.native\n│   └── snapshots\n├── io.containerd.snapshotter.v1.overlayfs # 存放解压过后的文件\n│   ├── metadata.db                        # 解压数据库文件\n│   └── snapshots                          # 解压的文件\n└── tmpmounts                              # 临时挂载目录\n\n15 directories, 2 files\n\n参考https://blog.frognew.com/2021/04/relearning-container-01.html\n","tags":["k8s","containerd"]},{"title":"containerd源码-content","url":"/2023/11/03/containerd%E6%BA%90%E7%A0%81-content/","content":"content 主要负责存储下载后的原本的层\n\n\n代码版本为v.17.5\ncontent\ncontent主要负责存储下载的layer接口定义在content/content.go中\n\n// content/content.go\n\ntype ReaderAt interface &#123;\n  io.ReaderAt\n  io.Closer\n  Size() int64\n&#125;\n\ntype Provider interface &#123;\n  ReaderAt(ctx context.Context, desc ocispec.Descriptor) (ReaderAt, error)\n&#125;\n\ntype Ingester interface &#123;\n  Writer(ctx context.Context, opts ...WriterOpt) (Writer, error)\n&#125;\n\ntype Info struct &#123;\n  Digest    digest.Digest\n  Size      int64\n  CreatedAt time.Time\n  UpdatedAt time.Time\n  Labels    map[string]string\n&#125;\n\n// Status of a content operation\ntype Status struct &#123;\n  Ref       string\n  Offset    int64\n  Total     int64\n  Expected  digest.Digest\n  StartedAt time.Time\n  UpdatedAt time.Time\n&#125;\n\n\ntype WalkFunc func(Info) error\ntype Manager interface &#123;\n  Info(ctx context.Context, dgst digest.Digest) (Info, error)\n  Update(ctx context.Context, info Info, fieldpaths ...string) (Info, error)\n  Walk(ctx context.Context, fn WalkFunc, filters ...string) error\n  Delete(ctx context.Context, dgst digest.Digest) error\n&#125;\ntype IngestManager interface &#123;\n  Status(ctx context.Context, ref string) (Status, error)\n  ListStatuses(ctx context.Context, filters ...string) ([]Status, error)\n  Abort(ctx context.Context, ref string) error\n&#125;\n\n\ntype Writer interface &#123;\n  io.WriteCloser\n  Digest() digest.Digest\n  Commit(ctx context.Context, size int64, expected digest.Digest, opts ...Opt) error\n  Status() (Status, error)\n  Truncate(size int64) error\n&#125;\n\ntype Store interface &#123;\n  Manager\n  Provider\n  IngestManager\n  Ingester\n&#125;\n\ncontent grpc类型\ngrpc类型的content注册在这里,使用统一的注册，申明名字类型以及依赖\n然后从initcontent中获取所有service的插件,然后拿到一个ContentService实例\n使用这个实例调用contentserver.New(),contentserver.New()实现了grpc相关方法\n\n// services/content/service.go\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.GRPCPlugin,\n    ID:   \"content\",\n    Requires: []plugin.Type&#123;\n      plugin.ServicePlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      plugins, err := ic.GetByType(plugin.ServicePlugin)\n      if err != nil &#123;\n        return nil, err\n      &#125;\n\n      p, ok := plugins[services.ContentService]\n      if !ok &#123;\n        return nil, errors.New(\"content store service not found\")\n      &#125;\n      cs, err := p.Instance()\n      if err != nil &#123;\n        return nil, err\n      &#125;\n      return contentserver.New(cs.(content.Store)), nil\n    &#125;,\n  &#125;)\n&#125;\n\n\nservice就是抽象了content.Store\n\nNew()设置了上层\n\n\n// services/content/contentserver/contentserver.go\n\ntype service struct &#123;\n  store content.Store\n&#125;\n\n// New returns the content GRPC server\nfunc New(cs content.Store) api.ContentServer &#123;\n  return &amp;service&#123;store: cs&#125;\n&#125;\n\nfunc (s *service) Register(server *grpc.Server) error &#123;\n  api.RegisterContentServer(server, s)\n  return nil\n&#125;\n\n\n由于接口很多就不一样介绍了，这里只介绍一个简单的接口\n可以看到grpc请求来的参数传到store.Status()然后再将返回的组装成grpc结果并返回，其他api也是类似这种\n\n// services/content/contentserver/contentserver.go\n\nfunc (s *service) Status(ctx context.Context, req *api.StatusRequest) (*api.StatusResponse, error) &#123;\n  status, err := s.store.Status(ctx, req.Ref)\n  if err != nil &#123;\n    return nil, errdefs.ToGRPCf(err, \"could not get status for ref %q\", req.Ref)\n  &#125;\n\n  var resp api.StatusResponse\n  resp.Status = &amp;api.Status&#123;\n    StartedAt: status.StartedAt,\n    UpdatedAt: status.UpdatedAt,\n    Ref:       status.Ref,\n    Offset:    status.Offset,\n    Total:     status.Total,\n    Expected:  status.Expected,\n  &#125;\n\n  return &amp;resp, nil\n&#125;\n\ncontent service类型\n这里他依赖plugin.MetadataPlugin这个类型,然后将获取的meteada传入meatadata.ContentStore()\n\n// services/content/store.go\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.ServicePlugin,\n    ID:   services.ContentService,\n    Requires: []plugin.Type&#123;\n      plugin.MetadataPlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      m, err := ic.Get(plugin.MetadataPlugin)\n      if err != nil &#123;\n        return nil, err\n      &#125;\n\n      // 这里注册 content的svc\n      s, err := newContentStore(m.(*metadata.DB).ContentStore(), ic.Events)\n      return s, err\n    &#125;,\n  &#125;)\n&#125;\n\n\nfunc newContentStore(cs content.Store, publisher events.Publisher) (content.Store, error) &#123;\n  return &amp;store&#123;\n    Store:     cs,\n    publisher: publisher,\n  &#125;, nil\n&#125;\n\n\n可以看到前面调用的ContentStore()返回的就是初始化,而meteadata创建的注册在services/server/server.go前面介绍启动过程介绍过\n\n// metadata/db.go\n\n\n// NewDB creates a new metadata database using the provided\n// bolt database, content store, and snapshotters.\nfunc NewDB(db *bolt.DB, cs content.Store, ss map[string]snapshots.Snapshotter, opts ...DBOpt) *DB &#123;\n  m := &amp;DB&#123;\n    db:      db,\n    ss:      make(map[string]*snapshotter, len(ss)),\n    dirtySS: map[string]struct&#123;&#125;&#123;&#125;,\n    dbopts: dbOptions&#123;\n      shared: true,\n    &#125;,\n  &#125;\n\n  for _, opt := range opts &#123;\n    opt(&amp;m.dbopts)\n  &#125;\n\n  // Initialize data stores\n  m.cs = newContentStore(m, m.dbopts.shared, cs)\n  for name, sn := range ss &#123;\n    m.ss[name] = newSnapshotter(m, name, sn)\n  &#125;\n\n  return m\n&#125;\n\n// ContentStore returns a namespaced content store\n// proxied to a content store.\nfunc (m *DB) ContentStore() content.Store &#123;\n  if m.cs == nil &#123;\n    return nil\n  &#125;\n  return m.cs\n&#125;\n\n\n同样实现了content的很多方法,下面得了例子可以看到这里先读取数据库，然后在调用store.Status()\n\n// metadata/content.go\n\nfunc (cs *contentStore) Status(ctx context.Context, ref string) (content.Status, error) &#123;\n  ns, err := namespaces.NamespaceRequired(ctx)\n\n  var bref string\n  if err := view(ctx, cs.db, func(tx *bolt.Tx) error &#123;\n    bref = getRef(tx, ns, ref)\n    if bref == \"\" &#123;\n      return errors.Wrapf(errdefs.ErrNotFound, \"reference %v\", ref)\n    &#125;\n\n    return nil\n  &#125;); err != nil &#123;\n    return content.Status&#123;&#125;, err\n  &#125;\n\n  st, err := cs.Store.Status(ctx, bref)\n  if err != nil &#123;\n    return content.Status&#123;&#125;, err\n  &#125;\n  st.Ref = ref\n  return st, nil\n&#125;\n\ncontent类型\ncontent有2中实现,一种本地(local),一种prox(远程)\n\n\nlocal:就是本地实现,目前可以理解为真正实现\n\nproxy:则是调用远程的实现，因为content有插件\n\n注册则在loadPlugin()中首先会将本地的注册，随后读取配置文件中的proxy_plugin配置在注册proxy类型的，需要注意的是插件在整理之后会返回第一个可能导致你注册的content需要再配置文件中disabled_plugins参数关闭local强制使用proxy类型的\n\n\n// services/server/server.go\n\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.ContentPlugin,\n    ID:   \"content\",\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      ic.Meta.Exports[\"root\"] = ic.Root\n      return local.NewStore(ic.Root)\n    &#125;,\n  &#125;)\n\n  clients := &amp;proxyClients&#123;&#125;\n  for name, pp := range config.ProxyPlugins &#123;\n    var (\n      t plugin.Type\n      f func(*grpc.ClientConn) interface&#123;&#125;\n\n      address = pp.Address\n    )\n\n    // nsap逻辑\n\n    case string(plugin.ContentPlugin), \"content\":\n      t = plugin.ContentPlugin\n      f = func(conn *grpc.ClientConn) interface&#123;&#125; &#123;\n        return csproxy.NewContentStore(csapi.NewContentClient(conn))\n      &#125;\n    default:\n      log.G(ctx).WithField(\"type\", pp.Type).Warn(\"unknown proxy plugin type\")\n    &#125;\n\n    plugin.Register(&amp;plugin.Registration&#123;\n      Type: t,\n      ID:   name,\n      InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n        ic.Meta.Exports[\"address\"] = address\n        conn, err := clients.getClient(address)\n        if err != nil &#123;\n          return nil, err\n        &#125;\n        return f(conn), nil\n      &#125;,\n    &#125;)\n\n\n接口实现本质就是读取存储的文件一些信息,然后返回\n\n// content/local/store.go\n\n// status works like stat above except uses the path to the ingest.\nfunc (s *store) status(ingestPath string) (content.Status, error) &#123;\n  dp := filepath.Join(ingestPath, \"data\")\n  fi, err := os.Stat(dp)\n\n  ref, err := readFileString(filepath.Join(ingestPath, \"ref\"))\n\n  startedAt, err := readFileTimestamp(filepath.Join(ingestPath, \"startedat\"))\n \n  updatedAt, err := readFileTimestamp(filepath.Join(ingestPath, \"updatedat\"))\n \n  // because we don't write updatedat on every write, the mod time may\n  // actually be more up to date.\n  if fi.ModTime().After(updatedAt) &#123;\n    updatedAt = fi.ModTime()\n  &#125;\n\n  return content.Status&#123;\n    Ref:       ref,\n    Offset:    fi.Size(),\n    Total:     s.total(ingestPath),\n    UpdatedAt: updatedAt,\n    StartedAt: startedAt,\n  &#125;, nil\n&#125;\n","tags":["k8s","containerd"]},{"title":"containerd源码-snapshots","url":"/2023/11/03/containerd%E6%BA%90%E7%A0%81-snapshots/","content":"snapshots主要负责存储解压层之后的存储\n\n\n代码版本为v.17.5\n\nsnapshot和content的结构类似,其接口定义如下\n\n接口定义// snapshots/snapshotter.go\ntype Snapshotter interface &#123;\n  Stat(ctx context.Context, key string) (Info, error)\n  Update(ctx context.Context, info Info, fieldpaths ...string) (Info, error)\n  Usage(ctx context.Context, key string) (Usage, error)\n  Mounts(ctx context.Context, key string) ([]mount.Mount, error) // 只是返回了mount参数并没有真正的mount\n  Prepare(ctx context.Context, key, parent string, opts ...Opt) ([]mount.Mount, error) // 创建snap\n  View(ctx context.Context, key, parent string, opts ...Opt) ([]mount.Mount, error) // 和commit一样只不过是只读的\n  Commit(ctx context.Context, name, key string, opts ...Opt) error // 提交\n  Remove(ctx context.Context, key string) error // 删除\n  Walk(ctx context.Context, fn WalkFunc, filters ...string) error\n  Close() error\n&#125;\n\nsnapshot grpc类型\n注册插件，他依赖于service类型,同样实现了Register方法调用了grpc进行api注册服务\n\n// services/snapshots/service.go\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.GRPCPlugin,\n    ID:   \"snapshots\",\n    Requires: []plugin.Type&#123;\n      plugin.ServicePlugin,\n    &#125;,\n    InitFn: newService,\n  &#125;)\n&#125;\n\nfunc newService(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n  plugins, err := ic.GetByType(plugin.ServicePlugin)\n  if err != nil &#123;\n    return nil, err\n  &#125;\n  p, ok := plugins[services.SnapshotsService]\n  if !ok &#123;\n    return nil, errors.New(\"snapshots service not found\")\n  &#125;\n  i, err := p.Instance()\n  if err != nil &#123;\n    return nil, err\n  &#125;\n  ss := i.(map[string]snapshots.Snapshotter)\n  return &amp;service&#123;ss: ss&#125;, nil\n&#125;\n\nfunc (s *service) Register(gs *grpc.Server) error &#123;\n  snapshotsapi.RegisterSnapshotsServer(gs, s)\n  return nil\n&#125;\n\n\nsnapshot的service有个map,因为snapshotter有很多实现，比如默认的overlayfs还有devmapper等\n\ntype service struct &#123;\n  ss map[string]snapshots.Snapshotter\n&#125;\n\n\n我们看下其中一个api的实现，主要是处理grpc的请求和响应操作，需要注意的是传入的参数中有Snapshotterid,然后执行对应的snap的api,后面就到了service层处理\n\nfunc (s *service) Prepare(ctx context.Context, pr *snapshotsapi.PrepareSnapshotRequest) (*snapshotsapi.PrepareSnapshotResponse, error) &#123;\n  log.G(ctx).WithField(\"parent\", pr.Parent).WithField(\"key\", pr.Key).Debugf(\"prepare snapshot\")\n  sn, err := s.getSnapshotter(pr.Snapshotter)\n\n  var opts []snapshots.Opt\n  if pr.Labels != nil &#123;\n    opts = append(opts, snapshots.WithLabels(pr.Labels))\n  &#125;\n  mounts, err := sn.Prepare(ctx, pr.Key, pr.Parent, opts...)\n\n  return &amp;snapshotsapi.PrepareSnapshotResponse&#123;\n    Mounts: fromMounts(mounts),\n  &#125;, nil\n&#125;\n\nsnapshot service类型\n依赖MetadataPlugin类型吗,调用db.Snapshotters()拿到snap,meterdata里通过NewDB()传值\n\nfunc init() &#123;\n// services/snapshots/snapshotters.go\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.ServicePlugin,\n    ID:   services.SnapshotsService,\n    Requires: []plugin.Type&#123;\n      plugin.MetadataPlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      m, err := ic.Get(plugin.MetadataPlugin)\n\n      db := m.(*metadata.DB)\n      ss := make(map[string]snapshots.Snapshotter)\n      for n, sn := range db.Snapshotters() &#123;\n        ss[n] = newSnapshotter(sn, ic.Events)\n      &#125;\n      return ss, nil\n    &#125;,\n  &#125;)\n&#125;\n\n\n实际调用了metedata的Prepare(),这里进行了大量的数据库操作\n\nfunc (s *snapshotter) Prepare(ctx context.Context, key, parent string, opts ...snapshots.Opt) ([]mount.Mount, error) &#123;\n  mounts, err := s.Snapshotter.Prepare(ctx, key, parent, opts...)\n  if err := s.publisher.Publish(ctx, \"/snapshot/prepare\", &amp;eventstypes.SnapshotPrepare&#123;\n    Key:    key,\n    Parent: parent,\n  &#125;); err != nil &#123;\n    return nil, err\n  &#125;\n  return mounts, nil\n&#125;\n\n\n这可以可以看到prepare和view实现都是一样的只不过view是只读的\n源码很长这里不放了,其主要在数据存记录snap相关信息\n随后调用真正的snap实现\n\n// containerd/metadata/snapshot.go\n\nfunc (s *snapshotter) Prepare(ctx context.Context, key, parent string, opts ...snapshots.Opt) ([]mount.Mount, error) &#123;\n  return s.createSnapshot(ctx, key, parent, false, opts)\n&#125;\n\nfunc (s *snapshotter) View(ctx context.Context, key, parent string, opts ...snapshots.Opt) ([]mount.Mount, error) &#123;\n  return s.createSnapshot(ctx, key, parent, true, opts)\n&#125;\n\nfunc (s *snapshotter) createSnapshot(ctx context.Context, key, parent string, readonly bool, opts []snapshots.Opt) ([]mount.Mount, error) &#123;\n  // 校验参数以及复制等代码略过\n\n  if readonly &#123;\n    m, err = s.Snapshotter.View(ctx, bkey, bparent, bopts...)\n  &#125; else &#123;\n    m, err = s.Snapshotter.Prepare(ctx, bkey, bparent, bopts...)\n  &#125;\n&#125;\n\nsnapshot类型\n直接返回了overlay.NewSnapshotter()\n\n// snapshots/overlay/plugin/plugin.go\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type:   plugin.SnapshotPlugin,\n    ID:     \"overlayfs\",\n    Config: &amp;Config&#123;&#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      ic.Meta.Platforms = append(ic.Meta.Platforms, platforms.DefaultSpec())\n      config, ok := ic.Config.(*Config)\n\n      root := ic.Root\n      if config.RootPath != \"\" &#123;\n        root = config.RootPath\n      &#125;\n\n      ic.Meta.Exports[\"root\"] = root\n      return overlay.NewSnapshotter(root, overlay.AsynchronousRemove)\n    &#125;,\n  &#125;)\n&#125;\n\n\nNew函数中依然执行opt相关的操作,然后创建了目录随后创建数据库文件到这个目录中，注意这个数据库不metedata的数据库而是snap自己的数据库\n读取了一些overlay相关参数\n\n// snapshots/overlay/overlay.go\n\nfunc NewSnapshotter(root string, opts ...Opt) (snapshots.Snapshotter, error) &#123;\n  var config SnapshotterConfig\n  for _, opt := range opts &#123;\n    if err := opt(&amp;config); err != nil &#123;\n      return nil, err\n    &#125;\n  &#125;\n\n  if err := os.MkdirAll(root, 0700); err != nil &#123;\n\n\n  ms, err := storage.NewMetaStore(filepath.Join(root, \"metadata.db\"))\n\n  if err := os.Mkdir(filepath.Join(root, \"snapshots\"), 0700); err != nil &amp;&amp; !os.IsExist(err) &#123;\n    return nil, err\n  &#125;\n\n  // figure out whether \"userxattr\" option is recognized by the kernel &amp;&amp; needed\n  userxattr, err := overlayutils.NeedsUserXAttr(root)\n  if err != nil &#123;\n    logrus.WithError(err).Warnf(\"cannot detect whether \\\"userxattr\\\" option needs to be used, assuming to be %v\", userxattr)\n  &#125;\n\n  return &amp;snapshotter&#123;\n    root:        root,\n    ms:          ms,\n    asyncRemove: config.asyncRemove,\n    indexOff:    indexOff,\n    userxattr:   userxattr,\n  &#125;, nil\n&#125;\n\n\n和前面调用的PrePare结构很相似,也只是传递的是否只读不一样\n\nfunc (o *snapshotter) Prepare(ctx context.Context, key, parent string, opts ...snapshots.Opt) ([]mount.Mount, error) &#123;\n  return o.createSnapshot(ctx, snapshots.KindActive, key, parent, opts)\n&#125;\n\nfunc (o *snapshotter) View(ctx context.Context, key, parent string, opts ...snapshots.Opt) ([]mount.Mount, error) &#123;\n  return o.createSnapshot(ctx, snapshots.KindView, key, parent, opts)\n&#125;\n\n\n首先创建一个临时目录然后数据中创建snap记录,如果有parent则修改guid,然后修改名字为正式的snap目录\n最后通过mount函数返回\n\n// snapshots/overlay/overlay.go\n\nfunc (o *snapshotter) createSnapshot(ctx context.Context, kind snapshots.Kind, key, parent string, opts []snapshots.Opt) (_ []mount.Mount, err error) &#123;\n  if err := o.ms.WithTransaction(ctx, true, func(ctx context.Context) (err error) &#123;\n    td, err = o.prepareDirectory(ctx, snapshotDir, kind)\n\n    s, err = storage.CreateSnapshot(ctx, kind, key, parent, opts...)\n \n    if len(s.ParentIDs) > 0 &#123;\n      st, err := os.Stat(o.upperPath(s.ParentIDs[0]))\n\n      stat := st.Sys().(*syscall.Stat_t)\n      if err := os.Lchown(filepath.Join(td, \"fs\"), int(stat.Uid), int(stat.Gid)); err != nil &#123;&#125;\n    &#125;\n\n    path = filepath.Join(snapshotDir, s.ID)\n    if err = os.Rename(td, path); err != nil &#123;\n      return fmt.Errorf(\"failed to rename: %w\", err)\n    &#125;\n    td = \"\"\n\n    return nil\n  &#125;); err != nil &#123;\n    return nil, err\n  &#125;\n\n  return o.mounts(s), nil\n&#125;\n&#125;\n\n\nmount函数根据snapshotter的ParentIDs来判断是否返回读写的bind类型挂载\n通过判断是否是active来返回只读的bind类型挂载\n最后通过ParentIDs组合overlay的参数\n\n// snapshots/overlay/overlay.go\nfunc (o *snapshotter) mounts(s storage.Snapshot) []mount.Mount &#123;\n  if len(s.ParentIDs) == 0 &#123;\n    // if we only have one layer/no parents then just return a bind mount as overlay\n    // will not work\n    roFlag := \"rw\"\n    if s.Kind == snapshots.KindView &#123;\n      roFlag = \"ro\"\n    &#125;\n\n    return []mount.Mount&#123;\n      &#123;\n        Source: o.upperPath(s.ID),\n        Type:   \"bind\",\n        Options: []string&#123;\n          roFlag,\n          \"rbind\",\n        &#125;,\n      &#125;,\n    &#125;\n  &#125;\n\n  options := o.options\n  if s.Kind == snapshots.KindActive &#123;\n    options = append(options,\n      fmt.Sprintf(\"workdir=%s\", o.workPath(s.ID)),\n      fmt.Sprintf(\"upperdir=%s\", o.upperPath(s.ID)),\n    )\n  &#125; else if len(s.ParentIDs) == 1 &#123;\n    return []mount.Mount&#123;\n      &#123;\n        Source: o.upperPath(s.ParentIDs[0]),\n        Type:   \"bind\",\n        Options: []string&#123;\n          \"ro\",\n          \"rbind\",\n        &#125;,\n      &#125;,\n    &#125;\n  &#125;\n  parentPaths := make([]string, len(s.ParentIDs))\n  for i := range s.ParentIDs &#123;\n    parentPaths[i] = o.upperPath(s.ParentIDs[i])\n  &#125;\n  options = append(options, fmt.Sprintf(\"lowerdir=%s\", strings.Join(parentPaths, \":\")))\n  return []mount.Mount&#123;\n    &#123;\n      Type:    \"overlay\",\n      Source:  \"overlay\",\n      Options: options,\n    &#125;,\n  &#125;\n\n&#125;\n\nbind mount\nbind的mount类型是linux内核实现的一种挂载他和链接(link)实现的功能很像,但是他实现更底层在vfs之下\n参数rbind表示目录下的目录递归挂载到而不是这是这个一个,\nro则表示只读\n\n\n\nbind相当于修改了文件的inode到挂载的目录上\n\n\nmkdir test1 test2\nls -li\n# 总用量 0\n# 34260425 drwxr-xr-x 2 root root 6 10月 31 17:32 test1\n# 50339286 drwxr-xr-x 2 root root 6 10月 31 17:32 test2\n\nmount --bind ./test1 ./test2/\n\nll -ti\n# 总用量 0\n# 34260425 drwxr-xr-x 2 root root 6 10月 31 17:32 test1\n# 34260425 drwxr-xr-x 2 root root 6 10月 31 17:32 test2\n\necho \"foo\"> ./test1/test\ncat ./test2/test\n# foo\n\n# 显示的挂载是vda1而是test1\nmount -l |grep test\n# /dev/vda1 on /data/test/test2 type xfs (rw,relatime,attr2,inode64,noquota)\n\n参考https://blog.csdn.net/weixin_40864891/article/details/107330218\n","tags":["k8s","containerd"]},{"title":"containerd源码-下载镜像","url":"/2023/11/03/containerd%E6%BA%90%E7%A0%81-%E4%B8%8B%E8%BD%BD%E9%95%9C%E5%83%8F/","content":"前面介绍了插件注册以及启动,本次介绍下载镜像的过程\n\n\n代码版本为v.17.5\n\n主要依据ctr命令行工具的代码来阅读(cri还有套逻辑)，本文视角视角主要在客户端，服务端在后面的解析\n\n下载命令行处理\n老套路，直接点进去看app.New()\n\n// cmd/ctr/main.go\nfunc main() &#123;\n  app := app.New()\n  app.Commands = append(app.Commands, pluginCmds...)\n  if err := app.Run(os.Args); err != nil &#123;\n    fmt.Fprintf(os.Stderr, \"ctr: %s\\n\", err)\n    os.Exit(1)\n  &#125;\n&#125;\n\n\n这里面手机了很多子命令，我们主要关注images.Command\n\n// cmd/ctr/app/main.go\napp.Commands = append([]cli.Command&#123;\n    plugins.Command,\n    versionCmd.Command,\n    containers.Command,\n    content.Command,\n    events.Command,\n    images.Command,\n    leases.Command,\n    namespacesCmd.Command,\n    pprof.Command,\n    run.Command,\n    snapshots.Command,\n    tasks.Command,\n    install.Command,\n    ociCmd.Command,\n  &#125;, extraCmds...)\n\n\n依然是收集命令的结构，点击去查看pullCommand\n\n// cmd/ctr/commands/images/images.go\nvar Command = cli.Command&#123;\n  Name:    \"images\",\n  Aliases: []string&#123;\"image\", \"i\"&#125;,\n  Usage:   \"manage images\",\n  Subcommands: cli.Commands&#123;\n    checkCommand,\n    exportCommand,\n    importCommand,\n    listCommand,\n    mountCommand,\n    unmountCommand,\n    pullCommand,\n    pushCommand,\n    removeCommand,\n    tagCommand,\n    setLabelsCommand,\n    convertCommand,\n  &#125;,\n&#125;\n\n\n这里开始进入pull相关代码，主要在Action下,精简了下代码\n首先创建了一个客户端，然后客户端获取了一个lease\n最后开始下载，点进去Fetch()\n\n// cmd/ctr/commands/images/pull.go\n\nclient, ctx, cancel, err := commands.NewClient(context)\n    defer cancel()\n\n    ctx, done, err := client.WithLease(ctx)\n    defer done(ctx)\n\n    config, err := content.NewFetchConfig(ctx, context)\n\n    img, err := content.Fetch(ctx, client, ref, config)\n\n\nfetch\n开始处理了下是否使用TraceHTTP,然后创建了一个显示进度的，这个显示进度得就是使用ctr i pull时显示的进度，从这里可以看到另外开了个携程负责显示\n紧接着根据配置文件将相关操作放到opts这个切片中,最后调用client.Fetch()\n\ndocker.io/library/alpine:3.18.3:                                                  resolved       |++++++++++++++++++++++++++++++++++++++|\nindex-sha256:7144f7bab3d4c2648d7e59409f15ec52a18006a128c733fcff20d3a4a54ba44a:    done           |++++++++++++++++++++++++++++++++++++++|\nmanifest-sha256:c5c5fda71656f28e49ac9c5416b3643eaa6a108a8093151d6d1afc9463be8e33: done           |++++++++++++++++++++++++++++++++++++++|\nlayer-sha256:7264a8db6415046d36d16ba98b79778e18accee6ffa71850405994cffa9be7de:    done           |++++++++++++++++++++++++++++++++++++++|\nconfig-sha256:7e01a0d0a1dcd9e539f8e9bbd80106d59efbdf97293b3d38f5d7a34501526cdb:   done           |++++++++++++++++++++++++++++++++++++++|\nelapsed: 9.9 s                                                                    total:  3.1 Mi (322.3 KiB/s)\nunpacking linux/amd64 sha256:7144f7bab3d4c2648d7e59409f15ec52a18006a128c733fcff20d3a4a54ba44a...\ndone: 294.389123ms\n\n// cmd/ctr/commands/content/fetch.go\n\n// Fetch loads all resources into the content store and returns the image\nfunc Fetch(ctx context.Context, client *containerd.Client, ref string, config *FetchConfig) (images.Image, error) &#123;\nongoing := NewJobs(ref)\n\n  if config.TraceHTTP &#123;\n    ctx = httptrace.WithClientTrace(ctx, commands.NewDebugClientTrace(ctx))\n  &#125;\n\n  // 进度条\n  pctx, stopProgress := context.WithCancel(ctx)\n  progress := make(chan struct&#123;&#125;)\n\n  go func() &#123;\n    if config.ProgressOutput != nil &#123;\n      // no progress bar, because it hides some debug logs\n      ShowProgress(pctx, ongoing, client.ContentStore(), config.ProgressOutput)\n    &#125;\n    close(progress)\n  &#125;()\n\n  h := images.HandlerFunc(func(ctx context.Context, desc ocispec.Descriptor) ([]ocispec.Descriptor, error) &#123;\n    if desc.MediaType != images.MediaTypeDockerSchema1Manifest &#123;\n      ongoing.Add(desc)\n    &#125;\n    return nil, nil\n  &#125;)\n\n  log.G(pctx).WithField(\"image\", ref).Debug(\"fetching\")\n  labels := commands.LabelArgs(config.Labels)\n  opts := []containerd.RemoteOpt&#123;\n    containerd.WithPullLabels(labels),\n    containerd.WithResolver(config.Resolver),\n    containerd.WithImageHandler(h),\n    containerd.WithSchema1Conversion,\n  &#125;\n  opts = append(opts, config.RemoteOpts...)\n\n  if config.AllMetadata &#123;\n    opts = append(opts, containerd.WithAllMetadata())\n  &#125;\n\n  if config.PlatformMatcher != nil &#123;\n    opts = append(opts, containerd.WithPlatformMatcher(config.PlatformMatcher))\n  &#125; else &#123;\n    for _, platform := range config.Platforms &#123;\n      opts = append(opts, containerd.WithPlatform(platform))\n    &#125;\n  &#125;\n\n  img, err := client.Fetch(pctx, ref, opts...)\n  stopProgress()\n  if err != nil &#123;\n    return images.Image&#123;&#125;, err\n  &#125;\n\n  &lt;-progress\n  return img, nil\n&#125;\n\n\n这里首先创建一个fetchCtx并将将上面一些opts执行到fetcCtx中\n然后判断是不是下载时就进行解包，ctr命令里是先下载所有layer到content存储，然后在解压到快照服务，如果这个为真则下载一个layer就解压一个，在这里不支持边下载边解压\n随后根据配置来决定下载的平台()\n最终执行c.fetch()，点进去\n\n// client.go\n\nfunc (c *Client) Fetch(ctx context.Context, ref string, opts ...RemoteOpt) (images.Image, error) &#123;\n  fetchCtx := defaultRemoteContext() // 申明最终用于下载的组件\n  for _, o := range opts &#123;\n    if err := o(c, fetchCtx); err != nil &#123;\n      return images.Image&#123;&#125;, err\n    &#125;\n  &#125;\n\n  if fetchCtx.Unpack &#123;\n    return images.Image&#123;&#125;, errors.Wrap(errdefs.ErrNotImplemented, \"unpack on fetch not supported, try pull\")\n  &#125;\n\n  if fetchCtx.PlatformMatcher == nil &#123;\n    if len(fetchCtx.Platforms) == 0 &#123;\n      fetchCtx.PlatformMatcher = platforms.All\n    &#125; else &#123;\n      var ps []ocispec.Platform\n      for _, s := range fetchCtx.Platforms &#123;\n        p, err := platforms.Parse(s)\n        if err != nil &#123;\n          return images.Image&#123;&#125;, errors.Wrapf(err, \"invalid platform %s\", s)\n        &#125;\n        ps = append(ps, p)\n      &#125;\n      fetchCtx.PlatformMatcher = platforms.Any(ps...)\n    &#125;\n  &#125;\n\n  ctx, done, err := c.WithLease(ctx)\n  defer done(ctx)\n\n  img, err := c.fetch(ctx, fetchCtx, ref, 0)\n\n  return c.createNewImage(ctx, img)\n&#125;\n\n\n通过客户创建另一个cotent，为下载存储做准备\nrCtx.Resolver.Resolve()镜像名字来解析index等信息\n接下来都是根据解析出来的desc解析出来的类型判断是否需要需要转换格式，主要是早期docker格式的v1版本转换,\n随后对childerHadner进行变量的判断来处理\n所有的hander都放到handlers这个切片中，然后后丢给images.Handlers处理\nimages.Dispatch对desc遍历并递归调用每层都会经过上面的hander处理\n最终返回一个images.Image对象,到此下下载镜像完成\n\n// pull.go\n\nfunc (c *Client) fetch(ctx context.Context, rCtx *RemoteContext, ref string, limit int) (images.Image, error) &#123;\n  store := c.ContentStore()\n\n  name, desc, err := rCtx.Resolver.Resolve(ctx, ref)\n\n  fetcher, err := rCtx.Resolver.Fetcher(ctx, name)\n\n  var (\n    handler images.Handler\n    isConvertible bool\n    converterFunc func(context.Context, ocispec.Descriptor) (ocispec.Descriptor, error)\n    limiter       *semaphore.Weighted\n  )\n\n  if desc.MediaType == images.MediaTypeDockerSchema1Manifest &amp;&amp; rCtx.ConvertSchema1 &#123;\n    schema1Converter := schema1.NewConverter(store, fetcher)\n    handler = images.Handlers(append(rCtx.BaseHandlers, schema1Converter)...)\n    isConvertible = true\n    converterFunc = func(ctx context.Context, _ ocispec.Descriptor) (ocispec.Descriptor, error) &#123;\n      return schema1Converter.Convert(ctx)\n    &#125;\n  &#125; else &#123;\n    // Get all the children for a descriptor\n    childrenHandler := images.ChildrenHandler(store)\n    // Set any children labels for that content\n    childrenHandler = images.SetChildrenMappedLabels(store, childrenHandler, rCtx.ChildLabelMap)\n    if rCtx.AllMetadata &#123;\n      // Filter manifests by platforms but allow to handle manifest\n      // and configuration for not-target platforms\n      childrenHandler = remotes.FilterManifestByPlatformHandler(childrenHandler, rCtx.PlatformMatcher)\n    &#125; else &#123;\n      // Filter children by platforms if specified.\n      childrenHandler = images.FilterPlatforms(childrenHandler, rCtx.PlatformMatcher)\n    &#125;\n    // Sort and limit manifests if a finite number is needed\n    if limit > 0 &#123;\n      childrenHandler = images.LimitManifests(childrenHandler, rCtx.PlatformMatcher, limit)\n    &#125;\n\n    // set isConvertible to true if there is application/octet-stream media type\n    convertibleHandler := images.HandlerFunc(\n      func(_ context.Context, desc ocispec.Descriptor) ([]ocispec.Descriptor, error) &#123;\n        if desc.MediaType == docker.LegacyConfigMediaType &#123;\n          isConvertible = true\n        &#125;\n\n        return []ocispec.Descriptor&#123;&#125;, nil\n      &#125;,\n    )\n\n    appendDistSrcLabelHandler, err := docker.AppendDistributionSourceLabel(store, ref)\n\n    handlers := append(rCtx.BaseHandlers,\n      remotes.FetchHandler(store, fetcher), // 负责下载\n      convertibleHandler,\n      childrenHandler,\n      appendDistSrcLabelHandler,\n    )\n\n    handler = images.Handlers(handlers...)\n\n    converterFunc = func(ctx context.Context, desc ocispec.Descriptor) (ocispec.Descriptor, error) &#123;\n      return docker.ConvertManifest(ctx, store, desc)\n    &#125;\n  &#125;\n\n  if rCtx.HandlerWrapper != nil &#123;\n    handler = rCtx.HandlerWrapper(handler)\n  &#125;\n\n  if rCtx.MaxConcurrentDownloads > 0 &#123;\n    limiter = semaphore.NewWeighted(int64(rCtx.MaxConcurrentDownloads))\n  &#125;\n\n  // 递归调用\n  if err := images.Dispatch(ctx, handler, limiter, desc); err != nil &#123;\n    return images.Image&#123;&#125;, err\n  &#125;\n\n  if isConvertible &#123;\n    if desc, err = converterFunc(ctx, desc); err != nil &#123;\n      return images.Image&#123;&#125;, err\n    &#125;\n  &#125;\n\n  return images.Image&#123;\n    Name:   name,\n    Target: desc,\n    Labels: rCtx.Labels,\n  &#125;, nil\n&#125;\n\n\n其中主要下载的函数是remotes.FetchHandler(store, fetcher),这个函数首先判断MediaType,docker v1的直接报错报错返回主要看fetch\n\n// remotes/handlers.go\n\n// FetchHandler returns a handler that will fetch all content into the ingester\n// discovered in a call to Dispatch. Use with ChildrenHandler to do a full\n// recursive fetch.\nfunc FetchHandler(ingester content.Ingester, fetcher Fetcher) images.HandlerFunc &#123;\n  return func(ctx context.Context, desc ocispec.Descriptor) (subdescs []ocispec.Descriptor, err error) &#123;\n    ctx = log.WithLogger(ctx, log.G(ctx).WithFields(logrus.Fields&#123;\n      \"digest\":    desc.Digest,\n      \"mediatype\": desc.MediaType,\n      \"size\":      desc.Size,\n    &#125;))\n\n    switch desc.MediaType &#123;\n    case images.MediaTypeDockerSchema1Manifest:\n      return nil, fmt.Errorf(\"%v not supported\", desc.MediaType)\n    default:\n      err := fetch(ctx, ingester, fetcher, desc) // 真正用来干活的\n      return nil, err\n    &#125;\n  &#125;\n&#125;\n\n\n调用content.OpenWriter()创建了cw,通过错误判断是不是已经存在了\n调用Status()获取状态，然后判断是content-service中大小，如果相同则提交\n随后调用fetcher.Fetch()开始真正的下载内容然后通过流式拷贝到content.Writer()\n\nfunc fetch(ctx context.Context, ingester content.Ingester, fetcher Fetcher, desc ocispec.Descriptor) error &#123;\n  \n  cw, err := content.OpenWriter(ctx, ingester, content.WithRef(MakeRefKey(ctx, desc)), content.WithDescriptor(desc))\n  if err != nil &#123;\n    if errdefs.IsAlreadyExists(err) &#123;\n      return nil\n    &#125;\n    return err\n  &#125;\n  defer cw.Close()\n\n  ws, err := cw.Status()\n\n  if desc.Size == 0 &#123;\n    // most likely a poorly configured registry/web front end which responded with no\n    // Content-Length header; unable (not to mention useless) to commit a 0-length entry\n    // into the content store. Error out here otherwise the error sent back is confusing\n    return errors.Wrapf(errdefs.ErrInvalidArgument, \"unable to fetch descriptor (%s) which reports content size of zero\", desc.Digest)\n  &#125;\n  if ws.Offset == desc.Size &#123;\n    // If writer is already complete, commit and return\n    err := cw.Commit(ctx, desc.Size, desc.Digest)\n    if err != nil &amp;&amp; !errdefs.IsAlreadyExists(err) &#123;\n      return errors.Wrapf(err, \"failed commit on ref %q\", ws.Ref)\n    &#125;\n    return nil\n  &#125;\n\n  rc, err := fetcher.Fetch(ctx, desc) // 下载数据\n\n  defer rc.Close()\n\n  return content.Copy(ctx, cw, rc, desc.Size, desc.Digest) // 拷贝数据\n&#125;\n\n创建IMAGE\n镜像layer下载完成之后需要解压因此回到Fetch函数，执行完c.fetch之后根据返回的image对象创建了一个image,可以看到其实是根据客户端创建了一个ImagesService然后创建了个imags,\n\nfunc (c *Client) Fetch(ctx context.Context, ref string, opts ...RemoteOpt) (images.Image, error) &#123;\n  // 省略\n  img, err := c.fetch(ctx, fetchCtx, ref, 0)\n  return c.createNewImage(ctx, img)\n&#125;\n\n\nfunc (c *Client) createNewImage(ctx context.Context, img images.Image) (images.Image, error) &#123;\n  is := c.ImageService()\n  for &#123;\n    if created, err := is.Create(ctx, img); err != nil &#123;\n      if !errdefs.IsAlreadyExists(err) &#123;\n        return images.Image&#123;&#125;, err\n      &#125;\n\n      updated, err := is.Update(ctx, img)\n      if err != nil &#123;\n        // if image was removed, try create again\n        if errdefs.IsNotFound(err) &#123;\n          continue\n        &#125;\n        return images.Image&#123;&#125;, err\n      &#125;\n\n      img = updated\n    &#125; else &#123;\n      img = created\n    &#125;\n\n    return img, nil\n  &#125;\n&#125;\n\n解压\n回到Action中,根据content.Fetch中返回的img,然后根据配置参数是否全平台来遍历\n循环中调用了containerd.NewImageWithPlatform()创建了一个containerd.Image类型的i,然后调用i.Unpack()开始解压,\n\n// cmd/ctr/commands/images/pull.go\nAction: func(context *cli.Context) error &#123;\n// 省略\n\nimg, err := content.Fetch(ctx, client, ref, config)\n\nvar p []ocispec.Platform\n    if context.Bool(\"all-platforms\") &#123;\n      p, err = images.Platforms(ctx, client.ContentStore(), img.Target)\n      if err != nil &#123;\n        return errors.Wrap(err, \"unable to resolve image platforms\")\n      &#125;\n    &#125; else &#123;\n      for _, s := range context.StringSlice(\"platform\") &#123;\n        ps, err := platforms.Parse(s)\n        if err != nil &#123;\n          return errors.Wrapf(err, \"unable to parse platform %s\", s)\n        &#125;\n        p = append(p, ps)\n      &#125;\n    &#125;\n    if len(p) == 0 &#123;\n      p = append(p, platforms.DefaultSpec())\n    &#125;\n\n    start := time.Now()\n    for _, platform := range p &#123;\n      fmt.Printf(\"unpacking %s %s...\\n\", platforms.Format(platform), img.Target.Digest)\n      i := containerd.NewImageWithPlatform(client, img, platforms.Only(platform))\n\n      err = i.Unpack(ctx, context.String(\"snapshotter\"))\n\n      if context.Bool(\"print-chainid\") &#123;\n        diffIDs, err := i.RootFS(ctx)\n\n        chainID := identity.ChainID(diffIDs).String()\n        fmt.Printf(\"image chain ID: %s\\n\", chainID)\n      &#125;\n    &#125;\n    fmt.Printf(\"done: %s\\t\\n\", time.Since(start))\n    return nil\n&#125;\n\n\n首先获取一个lease,然后执行opt传参\n调用i.getManifest()获取manifest，调用i.getLayers()获取layers\n申明一个DiffService和ContentStore以及一个snapshotter\n遍历layers,开始调用rootfs.ApplyLayerWithOpts开始对每一层进行解压\n\n// image.go\nfunc (i *image) Unpack(ctx context.Context, snapshotterName string, opts ...UnpackOpt) error &#123;\n  ctx, done, err := i.client.WithLease(ctx)\n  defer done(ctx)\n\n  var config UnpackConfig\n  for _, o := range opts &#123;\n    if err := o(ctx, &amp;config); err != nil &#123;\n      return err\n    &#125;\n  &#125;\n\n  manifest, err := i.getManifest(ctx, i.platform)\n\n  layers, err := i.getLayers(ctx, i.platform, manifest)\n\n  var (\n    a  = i.client.DiffService()\n    cs = i.client.ContentStore()\n\n    chain    []digest.Digest\n    unpacked bool\n  )\n  snapshotterName, err = i.client.resolveSnapshotterName(ctx, snapshotterName)\n\n  sn, err := i.client.getSnapshotter(ctx, snapshotterName)\n\n  if config.CheckPlatformSupported &#123;\n    if err := i.checkSnapshotterSupport(ctx, snapshotterName, manifest); err != nil &#123;\n      return err\n    &#125;\n  &#125;\n\n  for _, layer := range layers &#123;\n    unpacked, err = rootfs.ApplyLayerWithOpts(ctx, layer, chain, sn, a, config.SnapshotOpts, config.ApplyOpts)\n    if unpacked &#123;\n      // Set the uncompressed label after the uncompressed\n      // digest has been verified through apply.\n      cinfo := content.Info&#123;\n        Digest: layer.Blob.Digest,\n        Labels: map[string]string&#123;\n          \"containerd.io/uncompressed\": layer.Diff.Digest.String(),\n        &#125;,\n      &#125;\n      if _, err := cs.Update(ctx, cinfo, \"labels.containerd.io/uncompressed\"); err != nil &#123;\n        return err\n      &#125;\n    &#125;\n\n    chain = append(chain, layer.Diff.Digest)\n  &#125;\n\n  desc, err := i.i.Config(ctx, cs, i.platform)\n\n  rootfs := identity.ChainID(chain).String()\n\n  cinfo := content.Info&#123;\n    Digest: desc.Digest,\n    Labels: map[string]string&#123;\n      fmt.Sprintf(\"containerd.io/gc.ref.snapshot.%s\", snapshotterName): rootfs,\n    &#125;,\n  &#125;\n\n  _, err = cs.Update(ctx, cinfo, fmt.Sprintf(\"labels.containerd.io/gc.ref.snapshot.%s\", snapshotterName))\n  return err\n&#125;\n\n\n这里会请求snapshot的Stst()查看状态，通过判断错误是不是已存在来决定是否进行下一步，若一切没问题则调用applyLayers()正式解压\n这里需要注意这个chainID,chainID是通过每个layer的sha256(sha256+sha256)计算得来，其中这些sha256就是config类型的layer中diff_ids\n\n// rootfs/apply.go\n\n// ApplyLayerWithOpts applies a single layer on top of the given provided layer chain,\n// using the provided snapshotter, applier, and apply opts. If the layer was unpacked true\n// is returned, if the layer already exists false is returned.\nfunc ApplyLayerWithOpts(ctx context.Context, layer Layer, chain []digest.Digest, sn snapshots.Snapshotter, a diff.Applier, opts []snapshots.Opt, applyOpts []diff.ApplyOpt) (bool, error) &#123;\n  var (\n    chainID = identity.ChainID(append(chain, layer.Diff.Digest)).String()\n    applied bool\n  )\n\n  if _, err := sn.Stat(ctx, chainID); err != nil &#123;\n    if !errdefs.IsNotFound(err) &#123;\n      return false, errors.Wrapf(err, \"failed to stat snapshot %s\", chainID)\n    &#125;\n\n    if err := applyLayers(ctx, []Layer&#123;layer&#125;, append(chain, layer.Diff.Digest), sn, a, opts, applyOpts); err != nil &#123;\n      if !errdefs.IsAlreadyExists(err) &#123;\n        return false, err\n      &#125;\n    &#125; else &#123;\n      applied = true\n    &#125;\n  &#125;\n  return applied, nil\n\n&#125;\n\n\n主要发送请求snapshots.Prepare()接口,获取到mount作为参数请求apply参数,随后snapshots.Commit()\n到此整个解压完成\n\nfunc applyLayers(ctx context.Context, layers []Layer, chain []digest.Digest, sn snapshots.Snapshotter, a diff.Applier, opts []snapshots.Opt, applyOpts []diff.ApplyOpt) error &#123;\n  var (\n    parent  = identity.ChainID(chain[:len(chain)-1])\n    chainID = identity.ChainID(chain)\n    layer   = layers[len(layers)-1]\n    diff    ocispec.Descriptor\n    key     string\n    mounts  []mount.Mount\n    err     error\n  )\n  \n  for &#123;\n    key = fmt.Sprintf(snapshots.UnpackKeyFormat, uniquePart(), chainID)// 生成请求的key格式\n\n    // Prepare snapshot with from parent, label as root\n    mounts, err = sn.Prepare(ctx, key, parent.String(), opts...)\n    if err != nil &#123;\n      if errdefs.IsNotFound(err) &amp;&amp; len(layers) > 1 &#123;\n        if err := applyLayers(ctx, layers[:len(layers)-1], chain[:len(chain)-1], sn, a, opts, applyOpts); err != nil &#123;\n          if !errdefs.IsAlreadyExists(err) &#123;\n            return err\n          &#125;\n        &#125;\n        // Do no try applying layers again\n        layers = nil\n        continue\n      &#125; else if errdefs.IsAlreadyExists(err) &#123;\n        // Try a different key\n        continue\n      &#125;\n\n      // Already exists should have the caller retry\n      return errors.Wrapf(err, \"failed to prepare extraction snapshot %q\", key)\n\n    &#125;\n    break\n  &#125;\n  defer func() &#123;\n    if err != nil &#123;\n      if !errdefs.IsAlreadyExists(err) &#123;\n        log.G(ctx).WithError(err).WithField(\"key\", key).Infof(\"apply failure, attempting cleanup\")\n      &#125;\n\n      if rerr := sn.Remove(ctx, key); rerr != nil &#123;\n        log.G(ctx).WithError(rerr).WithField(\"key\", key).Warnf(\"extraction snapshot removal failed\")\n      &#125;\n    &#125;\n  &#125;()\n\n  diff, err = a.Apply(ctx, layer.Blob, mounts, applyOpts...)\n  if err != nil &#123;\n    err = errors.Wrapf(err, \"failed to extract layer %s\", layer.Diff.Digest)\n    return err\n  &#125;\n  if diff.Digest != layer.Diff.Digest &#123;\n    err = errors.Errorf(\"wrong diff id calculated on extraction %q\", diff.Digest)\n    return err\n  &#125;\n\n  if err = sn.Commit(ctx, chainID.String(), key, opts...); err != nil &#123;\n    err = errors.Wrapf(err, \"failed to commit snapshot %s\", key)\n    return err\n  &#125;\n\n  return nil\n&#125;\n\n\n回到Unpack这里获取镜像的config通过chan计算出id并更新到content的标签中\n\nfunc (i *image) Unpack(ctx context.Context, snapshotterName string, opts ...UnpackOpt) error &#123;\n  // ...\n  desc, err := i.i.Config(ctx, cs, i.platform)\n  if err != nil &#123;\n    return err\n  &#125;\n\n  rootfs := identity.ChainID(chain).String()\n\n  cinfo := content.Info&#123;\n    Digest: desc.Digest,\n    Labels: map[string]string&#123;\n      fmt.Sprintf(\"containerd.io/gc.ref.snapshot.%s\", snapshotterName): rootfs,\n    &#125;,\n  &#125;\n\n  _, err = cs.Update(ctx, cinfo, fmt.Sprintf(\"labels.containerd.io/gc.ref.snapshot.%s\", snapshotterName))\n&#125;\n\nchanid计算\n下面这个diff_ids是从镜像uhub.service.ucloud.cn&#x2F;library&#x2F;nginx:1.9.7中的config截取\n\n&#123;\n\"linux\": &#123;\n  \"rootfs\": &#123;\n    \"type\": \"layers\",\n    \"diff_ids\": [\n      \"sha256:12e469267d21d66ac9dcae33a4d3d202ccb2591869270b95d0aad7516c7d075b\",\n      \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\",\n      \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\",\n      \"sha256:031458dc7254bd4da9c9ca8186b60aef311d0c921a846c6d2b281779035e2c7c\",\n      \"sha256:ebfc3a74f1601ad380e5e5a09738e952a5f86861a24e6efc00d0e03c0bd47d93\",\n      \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\",\n      \"sha256:673cf6d9dedba7cfb37ebd2c06f2373d16a29504976ca7e40335fb53e81cab16\",\n      \"sha256:40f240c1cbdb8a32ef21e2ec9154e65cc84027f238e453d69a7bb33246d6890b\",\n      \"sha256:0b3fbb980e2d51043bd23f9af674a536225fe023605cc485bac77dbb6111b433\",\n      \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\",\n      \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\",\n      \"sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\"\n      ]\n    &#125;\n  &#125;\n&#125;\n\n\n如果只有一层的话就是自己\n\n拿第一层的sha256的值加上空格加上第二层的sha256的值然后使用sha256,我们发现结果是a8118485e4e7548235fa8a00da06ecc21b31dea6bf5a7dd2eed99b47f70ed000\n\n\necho -n \"sha256:12e469267d21d66ac9dcae33a4d3d202ccb2591869270b95d0aad7516c7d075b sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\" |shasum -a 256\neb0cfd964b3fe37432b0bb666bd537ca1ea730cf517eb2d0d3783b952ad10204  -\n\n\n同样用上一层的chan_id加上第三层的id\n\necho -n \"sha256:eb0cfd964b3fe37432b0bb666bd537ca1ea730cf517eb2d0d3783b952ad10204 sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef\" |shasum -a 256\na8118485e4e7548235fa8a00da06ecc21b31dea6bf5a7dd2eed99b47f70ed000  -\n\n\n我们可以验证下,使用boltbrowser打开位于在/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/metadata.db的snap的数据库，如下图所示符合预期\n\n\n\n所谓diff_id是当前层和父层的变化,将所有的diff_id组合起来就是一个我们在dockerfile中编写的镜像\n\n总结sequenceDiagram\n    autonumber\n    participant hub as 仓库\n    participant client as 客户端\n    participant diff as diff-service\n    participant content as content-service\n    participant snapshotter as snapshotter-service\n    participant image as image-service\n\n    client-&gt;&gt;hub:获取镜像index\n    client-&gt;&gt;content:存储index信息(content.Writer)\n    client-&gt;&gt;hub:获取manifests\n    client-&gt;&gt;content:存储manifests信息(content.Writer)\n    client-&gt;&gt;hub:获取config\n    client-&gt;&gt;content:存储manifests信息(content.Writer)\n\n    loop 保存所有的layers\n    client-&gt;&gt;hub:下载镜像\n    client-&gt;&gt;content:获取状态(content.Status)\n    client-&gt;&gt;content:写入layer(content.Writer)\n    client-&gt;&gt;content:提交layer(content.Commit)\n    client-&gt;&gt;content:读取元信息(content.ReadAt)\n    end\n    client-&gt;&gt;image:创建镜像(image.Create)\n    #client-&gt;&gt;image:创建镜像(如果已经存在)(image.Update)\n\n    loop 遍历layer解压缩到snapshot\n    client-&gt;&gt;snapshotter:获取snap状态判断是否已存在(snapper.Status)\n    client-&gt;&gt;snapshotter:创建snap(snapper.Prepare)\n    client-&gt;&gt;diff:apply layer(diff.Apply)\n    diff-&gt;&gt;content:读取layers(content.ReaderAt)\n    diff-&gt;&gt;diff:写入解压后的layers(路径是prepare给的)\n    client-&gt;&gt;snapshotter:提交快照\n    end\n\n参考https://blog.csdn.net/alex_yangchuansheng/article/details/111829103https://www.myway5.com/index.php/2021/05/24/containerd-storagehttps://www.myway5.com/index.php/2021/05/18/container-imagehttps://www.myway5.com/index.php/2021/05/24/containerd-storage/https://github.com/containerd/containerd/blob/main/docs/content-flow.mdhttps://blog.csdn.net/weixin_40864891/article/details/107330218\n","tags":["k8s","containerd"]},{"title":"containerd源码-启动过程插件注册","url":"/2023/10/27/containerd%E6%BA%90%E7%A0%81-%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E6%8F%92%E4%BB%B6%E6%B3%A8%E5%86%8C/","content":"前面介绍了containerd的存储以及oci等,现在将从源码开蓝启动过程和插件的注册\n\n\n\n代码版本为v.17.5\n\n启动过程\n入口在&#x2F;cmd&#x2F;containerd&#x2F;main.go,这里申明了一个app并执行了run方法\n\nfunc main() &#123;\n  app := command.App()\n  if err := app.Run(os.Args); err != nil &#123;\n    fmt.Fprintf(os.Stderr, \"containerd: %s\\n\", err)\n    os.Exit(1)\n  &#125;\n&#125;\n\n\n在App函数中主要处理命令行参数以及启动grpc等服务器\n\n// cmd/containerd/main.go\n...\n  log.G(ctx).WithFields(logrus.Fields&#123;\n      \"version\":  version.Version,\n      \"revision\": version.Revision,\n  &#125;).Info(\"starting containerd\")\n\n  server, err := server.New(ctx, config)\n  if err != nil &#123;\n      return err\n  &#125;\n\n  // Launch as a Windows Service if necessary\n  if err := launchService(server, done); err != nil &#123;\n    logrus.Fatal(err)\n  &#125;\n\n\n然后就是主要的启动流程了，创建了server这个对象\n\n加载插件，加载了配种的procesor插件以及主要的插件\n\n\n// services/server/server.go\nplugins, err := LoadPlugins(ctx, config)\nif err != nil &#123;\n  return nil, err\n&#125;\n\nfor id, p := range config.StreamProcessors &#123;\n  diff.RegisterProcessor(diff.BinaryHandler(id, p.Returns, p.Accepts, p.Path, p.Args, p.Env)) // 注册 processor\n&#125;\n\n\n进去LoadPlugins函数,这里主要加载几个重要的创建，首先注册了content类型的插件，id是content\n\n// services/server/server.go\nplugin.Register(&amp;plugin.Registration&#123;\n  Type: plugin.ContentPlugin,\n  ID:   \"content\",\n  InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n    ic.Meta.Exports[\"root\"] = ic.Root\n    return local.NewStore(ic.Root)\n  &#125;,\n&#125;)\n\n\n\n然后注册了bolt插件，bolt主要负责bolt数据库相关的,初始化第一步获取一个content类型的插件，然后将之前的注册的sn全部放到snapshotters中,随后根据配置文件创建了一个bolt.Open函数创建一个bolt，传入bolt和snapshotters创建出metadata,metadata主要存储元数据底层是boltDB\n\n// services/server/server.go\nplugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.MetadataPlugin,\n    ID:   \"bolt\",\n    Requires: []plugin.Type&#123;\n        plugin.ContentPlugin,\n        plugin.SnapshotPlugin,\n    &#125;,\n    Config: &amp;srvconfig.BoltConfig&#123;\n      ContentSharingPolicy: srvconfig.SharingPolicyShared,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      // 返回所有content插件中第一个\n      cs, err := ic.Get(plugin.ContentPlugin)\n\n      snapshottersRaw, err := ic.GetByType(plugin.SnapshotPlugin)\n\n      snapshotters := make(map[string]snapshots.Snapshotter)\n      for name, sn := range snapshottersRaw &#123;\n      sn, err := sn.Instance()\n      if err != nil &#123;\n        if !plugin.IsSkipPlugin(err) &#123;\n          log.G(ic.Context).WithError(err).\n          Warnf(\"could not use snapshotter %v in metadata plugin\", name)\n        &#125;\n        continue\n      &#125;\n        snapshotters[name] = sn.(snapshots.Snapshotter)\n      &#125;\n      shared := true\n      ic.Meta.Exports[\"policy\"] = srvconfig.SharingPolicyShared\n      if cfg, ok := ic.Config.(*srvconfig.BoltConfig); ok &#123;\n        if cfg.ContentSharingPolicy != \"\" &#123;\n          if err := cfg.Validate(); err != nil &#123;\n            return nil, err\n          &#125;\n          if cfg.ContentSharingPolicy == srvconfig.SharingPolicyIsolated &#123;\n            ic.Meta.Exports[\"policy\"] = srvconfig.SharingPolicyIsolated\n            shared = false\n          &#125;\n\n          log.L.WithField(\"policy\", cfg.ContentSharingPolicy).Info(\"metadata content store policy set\")\n        &#125;\n      &#125;\n\n      path := filepath.Join(ic.Root, \"meta.db\")\n      ic.Meta.Exports[\"path\"] = path\n      // 创建bolt数据库\n      db, err := bolt.Open(path, 0644, nil)\n      if err != nil &#123;\n          return nil, err\n      &#125;\n\n      var dbopts []metadata.DBOpt\n      if !shared &#123;\n        dbopts = append(dbopts, metadata.WithPolicyIsolated)\n      &#125;\n      // 初始化metadata 插件\n      mdb := metadata.NewDB(db, cs.(content.Store), snapshotters, dbopts...)\n      if err := mdb.Init(ic.Context); err != nil &#123;\n        return nil, err\n      &#125;\n      return mdb, nil\n    &#125;,\n  &#125;)\n... \n\n\n最后读取了配置文件中插件的配置然后初始化插件,先判断插件类型，然后根据不同类型注册插件\n\n// services/server/server.go\n\nclients := &amp;proxyClients&#123;&#125;\n  for name, pp := range config.ProxyPlugins &#123;\n    var (\n      t plugin.Type\n      f func(*grpc.ClientConn) interface&#123;&#125;\n      address = pp.Address\n  )\n\n    switch pp.Type &#123;\n    case string(plugin.SnapshotPlugin), \"snapshot\":\n        t = plugin.SnapshotPlugin\n        ssname := name\n        f = func(conn *grpc.ClientConn) interface&#123;&#125; &#123;\n        return ssproxy.NewSnapshotter(ssapi.NewSnapshotsClient(conn), ssname)\n        &#125;\n\n    case string(plugin.ContentPlugin), \"content\":\n        t = plugin.ContentPlugin\n        f = func(conn *grpc.ClientConn) interface&#123;&#125; &#123;\n          return csproxy.NewContentStore(csapi.NewContentClient(conn))\n        &#125;\n    default:\n      log.G(ctx).WithField(\"type\", pp.Type).Warn(\"unknown proxy plugin type\")\n    &#125;\n\n    plugin.Register(&amp;plugin.Registration&#123;\n      Type: t,\n      ID:   name,\n      InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n        ic.Meta.Exports[\"address\"] = address\n        conn, err := clients.getClient(address)\n        if err != nil &#123;\n          return nil, err\n        &#125;\n        return f(conn), nil\n      &#125;,\n    &#125;)\n\n  &#125;\n\n\n最后根据插件的依赖以及类型排序以及根据配置文件过滤关闭的插件\n\n// services/server/server.go\nfilter := srvconfig.V2DisabledFilter\nif config.GetVersion() == 1 &#123;\n  filter = srvconfig.V1DisabledFilter\n&#125;\n// return the ordered graph for plugins\nreturn plugin.Graph(filter(config.DisabledPlugins)), nil\n\n\n回到server.go的New函数继续往下看，接下来就是初始化grpc，tcp已经ttrp服务器的初始化，然后构造Server结构体,同事根据配置文件获取到需要的开启的插件存取required中\n\n// services/server/server.go\nserverOpts := []grpc.ServerOption&#123;\n    grpc.UnaryInterceptor(grpc_prometheus.UnaryServerInterceptor),\n    grpc.StreamInterceptor(grpc_prometheus.StreamServerInterceptor),\n  &#125;\n  if config.GRPC.MaxRecvMsgSize > 0 &#123;\n    serverOpts = append(serverOpts, grpc.MaxRecvMsgSize(config.GRPC.MaxRecvMsgSize))\n  &#125;\n  if config.GRPC.MaxSendMsgSize > 0 &#123;\n    serverOpts = append(serverOpts, grpc.MaxSendMsgSize(config.GRPC.MaxSendMsgSize))\n  &#125;\n  ttrpcServer, err := newTTRPCServer()\n  if err != nil &#123;\n    return nil, err\n  &#125;\n  tcpServerOpts := serverOpts\n  if config.GRPC.TCPTLSCert != \"\" &#123;\n    log.G(ctx).Info(\"setting up tls on tcp GRPC services...\")\n    creds, err := credentials.NewServerTLSFromFile(config.GRPC.TCPTLSCert, config.GRPC.TCPTLSKey)\n    if err != nil &#123;\n      return nil, err\n    &#125;\n    tcpServerOpts = append(tcpServerOpts, grpc.Creds(creds))\n  &#125;\n  var (\n    grpcServer = grpc.NewServer(serverOpts...)\n    tcpServer  = grpc.NewServer(tcpServerOpts...)\n\n    grpcServices  []plugin.Service\n    tcpServices   []plugin.TCPService\n    ttrpcServices []plugin.TTRPCService\n\n    s = &amp;Server&#123;\n      grpcServer:  grpcServer,\n      tcpServer:   tcpServer,\n      ttrpcServer: ttrpcServer,\n      events:      exchange.NewExchange(),\n      config:      config,\n    &#125;\n    initialized = plugin.NewPluginSet()\n    required    = make(map[string]struct&#123;&#125;)\n  )\n  for _, r := range config.RequiredPlugins &#123;\n    required[r] = struct&#123;&#125;&#123;&#125;\n  &#125;\n\n\n接着根据上面LoadPlugins获取的插件开始遍历然后初始化\nplugin.NewContext构造初始化需要的参数\np.Init执行注册插件时候的初始化内容\ninitialized.Add将初始化过的插件加入到已经初始化的结构体中，为后面的插件掉用\nresult.Instance()获取一个插件实例然后查看是否有grpc等方法实现，有则放到到grpc等服务中\n同时将required中初始化了的插件删除\n\nfor _, p := range plugins &#123;\n  id := p.URI()\n  reqID := id\n  if config.GetVersion() == 1 &#123;\n    reqID = p.ID\n  &#125;\n  log.G(ctx).WithField(\"type\", p.Type).Infof(\"loading plugin %q...\", id)\n\n  initContext := plugin.NewContext(\n    ctx,\n    p,\n    initialized,\n    config.Root,\n    config.State,\n  )\n  initContext.Events = s.events\n  initContext.Address = config.GRPC.Address\n  initContext.TTRPCAddress = config.TTRPC.Address\n\n  // load the plugin specific configuration if it is provided\n  if p.Config != nil &#123;\n    pc, err := config.Decode(p)\n    if err != nil &#123;\n      return nil, err\n    &#125;\n    initContext.Config = pc\n  &#125;\n  // 执行初始化\n  result := p.Init(initContext)\n  // 将已经初始化的插件加入到initialized中，因为后面的插件可能依赖前面的插件\n  if err := initialized.Add(result); err != nil &#123;\n    return nil, errors.Wrapf(err, \"could not add plugin result to plugin set\")\n  &#125;\n\n  instance, err := result.Instance()\n  if err != nil &#123;\n    if plugin.IsSkipPlugin(err) &#123;\n      log.G(ctx).WithError(err).WithField(\"type\", p.Type).Infof(\"skip loading plugin %q...\", id)\n    &#125; else &#123;\n      log.G(ctx).WithError(err).Warnf(\"failed to load plugin %s\", id)\n    &#125;\n    if _, ok := required[reqID]; ok &#123;\n      return nil, errors.Wrapf(err, \"load required plugin %s\", id)\n    &#125;\n    continue\n  &#125;\n\n  delete(required, reqID)\n  // check for grpc services that should be registered with the server\n  if src, ok := instance.(plugin.Service); ok &#123;\n    grpcServices = append(grpcServices, src)\n  &#125;\n  if src, ok := instance.(plugin.TTRPCService); ok &#123;\n    ttrpcServices = append(ttrpcServices, src)\n  &#125;\n  if service, ok := instance.(plugin.TCPService); ok &#123;\n    tcpServices = append(tcpServices, service)\n  &#125;\n\n  s.plugins = append(s.plugins, result)\n&#125;\n\n\n完成插件初始化后首先判断required是否还有，确认已经需要加载的已经加载\n依次将初始化插件的相关service注册到对应的服务器中,然后返回server\n\nif len(required) != 0 &#123;\n    var missing []string\n    for id := range required &#123;\n      missing = append(missing, id)\n    &#125;\n    return nil, errors.Errorf(\"required plugin %s not included\", missing)\n  &#125;\n\n  // register services after all plugins have been initialized\n  for _, service := range grpcServices &#123;\n    if err := service.Register(grpcServer); err != nil &#123;\n      return nil, err\n    &#125;\n  &#125;\n  for _, service := range ttrpcServices &#123;\n    if err := service.RegisterTTRPC(ttrpcServer); err != nil &#123;\n      return nil, err\n    &#125;\n  &#125;\n  for _, service := range tcpServices &#123;\n    if err := service.RegisterTCP(tcpServer); err != nil &#123;\n      return nil, err\n    &#125;\n  &#125;\n  return s, nil\n\n\n至此containerd启动完成\n\n插件注册\n利用go的import将所有模块的init函数执行\n在路径在cmd/containerd中builtins开头的文件皆是如此\n\n// cmd/containerd/builtins_linux.go\npackage main\n\nimport (\n  _ \"github.com/containerd/containerd/metrics/cgroups\"\n  _ \"github.com/containerd/containerd/metrics/cgroups/v2\"\n  _ \"github.com/containerd/containerd/runtime/v1/linux\"\n  _ \"github.com/containerd/containerd/runtime/v2\"\n  _ \"github.com/containerd/containerd/runtime/v2/runc/options\"\n  _ \"github.com/containerd/containerd/snapshots/native/plugin\"\n  _ \"github.com/containerd/containerd/snapshots/overlay/plugin\"\n)\n\n\n比如说overlayfs,需要填充类型，ID配置然后是初始化动作并返回,service类型的插他依赖metadata类型的插件\n\n// /containerd/services/snapshots/snapshotters.go\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.ServicePlugin,\n    ID:   services.SnapshotsService,\n    Requires: []plugin.Type&#123;\n      plugin.MetadataPlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      m, err := ic.Get(plugin.MetadataPlugin)\n      if err != nil &#123;\n        return nil, err\n      &#125;\n\n      db := m.(*metadata.DB)\n      ss := make(map[string]snapshots.Snapshotter)\n      for n, sn := range db.Snapshotters() &#123;\n        ss[n] = newSnapshotter(sn, ic.Events)\n      &#125;\n      return ss, nil\n    &#125;,\n  &#125;)\n&#125;\n\n插件类型\n每一层都有负责的事情\ngrpc负责处理grpc协议请求\nsvc主要记录到数据中\n最后是真正干活的\n\n\n插件\n插件相关的代码在/plugin/中\n\nplugin.go\n其中plugin.go主要负责插件注册,以及定义插件类型\n\ntype Registration struct &#123;\n  // Type of the plugin\n  Type Type\n  // ID of the plugin\n  ID string\n  // Config specific to the plugin\n  Config interface&#123;&#125;\n  // Requires is a list of plugins that the registered plugin requires to be available\n  Requires []Type\n\n  // InitFn is called when initializing a plugin. The registration and\n  // context are passed in. The init function may modify the registration to\n  // add exports, capabilities and platform support declarations.\n  InitFn func(*InitContext) (interface&#123;&#125;, error)\n  // Disable the plugin from loading\n  Disable bool\n&#125;\n\nconst (\n  // InternalPlugin implements an internal plugin to containerd\n  InternalPlugin Type = \"io.containerd.internal.v1\"\n  // RuntimePlugin implements a runtime\n  RuntimePlugin Type = \"io.containerd.runtime.v1\"\n  // RuntimePluginV2 implements a runtime v2\n  RuntimePluginV2 Type = \"io.containerd.runtime.v2\"\n  // ServicePlugin implements a internal service\n  ServicePlugin Type = \"io.containerd.service.v1\"\n  // GRPCPlugin implements a grpc service\n  GRPCPlugin Type = \"io.containerd.grpc.v1\"\n  // SnapshotPlugin implements a snapshotter\n  SnapshotPlugin Type = \"io.containerd.snapshotter.v1\"\n  // TaskMonitorPlugin implements a task monitor\n  TaskMonitorPlugin Type = \"io.containerd.monitor.v1\"\n  // DiffPlugin implements a differ\n  DiffPlugin Type = \"io.containerd.differ.v1\"\n  // MetadataPlugin implements a metadata store\n  MetadataPlugin Type = \"io.containerd.metadata.v1\"\n  // ContentPlugin implements a content store\n  ContentPlugin Type = \"io.containerd.content.v1\"\n  // GCPlugin implements garbage collection policy\n  GCPlugin Type = \"io.containerd.gc.v1\"\n)\n\n\n\n其中各个插件都需要使用Register注册\n\n// Register allows plugins to register\nfunc Register(r *Registration) &#123;\n  register.Lock()\n  defer register.Unlock()\n\n  if r.Type == \"\" &#123;\n    panic(ErrNoType)\n  &#125;\n  if r.ID == \"\" &#123;\n    panic(ErrNoPluginID)\n  &#125;\n  if err := checkUnique(r); err != nil &#123;\n    panic(err)\n  &#125;\n\n  var last bool\n  for _, requires := range r.Requires &#123;\n    if requires == \"*\" &#123;\n      last = true\n    &#125;\n  &#125;\n  if last &amp;&amp; len(r.Requires) != 1 &#123;\n    panic(ErrInvalidRequires)\n  &#125;\n\n  register.r = append(register.r, r)\n&#125;\n\n\nGraph根据注册的插件的依赖关系生成一个有序的切片\n\nfunc Graph(filter DisableFilter) (ordered []*Registration) &#123;\n  register.RLock()\n  defer register.RUnlock()\n\n  for _, r := range register.r &#123;\n    if filter(r) &#123;\n      r.Disable = true\n    &#125;\n  &#125;\n\n  added := map[*Registration]bool&#123;&#125;\n\n  for _, r := range register.r &#123;\n    if r.Disable &#123;\n      continue\n    &#125;\n    children(r, added, &amp;ordered)\n    if !added[r] &#123;\n      ordered = append(ordered, r)\n      added[r] = true\n    &#125;\n  &#125;\n  return ordered\n&#125;\n\ncontext.go\ncontext.go主要负责时插件的上下文,Set这个结构体负责存放所有执行过初始化的插件\n\ntype Set struct &#123;\n  ordered     []*Plugin // order of initialization\n  byTypeAndID map[Type]map[string]*Plugin\n&#125;\n\n\nget根据类型获取插件，需要注意的是如果同一个类型有多个只返回第一个\n\n// Get returns the first plugin by its type\nfunc (i *InitContext) Get(t Type) (interface&#123;&#125;, error) &#123;\n  return i.plugins.Get(t)\n&#125;\n\n\n添加一个插件到集合里\n\n// Add a plugin to the set\nfunc (ps *Set) Add(p *Plugin) error &#123;\n  if byID, typeok := ps.byTypeAndID[p.Registration.Type]; !typeok &#123;\n    ps.byTypeAndID[p.Registration.Type] = map[string]*Plugin&#123;\n      p.Registration.ID: p,\n    &#125;\n  &#125; else if _, idok := byID[p.Registration.ID]; !idok &#123;\n    byID[p.Registration.ID] = p\n  &#125; else &#123;\n    return errors.Wrapf(errdefs.ErrAlreadyExists, \"plugin %v already initialized\", p.Registration.URI())\n  &#125;\n\n  ps.ordered = append(ps.ordered, p)\n  return nil\n&#125;\n\n总结\n整体的代码比较清晰的,主要的逻辑在server.New()中\n\nflowchart TD\n   注册插件--&gt;main --&gt; command.App --&gt; server.New[&quot;&#96;loadPlugins \n    init_plugin\n    注册grpc\n    &#96;&quot;]  --&gt; App.Run\n","tags":["k8s","containerd"]},{"title":"containerd源码-启动容器","url":"/2023/11/20/containerd%E6%BA%90%E7%A0%81-%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8/","content":"containerd得启动也分为服务端和客户端\n\n\n代码版本为v.17.5\n客户端\nctr启动一个pod有两种方式，一个是是run命令直接启动一个pod，还有一种先创建container，在创建task在启动,run命令指示把contaIner和task一块处理了\n\ncontainer create\n入口这里\n\n// cmd/ctr/commands/containers/containers.go\nvar createCommand = cli.Command&#123;\n  Name:      \"create\",\n  Usage:     \"create container\",\n  ArgsUsage: \"[flags] Image|RootFS CONTAINER [COMMAND] [ARG...]\",\n  Flags:     append(commands.SnapshotterFlags, commands.ContainerFlags...),\n  Action: func(context *cli.Context) error &#123;\n    // 参数处理\n    client, ctx, cancel, err := commands.NewClient(context)\n    if err != nil &#123;\n      return err\n    &#125;\n    defer cancel()\n    _, err = run.NewContainer(ctx, client, context)\n    if err != nil &#123;\n      return err\n    &#125;\n    return nil\n  &#125;,\n&#125;\n\n\n去除配置相关的主要是查看snapshotter中有没有解压如果没有则解压，然后将处理后的配置信息传递给client.NewContainer()\n\n// cmd/ctr/commands/run/run_unix.go\n\nfunc NewContainer(ctx gocontext.Context, client *containerd.Client, context *cli.Context) (containerd.Container, error) &#123;\n      // ...\n      i, err := client.ImageService().Get(ctx, ref)\n      if ps := context.String(\"platform\"); ps != \"\" &#123;\n        platform, err := platforms.Parse(ps)\n        image = containerd.NewImageWithPlatform(client, i, platforms.Only(platform))\n      &#125; else &#123;\n        image = containerd.NewImage(client, i)\n\n      unpacked, err := image.IsUnpacked(ctx, snapshotter)\n      if err != nil &#123;\n        return nil, err\n      &#125;\n      if !unpacked &#123;\n        if err := image.Unpack(ctx, snapshotter); err != nil &#123;\n          return nil, err\n        &#125;\n      &#125;\n\n  cOpts = append(cOpts, spec)\n\n  return client.NewContainer(ctx, id, cOpts...)\n&#125;\n\n\n随后将前面的参数传递到Container，然后调用grpc创建container\n\n// client.go\n\n\n// NewContainer will create a new container in container with the provided id\n// the id must be unique within the namespace\nfunc (c *Client) NewContainer(ctx context.Context, id string, opts ...NewContainerOpts) (Container, error) &#123;\n  ctx, done, err := c.WithLease(ctx)\n  defer done(ctx)\n\n  container := containers.Container&#123;\n    ID: id,\n    Runtime: containers.RuntimeInfo&#123;\n      Name: c.runtime,\n    &#125;,\n  &#125;\n  for _, o := range opts &#123;\n    if err := o(ctx, c, &amp;container); err != nil &#123;\n      return nil, err\n    &#125;\n  &#125;\n\n  r, err := c.ContainerService().Create(ctx, container)\n\n  return containerFromRecord(c, r), nil\n&#125;\n\ntask start\n前面创建完容器之后就需要创建一个task\n创建完task之后就启动task，这里处理detach这个参数如果有则不会退出之后删除\n\n// cmd/ctr/commands/tasks/start.go\n\nvar startCommand = cli.Command&#123;\n  Name:      \"start\",\n  Usage:     \"start a container that has been created\",\n  ArgsUsage: \"CONTAINER\",\n  Flags: []cli.Flag&#123;\n    cli.BoolFlag&#123;\n      Name:  \"null-io\",\n      Usage: \"send all IO to /dev/null\",\n    &#125;,\n    cli.StringFlag&#123;\n      Name:  \"log-uri\",\n      Usage: \"log uri\",\n    &#125;,\n    cli.StringFlag&#123;\n      Name:  \"fifo-dir\",\n      Usage: \"directory used for storing IO FIFOs\",\n    &#125;,\n    cli.StringFlag&#123;\n      Name:  \"pid-file\",\n      Usage: \"file path to write the task's pid\",\n    &#125;,\n    cli.BoolFlag&#123;\n      Name:  \"detach,d\",\n      Usage: \"detach from the task after it has started execution\",\n    &#125;,\n  &#125;,\n  Action: func(context *cli.Context) error &#123;\n    client, ctx, cancel, err := commands.NewClient(context)\n\n    defer cancel()\n    container, err := client.LoadContainer(ctx, id)\n\n    spec, err := container.Spec(ctx)\n    var con console.Console\n    if tty &#123;\n      con = console.Current()\n      defer con.Reset()\n      if err := con.SetRaw(); err != nil &#123;\n        return err\n      &#125;\n    &#125;\n\n    task, err := NewTask(ctx, client, container, \"\", con, context.Bool(\"null-io\"), context.String(\"log-uri\"), ioOpts, opts...)\n\n    var statusC &lt;-chan containerd.ExitStatus\n    if !detach &#123;\n      defer task.Delete(ctx)\n      if statusC, err = task.Wait(ctx); err != nil &#123;\n        return err\n      &#125;\n    &#125;\n    if context.IsSet(\"pid-file\") &#123;\n      if err := commands.WritePidFile(context.String(\"pid-file\"), int(task.Pid())); err != nil &#123;\n        return err\n      &#125;\n    &#125;\n    if err := task.Start(ctx); err != nil &#123;\n      return err\n    &#125;\n    if detach &#123;\n      return nil\n    &#125;\n\n    status := &lt;-statusC\n    code, _, err := status.Result()\n    if err != nil &#123;\n      return err\n    &#125;\n    if _, err := task.Delete(ctx); err != nil &#123;\n      return err\n    &#125;\n    if code != 0 &#123;\n      return cli.NewExitError(\"\", int(code))\n    &#125;\n    return nil\n  &#125;,\n&#125;\n\n\nNewTask处理了下命令行相关，然后调用了container.NewTask()\n\n// cmd/ctr/commands/tasks/tasks_unix.go\n\nfunc NewTask(ctx gocontext.Context, client *containerd.Client, container containerd.Container, checkpoint string, con console.Console, nullIO bool, logURI string, ioOpts []cio.Opt, opts ...containerd.NewTaskOpts) (containerd.Task, error) &#123;\n  stdinC := &amp;stdinCloser&#123;\n    stdin: os.Stdin,\n  &#125;\n  if checkpoint != \"\" &#123;\n    im, err := client.GetImage(ctx, checkpoint)\n\n    opts = append(opts, containerd.WithTaskCheckpoint(im))\n  &#125;\n  var ioCreator cio.Creator\n  if con != nil &#123;\n    if nullIO &#123;\n      return nil, errors.New(\"tty and null-io cannot be used together\")\n    &#125;\n    ioCreator = cio.NewCreator(append([]cio.Opt&#123;cio.WithStreams(con, con, nil), cio.WithTerminal&#125;, ioOpts...)...)\n  &#125; else if nullIO &#123;\n    ioCreator = cio.NullIO\n  &#125; else if logURI != \"\" &#123;\n    u, err := url.Parse(logURI)\n    ioCreator = cio.LogURI(u)\n  &#125; else &#123;\n    ioCreator = cio.NewCreator(append([]cio.Opt&#123;cio.WithStreams(stdinC, os.Stdout, os.Stderr)&#125;, ioOpts...)...)\n  &#125;\n  t, err := container.NewTask(ctx, ioCreator, opts...)\n\n  stdinC.closer = func() &#123;\n    t.CloseIO(ctx, containerd.WithStdinCloser)\n  &#125;\n  return t, nil\n&#125;\n\nrun命令\n基本就是将container create和task start的逻辑组合到一块\n\n// Command runs a container\nvar Command = cli.Command&#123;\n  Name:           \"run\",\n  Usage:          \"run a container\",\n  ArgsUsage:      \"[flags] Image|RootFS ID [COMMAND] [ARG...]\",\n  SkipArgReorder: true,\n  Flags: append([]cli.Flag&#123;\n    cli.BoolFlag&#123;\n      Name:  \"detach,d\",\n      Usage: \"detach from the task after it has started execution\",\n    &#125;,\n\n  &#125;, append(platformRunFlags, append(commands.SnapshotterFlags, commands.ContainerFlags...)...)...),\n  Action: func(context *cli.Context) error &#123;\n\n    client, ctx, cancel, err := commands.NewClient(context)\n  \n    defer cancel()\n    container, err := NewContainer(ctx, client, context)\n  \n    if context.Bool(\"rm\") &amp;&amp; !detach &#123;\n      defer container.Delete(ctx, containerd.WithSnapshotCleanup)\n    &#125;\n    var con console.Console\n    if tty &#123;\n      con = console.Current()\n      defer con.Reset()\n      if err := con.SetRaw(); err != nil &#123;\n        return err\n      &#125;\n    &#125;\n    var network gocni.CNI\n    if enableCNI &#123;\n      if network, err = gocni.New(gocni.WithDefaultConf); err != nil &#123;\n        return err\n      &#125;\n    &#125;\n\n    opts := getNewTaskOpts(context)\n    ioOpts := []cio.Opt&#123;cio.WithFIFODir(context.String(\"fifo-dir\"))&#125;\n    task, err := tasks.NewTask(ctx, client, container, context.String(\"checkpoint\"), con, context.Bool(\"null-io\"), context.String(\"log-uri\"), ioOpts, opts...)\n \n\n    var statusC &lt;-chan containerd.ExitStatus\n    if !detach &#123;\n      defer func() &#123;\n        if enableCNI &#123;\n          if err := network.Remove(ctx, fullID(ctx, container), \"\"); err != nil &#123;\n            logrus.WithError(err).Error(\"network review\")\n          &#125;\n        &#125;\n        task.Delete(ctx)\n      &#125;()\n\n      if statusC, err = task.Wait(ctx); err != nil &#123;\n        return err\n      &#125;\n    &#125;\n\n    if err := task.Start(ctx); err != nil &#123;\n      return err\n    &#125;\n    &#125;\n    if tty &#123;\n      if err := tasks.HandleConsoleResize(ctx, task, con); err != nil &#123;\n        logrus.WithError(err).Error(\"console resize\")\n      &#125;\n    &#125; else &#123;\n      sigc := commands.ForwardAllSignals(ctx, task)\n      defer commands.StopCatch(sigc)\n    &#125;\n    status := &lt;-statusC\n    code, _, err := status.Result()\n    if err != nil &#123;\n      return err\n    &#125;\n    if _, err := task.Delete(ctx); err != nil &#123;\n      return err\n    &#125;\n    if code != 0 &#123;\n      return cli.NewExitError(\"\", int(code))\n    &#125;\n    return nil\n  &#125;,\n&#125;\n\n服务端containercontainer grpc\n插件注册，依赖一个service\n\n// services/containers/service.go\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.GRPCPlugin,\n    ID:   \"containers\",\n    Requires: []plugin.Type&#123;\n      plugin.ServicePlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      plugins, err := ic.GetByType(plugin.ServicePlugin)\n      if err != nil &#123;\n        return nil, err\n      &#125;\n      p, ok := plugins[services.ContainersService]\n      if !ok &#123;\n        return nil, errors.New(\"containers service not found\")\n      &#125;\n      i, err := p.Instance()\n      if err != nil &#123;\n        return nil, err\n      &#125;\n      return &amp;service&#123;local: i.(api.ContainersClient)&#125;, nil\n    &#125;,\n  &#125;)\n&#125;\n\n\napi则直接调用了上层\n\nfunc (s *service) Create(ctx context.Context, req *api.CreateContainerRequest) (*api.CreateContainerResponse, error) &#123;\n  return s.local.Create(ctx, req)\n&#125;\n\ncontainer service\n插件注册\n\n// services/containers/local.go\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.ServicePlugin,\n    ID:   services.ContainersService,\n    Requires: []plugin.Type&#123;\n      plugin.MetadataPlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      m, err := ic.Get(plugin.MetadataPlugin)\n\n      db := m.(*metadata.DB)\n      return &amp;local&#123;\n        Store:     metadata.NewContainerStore(db),\n        db:        db,\n        publisher: ic.Events,\n      &#125;, nil\n    &#125;,\n  &#125;)\n&#125;\n\n\n主要调用了Store.Create()数据库中创建一个container,且上传了事件\n\n// services/containers/local.go\n\nfunc (l *local) Create(ctx context.Context, req *api.CreateContainerRequest, _ ...grpc.CallOption) (*api.CreateContainerResponse, error) &#123;\n  var resp api.CreateContainerResponse\n  if err := l.withStoreUpdate(ctx, func(ctx context.Context) error &#123;\n    container := containerFromProto(&amp;req.Container)\n    created, err := l.Store.Create(ctx, container)\n    resp.Container = containerToProto(&amp;created)\n  &#125;); err != nil &#123;\n    return &amp;resp, errdefs.ToGRPC(err)\n  &#125;\n  if err := l.publisher.Publish(ctx, \"/containers/create\", &amp;eventstypes.ContainerCreate&#123;\n    ID:    resp.Container.ID,\n    Image: resp.Container.Image,\n    Runtime: &amp;eventstypes.ContainerCreate_Runtime&#123;\n      Name:    resp.Container.Runtime.Name,\n      Options: resp.Container.Runtime.Options,\n    &#125;,\n  &#125;); err != nil &#123;\n    return &amp;resp, err\n  &#125;\n\n  return &amp;resp, nil\n&#125;\n\n\n看下数据中对于Container的实现\n\n// metadata/containers.go\n\n// NewContainerStore returns a Store backed by an underlying bolt DB\nfunc NewContainerStore(db *DB) containers.Store &#123;\n  return &amp;containerStore&#123;\n    db: db,\n  &#125;\n&#125;\n\n\n首先校验了了一下container,然后在数据库中创建一个记录\n\n// metadata/containers.go\n\nfunc (s *containerStore) Create(ctx context.Context, container containers.Container) (containers.Container, error) &#123;\n  namespace, err := namespaces.NamespaceRequired(ctx)\n  if err := validateContainer(&amp;container); err != nil &#123;\n    return containers.Container&#123;&#125;, errors.Wrap(err, \"create container failed validation\")\n  &#125;\n  if err := update(ctx, s.db, func(tx *bolt.Tx) error &#123;\n    bkt, err := createContainersBucket(tx, namespace)\n    cbkt, err := bkt.CreateBucket([]byte(container.ID))\n    if err != nil &#123;\n      if err == bolt.ErrBucketExists &#123;\n        err = errors.Wrapf(errdefs.ErrAlreadyExists, \"container %q\", container.ID)\n      &#125;\n      return err\n    &#125;\n    container.CreatedAt = time.Now().UTC()\n    container.UpdatedAt = container.CreatedAt\n    if err := writeContainer(cbkt, &amp;container); err != nil &#123;\n      return errors.Wrapf(err, \"failed to write container %q\", container.ID)\n    &#125;\n    return nil\n  &#125;); err != nil &#123;\n    return containers.Container&#123;&#125;, err\n  &#125;\n  return container, nil\n&#125;\n\ntasktask grpc\n注册依赖于service的插件\n\n// services/tasks/service.go\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.GRPCPlugin,\n    ID:   \"tasks\",\n    Requires: []plugin.Type&#123;\n      plugin.ServicePlugin,\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      plugins, err := ic.GetByType(plugin.ServicePlugin)\n      p, ok := plugins[services.TasksService]\n      if !ok &#123;\n        return nil, errors.New(\"tasks service not found\")\n      &#125;\n      i, err := p.Instance()\n      return &amp;service&#123;local: i.(api.TasksClient)&#125;, nil\n    &#125;,\n  &#125;)\n&#125;\n\n\napi直接调用上层的插件了\n\nfunc (s *service) Create(ctx context.Context, r *api.CreateTaskRequest) (*api.CreateTaskResponse, error) &#123;\n  return s.local.Create(ctx, r)\n&#125;\n\nfunc (s *service) Start(ctx context.Context, r *api.StartRequest) (*api.StartResponse, error) &#123;\n  return s.local.Start(ctx, r)\n&#125;\n\n\ntask serivce\n初始化的过程根据平台来选,分别有bsd，unix和win主要看unix\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type:     plugin.ServicePlugin,\n    ID:       services.TasksService,\n    Requires: tasksServiceRequires,\n    InitFn:   initFunc,\n  &#125;)\n\n  timeout.Set(stateTimeout, 2*time.Second)\n\n\nunix加载的插件\n\n// services/tasks/local_unix.go\nvar tasksServiceRequires = []plugin.Type&#123;\n  plugin.RuntimePlugin,\n  plugin.RuntimePluginV2,\n  plugin.MetadataPlugin,\n  plugin.TaskMonitorPlugin,\n&#125;\n\n\n根据平台调用loadV1Runtimes()加载runtimev1然后遍历\nruntimev2 则是通过ic.Get(plugin.RuntimePluginV2)通过插件的形式拿到\n\n// services/tasks/local.go\n\nfunc initFunc(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n  runtimes, err := loadV1Runtimes(ic)\n\n  v2r, err := ic.Get(plugin.RuntimePluginV2)\n\n  m, err := ic.Get(plugin.MetadataPlugin)\n\n  monitor, err := ic.Get(plugin.TaskMonitorPlugin)\n  if err != nil &#123;\n    if !errdefs.IsNotFound(err) &#123;\n      return nil, err\n    &#125;\n    monitor = runtime.NewNoopMonitor()\n  &#125;\n\n  db := m.(*metadata.DB)\n  l := &amp;local&#123;\n    runtimes:   runtimes,\n    containers: metadata.NewContainerStore(db),\n    store:      db.ContentStore(),\n    publisher:  ic.Events,\n    monitor:    monitor.(runtime.TaskMonitor),\n    v2Runtime:  v2r.(*v2.TaskManager),\n  &#125;\n  for _, r := range runtimes &#123;\n    tasks, err := r.Tasks(ic.Context, true)\n    for _, t := range tasks &#123;\n      l.monitor.Monitor(t)\n    &#125;\n  &#125;\n  v2Tasks, err := l.v2Runtime.Tasks(ic.Context, true)\n  for _, t := range v2Tasks &#123;\n    l.monitor.Monitor(t)\n  &#125;\n  return l, nil\n&#125;\n&#125;\n\n\n具体api实现方面，通过请求的容器id获取容器，处理下需要恢复路径，因为contaInerd重启容器并不会退出，所以需要contaIner找到之前的容器\n处理rootfs\n获取一个runtime.get() 获取一个task没然后执行创建task\n随后调用monitor.Monitor(c)监控容器\n\n// services/tasks/local.go\n\nfunc (l *local) Create(ctx context.Context, r *api.CreateTaskRequest, _ ...grpc.CallOption) (*api.CreateTaskResponse, error) &#123;\n  container, err := l.getContainer(ctx, r.ContainerID)\n  checkpointPath, err := getRestorePath(container.Runtime.Name, r.Options)\n\n  if checkpointPath == \"\" &amp;&amp; r.Checkpoint != nil &#123;\n    checkpointPath, err = ioutil.TempDir(os.Getenv(\"XDG_RUNTIME_DIR\"), \"ctrd-checkpoint\")\n \n    if r.Checkpoint.MediaType != images.MediaTypeContainerd1Checkpoint &#123;\n      return nil, fmt.Errorf(\"unsupported checkpoint type %q\", r.Checkpoint.MediaType)\n    &#125;\n    reader, err := l.store.ReaderAt(ctx, ocispec.Descriptor&#123;\n      MediaType:   r.Checkpoint.MediaType,\n      Digest:      r.Checkpoint.Digest,\n      Size:        r.Checkpoint.Size_,\n      Annotations: r.Checkpoint.Annotations,\n    &#125;)\n\n    _, err = archive.Apply(ctx, checkpointPath, content.NewReader(reader))\n    reader.Close()\n\n  &#125;\n  opts := runtime.CreateOpts&#123;\n    Spec: container.Spec,\n    IO: runtime.IO&#123;\n      Stdin:    r.Stdin,\n      Stdout:   r.Stdout,\n      Stderr:   r.Stderr,\n      Terminal: r.Terminal,\n    &#125;,\n    Checkpoint:     checkpointPath,\n    Runtime:        container.Runtime.Name,\n    RuntimeOptions: container.Runtime.Options,\n    TaskOptions:    r.Options,\n  &#125;\n  for _, m := range r.Rootfs &#123;\n    opts.Rootfs = append(opts.Rootfs, mount.Mount&#123;\n      Type:    m.Type,\n      Source:  m.Source,\n      Options: m.Options,\n    &#125;)\n  &#125;\n  if strings.HasPrefix(container.Runtime.Name, \"io.containerd.runtime.v1.\") &#123;\n    log.G(ctx).Warn(\"runtime v1 is deprecated since containerd v1.4, consider using runtime v2\")\n  &#125; else if container.Runtime.Name == plugin.RuntimeRuncV1 &#123;\n    log.G(ctx).Warnf(\"%q is deprecated since containerd v1.4, consider using %q\", plugin.RuntimeRuncV1, plugin.RuntimeRuncV2)\n  &#125;\n  rtime, err := l.getRuntime(container.Runtime.Name)\n  _, err = rtime.Get(ctx, r.ContainerID)\n  if err != nil &amp;&amp; err != runtime.ErrTaskNotExists &#123;\n    return nil, errdefs.ToGRPC(err)\n  &#125;\n  if err == nil &#123;\n    return nil, errdefs.ToGRPC(fmt.Errorf(\"task %s already exists\", r.ContainerID))\n  &#125;\n  c, err := rtime.Create(ctx, r.ContainerID, opts)\n  if err != nil &#123;\n    return nil, errdefs.ToGRPC(err)\n  &#125;\n  if err := l.monitor.Monitor(c); err != nil &#123;\n    return nil, errors.Wrap(err, \"monitor task\")\n  &#125;\n  return &amp;api.CreateTaskResponse&#123;\n    ContainerID: r.ContainerID,\n    Pid:         c.PID(),\n  &#125;, nil\n&#125;\n\n\nstart根据上一步创建的task获取进程然后启动进程\n\n// services/tasks/local.go\n\nfunc (l *local) Start(ctx context.Context, r *api.StartRequest, _ ...grpc.CallOption) (*api.StartResponse, error) &#123;\n  t, err := l.getTask(ctx, r.ContainerID)\n\n  p := runtime.Process(t)\n  if r.ExecID != \"\" &#123;\n    if p, err = t.Process(ctx, r.ExecID); err != nil &#123;\n      return nil, errdefs.ToGRPC(err)\n    &#125;\n  &#125;\n  if err := p.Start(ctx); err != nil &#123;\n    return nil, errdefs.ToGRPC(err)\n  &#125;\n  state, err := p.State(ctx)\n  if err != nil &#123;\n    return nil, errdefs.ToGRPC(err)\n  &#125;\n  return &amp;api.StartResponse&#123;\n    Pid: state.Pid,\n  &#125;, nil\n&#125;\n\nruntime\nruntime有2个版本现在普遍使用v2,他的初始化会根据平台传递一个config，最后拿到的参数传递给New()\n\n// runtime/v2/manager.go\n\nfunc init() &#123;\n  plugin.Register(&amp;plugin.Registration&#123;\n    Type: plugin.RuntimePluginV2,\n    ID:   \"task\",\n    Requires: []plugin.Type&#123;\n      plugin.MetadataPlugin,\n    &#125;,\n    Config: &amp;Config&#123;\n      Platforms: defaultPlatforms(),\n    &#125;,\n    InitFn: func(ic *plugin.InitContext) (interface&#123;&#125;, error) &#123;\n      supportedPlatforms, err := parsePlatforms(ic.Config.(*Config).Platforms)\n\n      ic.Meta.Platforms = supportedPlatforms\n      if err := os.MkdirAll(ic.Root, 0711); err != nil &#123;\n        return nil, err\n      &#125;\n      if err := os.MkdirAll(ic.State, 0711); err != nil &#123;\n        return nil, err\n      &#125;\n      m, err := ic.Get(plugin.MetadataPlugin)\n\n      cs := metadata.NewContainerStore(m.(*metadata.DB))\n\n      return New(ic.Context, ic.Root, ic.State, ic.Address, ic.TTRPCAddress, ic.Events, cs)\n    &#125;,\n  &#125;)\n&#125;\n\n\nNew()创建了文件夹和初始化了一个TaskManager,然后调用loadExistingTasks()方法加载已经存在的task\n\n// New task manager for v2 shims\nfunc New(ctx context.Context, root, state, containerdAddress, containerdTTRPCAddress string, events *exchange.Exchange, cs containers.Store) (*TaskManager, error) &#123;\n  for _, d := range []string&#123;root, state&#125; &#123;\n    if err := os.MkdirAll(d, 0711); err != nil &#123;\n      return nil, err\n    &#125;\n  &#125;\n  m := &amp;TaskManager&#123;\n    root:                   root,\n    state:                  state,\n    containerdAddress:      containerdAddress,\n    containerdTTRPCAddress: containerdTTRPCAddress,\n    tasks:                  runtime.NewTaskList(),\n    events:                 events,\n    containers:             cs,\n  &#125;\n  if err := m.loadExistingTasks(ctx); err != nil &#123;\n    return nil, err\n  &#125;\n  return m, nil\n&#125;\n\n\n最终通过shim创建容器，然后添加task\n\n// Create a new task\nfunc (m *TaskManager) Create(ctx context.Context, id string, opts runtime.CreateOpts) (_ runtime.Task, retErr error) &#123;\n  bundle, err := NewBundle(ctx, m.root, m.state, id, opts.Spec.Value)\n  defer func() &#123;\n    if retErr != nil &#123;\n      bundle.Delete()\n    &#125;\n  &#125;()\n\n  shim, err := m.startShim(ctx, bundle, id, opts)\n\n  defer func() &#123;\n    if retErr != nil &#123;\n      m.deleteShim(shim)\n    &#125;\n  &#125;()\n\n  t, err := shim.Create(ctx, opts)\n\n  if err := m.tasks.Add(ctx, t); err != nil &#123;\n    return nil, errors.Wrap(err, \"failed to add task\")\n  &#125;\n\n  return t, nil\n&#125;\n\n\n看了startShim实现。通过bundel等参数构造出一个binary,然后调用start()方法\n\n// runtime/v2/manager.go\n\nfunc (m *TaskManager) startShim(ctx context.Context, bundle *Bundle, id string, opts runtime.CreateOpts) (*shim, error) &#123;\n  ns, err := namespaces.NamespaceRequired(ctx)\n\n  topts := opts.TaskOptions\n\n  b := shimBinary(ctx, bundle, opts.Runtime, m.containerdAddress, m.containerdTTRPCAddress, m.events, m.tasks)\n  shim, err := b.Start(ctx, topts, func() &#123;\n    log.G(ctx).WithField(\"id\", id).Info(\"shim disconnected\")\n\n    cleanupAfterDeadShim(context.Background(), id, ns, m.tasks, m.events, b)\n    // Remove self from the runtime task list. Even though the cleanupAfterDeadShim()\n    // would publish taskExit event, but the shim.Delete() would always failed with ttrpc\n    // disconnect and there is no chance to remove this dead task from runtime task lists.\n    // Thus it's better to delete it here.\n    m.tasks.Delete(ctx, id)\n  &#125;)\n\n  return shim, nil\n&#125;\n\n\n这里通过client.Command()组装出命令然后启动程序\n随后创建一个ttrpc客户端返回\n\nfunc (b *binary) Start(ctx context.Context, opts *types.Any, onClose func()) (_ *shim, err error) &#123;\n  args := []string&#123;\"-id\", b.bundle.ID&#125;\n  if logrus.GetLevel() == logrus.DebugLevel &#123;\n    args = append(args, \"-debug\")\n  &#125;\n  args = append(args, \"start\")\n\n  cmd, err := client.Command(\n    ctx,\n    b.runtime,\n    b.containerdAddress,\n    b.containerdTTRPCAddress,\n    b.bundle.Path,\n    opts,\n    args...,\n  )\n\n  // Windows needs a namespace when openShimLog\n  ns, _ := namespaces.Namespace(ctx)\n  shimCtx, cancelShimLog := context.WithCancel(namespaces.WithNamespace(context.Background(), ns))\n  defer func() &#123;\n    if err != nil &#123;\n      cancelShimLog()\n    &#125;\n  &#125;()\n  f, err := openShimLog(shimCtx, b.bundle, client.AnonDialer)\n  if err != nil &#123;\n    return nil, errors.Wrap(err, \"open shim log pipe\")\n  &#125;\n  defer func() &#123;\n    if err != nil &#123;\n      f.Close()\n    &#125;\n  &#125;()\n  // open the log pipe and block until the writer is ready\n  // this helps with synchronization of the shim\n  // copy the shim's logs to containerd's output\n  go func() &#123;\n    defer f.Close()\n    _, err := io.Copy(os.Stderr, f)\n    // To prevent flood of error messages, the expected error\n    // should be reset, like os.ErrClosed or os.ErrNotExist, which\n    // depends on platform.\n    err = checkCopyShimLogError(ctx, err)\n    if err != nil &#123;\n      log.G(ctx).WithError(err).Error(\"copy shim log\")\n    &#125;\n  &#125;()\n  out, err := cmd.CombinedOutput()\n  if err != nil &#123;\n    return nil, errors.Wrapf(err, \"%s\", out)\n  &#125;\n  address := strings.TrimSpace(string(out))\n  conn, err := client.Connect(address, client.AnonDialer)\n  if err != nil &#123;\n    return nil, err\n  &#125;\n  onCloseWithShimLog := func() &#123;\n    onClose()\n    cancelShimLog()\n    f.Close()\n  &#125;\n  client := ttrpc.NewClient(conn, ttrpc.WithOnClose(onCloseWithShimLog))\n  return &amp;shim&#123;\n    bundle:  b.bundle,\n    client:  client,\n    task:    task.NewTaskClient(client),\n    events:  b.events,\n    rtTasks: b.rtTasks,\n  &#125;, nil\n&#125;\n\n总结sequenceDiagram\n    autonumber\n    participant client as 客户端\n    participant container as container-service\n    participant task as task-service\n    #participant content as content-service\n    #participant snapshotter as snapshotter-service\n    participant image as image-service\n    \n    client-&gt;&gt;image:获取image信息\n    client-&gt;&gt;container:创建容器\n    client-&gt;&gt;task:创建task\n    client-&gt;&gt;task:启动task\n\n参考资料http://blog.naturelr.cc\n","tags":["k8s","containerd"]},{"title":"determined-ai代码分析","url":"/2024/06/28/determined-ai%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/","content":"determined-ai是一个ai训练平台\n\n\n启动流程\n入口,使用了cobra作为命令行框架\n\n// master/cmd/determined-master/main.go\nfunc main() &#123;\n  logger.SetLogrus(*logger.DefaultConfig())\n\n  if err := rootCmd.Execute(); err != nil &#123;\n    log.WithError(err).Fatal(\"fatal error running Determined master\")\n  &#125;\n&#125;\n\n\n\n命令行处理,全局变量rootCmd由newRootCmd()实现,RUN下开始真正的执行,runRoot\n\n// master/cmd/determined-master/root.go\n\nvar rootCmd = newRootCmd()\n\nfunc newRootCmd() *cobra.Command &#123;\n  cmd := &amp;cobra.Command&#123;\n    Use: \"determined-master\",\n    Run: func(cmd *cobra.Command, args []string) &#123;\n      if err := runRoot(); err != nil &#123;\n        log.Error(fmt.Sprintf(\"%+v\", err))\n        os.Exit(1)\n      &#125;\n    &#125;,\n  &#125;\n&#125;\n\n\n处理日志和初始化配置文件以及必要的路径，做完这些准备之后就开始调用internal.New()初始化一个master\n\n然后执行run\n\n\n// master/cmd/determined-master/root.go\nfunc runRoot() error &#123;\n  logStore := logger.NewLogBuffer(logStoreSize)\n  log.AddHook(logStore)\n\n  err := initializeConfig()\n  config := config.GetMasterConfig()\n  printableConfig, err := config.Printable()\n  err = os.MkdirAll(config.Cache.CacheDir, 0o700)\n  m := internal.New(logStore, config)\n  return m.Run(context.TODO(), nil)\n&#125;\n\n\nRUN这里是整个程序的主体结构\n\n// master/internal/core.go\n\nfunc (m *Master) Run(ctx context.Context, gRPCLogInitDone chan struct&#123;&#125;) error &#123;\n    // 判断是否是新集群以创建密码，略过\n \n    // 设置数据库\n    m.db, err = db.Setup(&amp;m.config.DB, newClustersRequirePasswords)\n\n    // 设置webbhook\n    webhookManager, err := webhooks.New(ctx)\n    webhooks.SetDefault(webhookManager)\n\n    l, err := logpattern.New(ctx)\n    logpattern.SetDefault(l)\n\n\n    // 根据配置文件的资源管理\n    for _, r := range m.config.ResourceManagers() &#123;\n      err = m.checkIfRMDefaultsAreUnbound(r.ResourceManager)\n    &#125;\n\n    // 初始化用户服务\n    user.InitService(m.db, &amp;m.config.InternalConfig.ExternalSessions)\n    userService := user.GetService()\n\n    // 初始化代理\n    proxy.InitProxy(processProxyAuthentication)\n    portregistry.InitPortRegistry(config.GetMasterConfig().ReservedPorts)\n\n    // 初始化http服务\n    m.echo = echo.New()\n    \n    // 中间有些路由和中间件注册\n\n    // 初始化资源管理\n    if m.rm, err = buildRM(m.db, m.echo, m.config.ResourceManagers(),\n    &amp;m.config.TaskContainerDefaults,\n    &amp;aproto.MasterSetAgentOptions&#123;\n      MasterInfo:     m.Info(),\n      LoggingOptions: m.config.Logging,\n    &#125;,\n    cert,\n    )\n    // 设置jobservice\n    jobservice.SetDefaultService(m.rm)\n\n    // 命令服务初始化\n    cs, err := command.NewService(m.db, m.rm)\n    command.SetDefaultService(cs)\n\n    // 一些文档之类的路由注册\n\n    // 最后进入启动环节\n    return m.startServers(ctx, cert, gRPCLogInitDone)\n&#125;\n\n\n启动服务，这里需要注意使用cmux实现了一个监听接口同时可以http和grpc，且将grpc的一些api注册到http中 可以参考https://golang2.eddycjy.com/posts/ch3/06-grpc-http-support/\n\n// master/internal/core.go\n\n// baseListener是最终使用的不过在此之前先获取systemd的，如果没有用systemd则使用配置文件\nvar baseListener net.Listener\nsystemdListener, err := m.getSystemdListener()\nswitch &#123;\ncase err != nil:\ncase systemdListener != nil:\n    baseListener = systemdListener\n    port, pErr := m.findListeningPort(systemdListener)\n    m.config.Port = int(port)\ndefault:\n    baseListener, err = net.Listen(\"tcp\", fmt.Sprintf(\":%d\", m.config.Port))\n&#125;\n\n// 配置tls证书\nif cert != nil &#123;\n    // ...\n    baseListener = tls.NewListener(baseListener, &amp;tls.Config&#123;\n        Certificates:             []tls.Certificate&#123;*cert&#125;,\n        MinVersion:               tls.VersionTLS12,\n        PreferServerCipherSuites: true,\n        ClientCAs:                clientCAs,\n        ClientAuth:               clientAuthMode,\n    &#125;)\n&#125;\n\n\n// grpc服务创建，其中 apiServer实现了`proto.DeterminedServer`中所有的接口\n\n// This must be before grpcutil.RegisterHTTPProxy is called since it may use stuff set up by the\n// gRPC server (logger initialization, maybe more). Found by --race.\ngRPCServer := grpcutil.NewGRPCServer(m.db, &amp;apiServer&#123;m: m&#125;,\n    m.config.Observability.EnablePrometheus,\n    &amp;m.config.InternalConfig.ExternalSessions,\n    m.logs,\n)\n\n// 将grpc的中的接口注册到http中\nerr = grpcutil.RegisterHTTPProxy(ctx, m.echo, m.config.Port, cert)\n\n// 这里使用cumx多路服务器，来实现一个端口就时可以使用http和grpc服务\n// Initialize listeners and multiplexing.\nmux := cmux.New(baseListener)\n// 设置cmux匹配grp的条件\ngrpcListener := mux.MatchWithWriters(\n    cmux.HTTP2MatchHeaderFieldSendSettings(\"content-type\", \"application/grpc\"),\n)\ndefer closeWithErrCheck(\"grpc\", grpcListener)\n\n// 设置http 匹配协议\nhttpListener := mux.Match(cmux.HTTP1(), cmux.HTTP2())\ndefer closeWithErrCheck(\"http\", httpListener)\n\n// 启动服务并通过携程来传递错误\n// Start all servers and return the first error. This leaks a channel, but the complexity of\n// perfectly handling cleanup and all the error cases doesn't seem worth it for a function that is\n// called exactly once and causes the whole process to exit immediately when it returns.\nerrs := make(chan error)\nstart := func(name string, run func() error) &#123;\n    go func() &#123;\n        errs &lt;- errors.Wrap(run(), name+\" failed\")\n    &#125;()\n&#125;\nstart(\"gRPC server\", func() error &#123;\n    // We should defer srv.Stop() here, but cmux does not unblock accept calls when underlying\n    // listeners close and grpc-go depends on cmux unblocking and closing, Stop() blocks\n    // indefinitely when using cmux.\n    // To be fixed by https://github.com/soheilhy/cmux/pull/69 which makes cmux an io.Closer.\n    return gRPCServer.Serve(grpcListener)\n&#125;)\ndefer gRPCServer.Stop()\n\nstart(\"HTTP server\", func() error &#123;\n    m.echo.Listener = httpListener\n    m.echo.HidePort = true\n    m.echo.Server.ConnContext = connsave.SaveConn\n    return m.echo.StartServer(m.echo.Server)\n&#125;)\ndefer closeWithErrCheck(\"echo\", m.echo)\n\nstart(\"cmux listener\", mux.Serve)\n\n// 堵住 防止退出，只有接受到错误消息或者ctx.Done()才退出\nselect &#123;\ncase err := &lt;-errs:\n    return err\ncase &lt;-ctx.Done():\n    return ctx.Err()\n&#125;\n\n创建notebook分析流程\n上面启动流程中说了一些路由是通过grpc来实现的是需要实现，主要是实现proto.DeterminedServer这个接口启动LaunchNotebook()是创建Experiment的实现逻辑\n\n\n在protobuf中定义了接口，且定义了http的接口\n\n// Launch a notebook.\nrpc LaunchNotebook(LaunchNotebookRequest) returns (LaunchNotebookResponse) &#123;\n  option (google.api.http) = &#123;\n    post: \"/api/v1/notebooks\"\n    body: \"*\"\n  &#125;;\n  option (grpc.gateway.protoc_gen_swagger.options.openapiv2_operation) = &#123;\n    tags: \"Notebooks\"\n  &#125;;\n&#125;\n\n\ngoapiServer中则要实现\n\n// master/internal/api_notebook.go\n\nfunc (a *apiServer) LaunchNotebook(\n  ctx context.Context, req *apiv1.LaunchNotebookRequest,\n) (*apiv1.LaunchNotebookResponse, error) &#123;\n\n  launchReq, launchWarnings, err := a.getCommandLaunchParams(ctx, &amp;protoCommandParams&#123;\n  TemplateName: req.TemplateName,\n  WorkspaceID:  req.WorkspaceId,\n  Config:       req.Config,\n  Files:        req.Files,\n  &#125;, user)\n\n/*\n    中间都是一些参数处理launchReq\n*/\n\n  // Launch a Notebook.\n  // 拉起一个task和job类型都是是notebook的通用命令\n  genericCmd, err := command.DefaultCmdService.LaunchGenericCommand(\n    model.TaskTypeNotebook,\n    model.JobTypeNotebook,\n    launchReq)\n  if err != nil &#123;\n    return nil, err\n  &#125;\n\n  // 返回给前端\n  return &amp;apiv1.LaunchNotebookResponse&#123;\n    Notebook: genericCmd.ToV1Notebook(),\n    Config:   protoutils.ToStruct(launchReq.Spec.Config),\n    Warnings: pkgCommand.LaunchWarningToProto(launchWarnings),\n  &#125;, nil\n \n&#125;\n\n\nLaunchGenericCommand()\n\n// master/internal/command/command_service.go\n// LaunchGenericCommand creates NTSC commands and persists them to the database.\nfunc (cs *CommandService) LaunchGenericCommand(\n    taskType model.TaskType,\n    jobType model.JobType,\n    req *CreateGeneric,\n) (*Command, error) &#123;\n\n    // 省略掉一些创建id和传值的代码\n    cmd := &amp;Command&#123;\n        db: cs.db,\n        rm: cs.rm,\n        GenericCommandSpec: *req.Spec,\n        taskID:           taskID,\n        taskType:         taskType,\n        jobType:          jobType,\n        jobID:            jobID,\n        contextDirectory: req.ContextDirectory,\n        logCtx:           logCtx,\n        syslog:           logrus.WithFields(logrus.Fields&#123;\"component\": \"command\"&#125;).WithFields(logCtx.Fields()),\n    &#125;\n    // 开始启动,看下start方法\n    if err := cmd.Start(context.TODO()); err != nil &#123;\n        return nil, err\n    &#125;\n    // 启动完成之后将task保存\n    // Add it to the registry.\n    cs.commands[cmd.taskID] = cmd\n    return cmd, nil\n&#125;\n\n// master/internal/command/command.go\n\n// Start starts the command &amp; its respective allocation. Once started, it persists to the db.\nfunc (c *Command) Start(ctx context.Context) error &#123;\n\n    // 开始分配\n    err := task.DefaultService.StartAllocation(c.logCtx,\n        sproto.AllocateRequest&#123;\n            AllocationID:        c.allocationID,\n            TaskID:              c.taskID,\n            JobID:               c.jobID,\n            JobSubmissionTime:   c.registeredTime,\n            IsUserVisible:       true,\n            Name:                c.Config.Description,\n            SlotsNeeded:         c.Config.Resources.Slots,\n            ResourcePool:        c.Config.Resources.ResourcePool,\n            FittingRequirements: sproto.FittingRequirements&#123;SingleAgent: true&#125;,\n            ProxyPorts:          sproto.NewProxyPortConfig(c.GenericCommandSpec.ProxyPorts(), c.taskID),\n            IdleTimeout:         idleWatcherConfig,\n            Restore:             c.restored,\n            ProxyTLS:            c.TaskType == model.TaskTypeNotebook,\n        &#125;, c.db, c.rm, c.GenericCommandSpec, c.OnExit)\n\n    // Once the command is persisted to the dbs &amp; allocation starts, register it with the local job service.\n    // 注册到job server\n    jobservice.DefaultService.RegisterJob(c.jobID, c)\n\n    // 持久化到到数据库\n    if err := c.persist(); err != nil &#123;\n        c.syslog.WithError(err).Warnf(\"command persist failure\")\n    &#125;\n    return nil\n&#125;\n\n\nStartAllocation,是一个接口\n\n// master/internal/task/allocation_service.go\n\n// StartAllocation starts an allocation and returns a handle to it.\nfunc (as *allocationService) StartAllocation(\n    logCtx detLogger.Context,\n    req sproto.AllocateRequest,\n    db db.DB,\n    rm rm.ResourceManager,\n    specifier tasks.TaskSpecifier,\n    onExit func(*AllocationExited),\n) error &#123;\n/*\n...\n*/\n    // 随后进入分配环节\n    ref, err := newAllocation(logCtx, req, db, rm, specifier)\n    as.allocations[req.AllocationID] = ref\n    go func() &#123;\n        // 开启一个协程等待的资源结束\n        // 返回请求消息\n        _ = ref.awaitTermination()\n        ref.Cleanup()\n\n        as.mu.Lock()\n        delete(as.allocations, req.AllocationID)\n        as.mu.Unlock() // don't defer in case onExit calls back into the service\n\n        onExit(ref.exited)\n\n        as.syslog.Info(\"allocation cleaned up and removed from cache\")\n    &#125;()\n    return nil\n&#125;\n\n// master/internal/task/allocation.go\n\n\n// newAllocation returns a new allocation, which tracks allocation state in a fairly generic way.\nfunc newAllocation(\n    logCtx detLogger.Context, req sproto.AllocateRequest, db db.DB, rm rm.ResourceManager,\n    specifier tasks.TaskSpecifier,\n) (*allocation, error) &#123;\n    a := &amp;allocation&#123;\n        db: db,\n        rm: rm,\n        wg:     waitgroupx.WithContext(context.Background()),\n        syslog: logrus.WithFields(logCtx.Fields()),\n        req: req,\n        model: model.Allocation&#123;\n            AllocationID: req.AllocationID,\n            TaskID:       req.TaskID,\n            Slots:        req.SlotsNeeded,\n            ResourcePool: req.ResourcePool,\n            Ports:        map[string]int&#123;&#125;,\n        &#125;,\n        specifier: specifier,\n        resources: resourcesList&#123;&#125;,\n        logCtx: req.LogContext,\n    &#125;\n\n    // 请求资源\n    rmEvents, err := a.requestResources()\n    // 根据返回的rm事件运行\n    a.wg.Go(func(ctx context.Context) &#123; a.run(ctx, rmEvents) &#125;)\n    return a, nil\n&#125;\n\nrequestResources// master/internal/task/allocation.go\n\n// requestResources sets up the allocation.\nfunc (a *allocation) requestResources() (*sproto.ResourcesSubscription, error) &#123;\n    // 数据库保存\n    a.setModelState(model.AllocationStatePending)\n    if err := db.AddAllocation(context.TODO(), &amp;a.model); err != nil &#123;\n        return nil, errors.Wrap(err, \"saving trial allocation\")\n    &#125;\n    // 调用资源管理的Allocate方法，这也是个接口\n    sub, err := a.rm.Allocate(a.req)\n    return sub, nil\n&#125;\n\n\nAllocate\n\n// master/internal/rm/kubernetesrm/kubernetes_resource_manager.go\n\n// Allocate implements rm.ResourceManager.\nfunc (k *ResourceManager) Allocate(msg sproto.AllocateRequest) (*sproto.ResourcesSubscription, error) &#123;\n    // This code exists to handle the case where an experiment does not have\n    // an explicit resource pool specified in the config. This should never happen\n    // for newly created/forked experiments as the default pool is filled in to the\n    // config at creation time. However, old experiments which were created prior to\n    // the introduction of resource pools could have no resource pool associated with\n    // them and so we need to handle that case gracefully.\n\n    // 通过传入的资源池找到该资源池\n    rp, err := k.poolByName(msg.ResourcePool)\n\n    // 订阅事件这个AllocationID事件\n    sub := rmevents.Subscribe(msg.AllocationID)\n    fmt.Println(\"分配请求\")\n    // 分配请求的资源\n    rp.AllocateRequest(msg)\n    return sub, nil\n&#125;\n\n\nAllocateRequest\n\n// master/internal/rm/kubernetesrm/resource_pool.go\n\nfunc (k *kubernetesResourcePool) AllocateRequest(msg sproto.AllocateRequest) &#123;\n    k.mu.Lock()\n    defer k.mu.Unlock()\n    k.reschedule = true\n    // 添加一个task\n    k.addTask(msg)\n&#125;\n\n\naddTask\n\nfunc (k *kubernetesResourcePool) addTask(msg sproto.AllocateRequest) &#123;\n    if len(msg.AllocationID) == 0 &#123;\n        msg.AllocationID = model.AllocationID(uuid.New().String())\n    &#125;\n    k.getOrCreateGroup(msg.JobID)\n    if len(msg.Name) == 0 &#123;\n        msg.Name = \"Unnamed-k8-Task\"\n    &#125;\n\n    k.syslog.WithField(\"restore\", msg.Restore).Infof(\n        \"resources are requested by %s (Allocation ID: %s)\",\n        msg.Name, msg.AllocationID,\n    )\n    if msg.IsUserVisible &#123;\n        if _, ok := k.queuePositions[msg.JobID]; !ok &#123;\n            k.queuePositions[msg.JobID] = tasklist.InitializeQueuePosition(\n                msg.JobSubmissionTime,\n                true,\n            )\n        &#125;\n        k.jobIDToAllocationID[msg.JobID] = msg.AllocationID\n        k.allocationIDToJobID[msg.AllocationID] = msg.JobID\n        k.allocationIDToRunningPods[msg.AllocationID] = 0\n    &#125;\n    // 添加到 reqlist中\n    k.reqList.AddTask(&amp;msg)\n&#125;\n\n\n随后rp.Schedule将分配资源并发布消息，跳转至schedul\n\nrun\n回到newAllocation,requestResources执行完成之后开始run\n\n// master/internal/task/allocation.go\n\n    // 请求资源\n    rmEvents, err := a.requestResources()\n    // 根据返回的rm事件运行\n    a.wg.Go(func(ctx context.Context) &#123; a.run(ctx, rmEvents) &#125;)\n    return a, nil\n\n\nrun\n\n// master/internal/task/allocation.go\n\nfunc (a *allocation) run(ctx context.Context, sub *sproto.ResourcesSubscription) &#123;\n    for &#123;\n        // 循环获取sub事件\n        event, err := sub.GetWithContext(ctx)\n        if err != nil &#123;\n            // The following block is only used by tests to simulate a master crash by calling detach().\n            // It follows, though, no one should ever call detach() or wg.Cancel() in the code unless you are\n            // implementing graceful shutdown.\n            return\n        &#125;\n        // 处理获取的时间\n        done := a.HandleRMEvent(event)\n        if done &#123;\n            return\n        &#125;\n    &#125;\n&#125;\n\n\nHandleRMEvent\n\n// master/internal/task/allocation.go\n\n// HandleRMEvent handles downstream events from the resource manager.\nfunc (a *allocation) HandleRMEvent(msg sproto.ResourcesEvent) (done bool) &#123;\n    switch msg := msg.(type) &#123;\n    case *sproto.ResourcesAllocated:\n        // 资源创建事件处理\n        if err := a.resourcesAllocated(msg); err != nil &#123;\n            a.crash(err)\n        &#125;\n    case *sproto.ResourcesStateChanged:\n        a.resourcesStateChanged(msg)\n    case *sproto.ReleaseResources:\n        a.releaseResources(msg)\n    case *sproto.ContainerLog:\n        a.sendTaskLog(msg.ToTaskLog())\n    case *sproto.ResourcesRestoreError:\n        a.restoreResourceFailure(msg)\n        return true\n    case *sproto.InvalidResourcesRequestError:\n        a.crash(msg.Cause)\n        return true\n    case sproto.ResourcesReleasedEvent:\n        return true\n    default:\n        panic(fmt.Errorf(\"unexpected RM event\"))\n    &#125;\n    return false\n&#125;\n\n\nresourcesAllocated\n\n// master/internal/task/allocation.go\n\n// resourcesAllocated handles receiving resources from the resource manager. Note: it makes a single\n// ask to the parent to build its task spec.. this is mostly a hack to defer lots of computationally\n// heavy stuff unless it is necessarily (which also works to spread occurrences of the same work\n// out). Eventually, Allocations should just be started with their TaskSpec.\nfunc (a *allocation) resourcesAllocated(msg *sproto.ResourcesAllocated) error &#123;\n        for cID, r := range a.resources &#123;\n            // 启动函数这个也是个接口\n            if err := r.Start(a.logCtx, spec, sproto.ResourcesRuntimeInfo&#123;\n                Token:        token,\n                AgentRank:    a.resources[cID].Rank,\n                IsMultiAgent: len(a.resources) > 1,\n            &#125;); err != nil &#123;\n                return fmt.Errorf(\"starting resources (%v): %w\", r, err)\n            &#125;\n        &#125;\n\n\nStart\n\n// master/internal/rm/kubernetesrm/resource_pool.go\n\n// Start notifies the pods actor that it should launch a pod for the provided task spec.\nfunc (p k8sPodResources) Start(\n    logCtx logger.Context, spec tasks.TaskSpec, rri sproto.ResourcesRuntimeInfo,\n) error &#123;\n    // 调用podSservice的StartTaskPod\n    return p.podsService.StartTaskPod(StartTaskPod&#123;\n        Req:          p.req,\n        AllocationID: p.req.AllocationID,\n        Spec:         spec,\n        Slots:        p.slots,\n        Rank:         rri.AgentRank,\n        Namespace:    p.namespace,\n        LogContext:   logCtx,\n    &#125;)\n&#125;\n\n\nStartTaskPod\n\n// master/internal/rm/kubernetesrm/pods.go\n\nfunc (p *pods) StartTaskPod(msg StartTaskPod) error &#123;\n    p.mu.Lock()\n    defer p.mu.Unlock()\n    // 执行接受启动任务\n    return p.receiveStartTaskPod(msg)\n&#125;\n\n\nreceiveStartTaskPod\n\n// master/internal/rm/kubernetesrm/pods.go\nfunc (p *pods) receiveStartTaskPod(msg StartTaskPod) error &#123;\n    // podHandle启动pod\n    err := newPodHandler.start()\n    if err != nil &#123;\n        return fmt.Errorf(\"creating pod: %w\", err)\n    &#125;\n\n    return nil\n&#125;\n\n\nstart\n\nfunc (p *pod) start() error &#123;\n    if p.restore &#123;\n        if p.container.State == cproto.Running &#123;\n            err := p.startPodLogStreamer()\n        &#125;\n    &#125; else &#123;\n        // 创建pod并提交\n        if err := p.createPodSpecAndSubmit(); err != nil &#123;\n            return fmt.Errorf(\"creating pod spec: %w\", err)\n        &#125;\n    &#125;\n    return nil\n&#125;\n\n\ncreatePodSpecAndSubmit\n\n// master/internal/rm/kubernetesrm/pod.go\nfunc (p *pod) createPodSpecAndSubmit() error &#123;\n    // 创建k8spod的配置\n    if err := p.createPodSpec(p.scheduler); err != nil &#123;\n        return err\n    &#125;\n\n    // 调用资源请求队列的创建资源\n    p.resourceRequestQueue.createKubernetesResources(p.pod, p.configMap)\n    return nil\n&#125;\n\n\ncreateKubernetesResources\n\n// master/internal/rm/kubernetesrm/request_queue.go\nfunc (r *requestQueue) createKubernetesResources(\n    podSpec *k8sV1.Pod,\n    configMapSpec *k8sV1.ConfigMap,\n) &#123;\n    // 发送创建消息工作ch，资源申请创建完成，\n    select &#123;\n    case r.workerChan &lt;- msg:\n        r.creationInProgress.Insert(ref)\n    default:\n        queuedRequest := &amp;queuedResourceRequest&#123;createResources: &amp;msg&#125;\n        r.queue = append(r.queue, queuedRequest)\n        r.pendingResourceCreations[ref] = queuedRequest\n    &#125;\n&#125;\n\nbuiuldRM\n上面最终发给了一个workchan，workchan是在Run中的buuldRM中初始化\n\n// master/internal/core.go\n\nfunc buildRM(\n    db *db.PgDB,\n    echo *echo.Echo,\n    rmConfigs []*config.ResourceManagerWithPoolsConfig,\n    tcd *model.TaskContainerDefaultsConfig,\n    opts *aproto.MasterSetAgentOptions,\n    cert *tls.Certificate,\n) (rm.ResourceManager, error) &#123;\n    if len(rmConfigs) &lt;= 1 &#123;\n        config := rmConfigs[0]\n        switch &#123;\n        case config.ResourceManager.AgentRM != nil:\n            return agentrm.New(db, echo, config, opts, cert)\n        case config.ResourceManager.KubernetesRM != nil:\n            return kubernetesrm.New(db, config, tcd, opts, cert)\n        case config.ResourceManager.DispatcherRM != nil,\n            config.ResourceManager.PbsRM != nil:\n            license.RequireLicense(\"dispatcher resource manager\")\n            return dispatcherrm.New(db, echo, config, opts, cert)\n        default:\n            return nil, fmt.Errorf(\"no expected resource manager config is defined\")\n        &#125;\n    &#125;\n\n    return multirm.New(defaultRMName, rms), nil\n&#125;\n\n\nkubernetesrm.New\n\n// New returns a new ResourceManager, which communicates with\n// and submits work to a Kubernetes apiserver.\nfunc New(\n    db *db.PgDB,\n    rmConfigs *config.ResourceManagerWithPoolsConfig,\n    taskContainerDefaults *model.TaskContainerDefaultsConfig,\n    opts *aproto.MasterSetAgentOptions,\n    cert *tls.Certificate,\n) (*ResourceManager, error) &#123;\n\n    poolNamespaces := make(map[string]string)\n    for i := range k.poolsConfig &#123;\n        if k.poolsConfig[i].KubernetesNamespace == \"\" &#123;\n            k.poolsConfig[i].KubernetesNamespace = k.config.Namespace\n        &#125;\n\n        poolNamespaces[k.poolsConfig[i].KubernetesNamespace] = k.poolsConfig[i].PoolName\n    &#125;\n\n    // 创建一个新的podserver\n    k.podsService = newPodsService()\n\n    for _, poolConfig := range k.poolsConfig &#123;\n        poolConfig := poolConfig\n        rp := newResourcePool(maxSlotsPerPod, &amp;poolConfig, k.podsService, k.db)\n        go func() &#123;\n            // 隔一段时间就从处理 rqelist 中的创建任务\n            t := time.NewTicker(podSubmissionInterval)\n            defer t.Stop()\n            for range t.C &#123;\n                // 调度任务并发布消息\n                rp.Schedule()\n            &#125;\n        &#125;()\n        k.pools[poolConfig.PoolName] = rp\n    &#125;\n    return k, nil\n&#125;\n\nnewPodsService\nnewPodsService\n\n// master/internal/rm/kubernetesrm/pods.go\n\n// newPodsService creates a new pod service for launching, querying and interacting with k8s pods.\nfunc newPodsService(\n    namespace string,\n    namespaceToPoolName map[string]string,\n    masterServiceName string,\n    masterTLSConfig model.TLSClientConfig,\n    loggingConfig model.LoggingConfig,\n    scheduler string,\n    slotType device.Type,\n    slotResourceRequests config.PodSlotResourceRequests,\n    resourcePoolConfigs []config.ResourcePoolConfig,\n    taskContainerDefaults *model.TaskContainerDefaultsConfig,\n    detMasterIP string,\n    detMasterPort int32,\n    kubeconfigPath string,\n    podStatusUpdateCallback podStatusUpdateCallback,\n) *pods &#123;\n    loggingTLSConfig := masterTLSConfig\n    if loggingConfig.ElasticLoggingConfig != nil &#123;\n        loggingTLSConfig = loggingConfig.ElasticLoggingConfig.Security.TLS\n    &#125;\n    p := &amp;pods&#123;\n        wg: waitgroupx.WithContext(context.Background()),\n\n        namespace:                    namespace,\n        namespaceToPoolName:          namespaceToPoolName,\n        masterServiceName:            masterServiceName,\n        masterTLSConfig:              masterTLSConfig,\n        scheduler:                    scheduler,\n        loggingTLSConfig:             loggingTLSConfig,\n        loggingConfig:                loggingConfig,\n        podNameToPodHandler:          make(map[string]*pod),\n        podNameToResourcePool:        make(map[string]string),\n        containerIDToPodName:         make(map[string]string),\n        containerIDToSchedulingState: make(map[string]sproto.SchedulingState),\n        podNameToContainerID:         make(map[string]string),\n        podHandlerToMetadata:         make(map[*pod]podMetadata),\n        slotType:                     slotType,\n        slotResourceRequests:         slotResourceRequests,\n        resourcePoolConfigs:          resourcePoolConfigs,\n        baseContainerDefaults:        taskContainerDefaults,\n        detMasterIP:                  detMasterIP,\n        detMasterPort:                detMasterPort,\n        currentNodes:                 make(map[string]*k8sV1.Node),\n        nodeToSystemResourceRequests: make(map[string]int64),\n        podInterfaces:                make(map[string]typedV1.PodInterface),\n        configMapInterfaces:          make(map[string]typedV1.ConfigMapInterface),\n        syslog:                       logrus.WithField(\"namespace\", namespace),\n        podStatusUpdateCallback:      podStatusUpdateCallback,\n\n        kubeconfigPath: kubeconfigPath,\n    &#125;\n    // 初始化k8s客户端\n    if err := p.startClientSet(); err != nil &#123;\n    &#125;\n    if err := p.getMasterIPAndPort(); err != nil &#123;\n    &#125;\n    if err := p.getSystemResourceRequests(); err != nil &#123;\n    &#125;\n\n    // 启动资源请求队列\n    // 这里会创建一些woker 这些work监听workerChan发送过来的请求\n    p.startResourceRequestQueue()\n\n    // 启动pod的Informer\n    err := p.startPodInformer()\n    // 启动node的Informer\n    err = p.startNodeInformer()\n    switch &#123;\n    case err != nil &amp;&amp; k8error.IsForbidden(err):\n    case err != nil:\n        panic(err)\n    &#125;\n    // k8s 事件监听\n    err = p.startEventListeners()\n    err = p.startPreemptionListeners()\n    return p\n&#125;\n\n\nstartResourceRequestQueue\n\n// master/internal/rm/kubernetesrm/pods.go\n\n\nfunc (p *pods) startResourceRequestQueue() &#123;\n    failures := make(chan resourcesRequestFailure, 16)\n    // 启动请求队列\n    p.resourceRequestQueue = startRequestQueue(p.podInterfaces, p.configMapInterfaces, failures)\n    p.wg.Go(func(ctx context.Context) &#123;\n        for &#123;\n            select &#123;\n            case failure := &lt;-failures:\n                // 处理情况\n                p.handleResourceRequestFailure(failure)\n            case &lt;-ctx.Done():\n                return\n            &#125;\n        &#125;\n    &#125;)\n&#125;\n\n// master/internal/rm/kubernetesrm/pods.go\n\nfunc startRequestQueue(\n    podInterfaces map[string]typedV1.PodInterface,\n    configMapInterfaces map[string]typedV1.ConfigMapInterface,\n    failures chan&lt;- resourcesRequestFailure,\n) *requestQueue &#123;\n    r := &amp;requestQueue&#123;\n        podInterfaces:       podInterfaces,\n        configMapInterfaces: configMapInterfaces,\n        failures:            failures,\n\n        workerChan: make(chan interface&#123;&#125;),\n\n        queue: make([]*queuedResourceRequest, 0),\n\n        creationInProgress:       make(set.Set[requestID]),\n        pendingResourceCreations: make(map[requestID]*queuedResourceRequest),\n        blockedResourceDeletions: make(map[requestID]*queuedResourceRequest),\n\n        syslog: logrus.New().WithField(\"component\", \"kubernetesrm-queue\"),\n    &#125;\n    // 启动workers\n    r.startWorkers()\n    return r\n&#125;\n\n\nstartWorkers\n\n// master/internal/rm/kubernetesrm/pods.go\n\nfunc (r *requestQueue) startWorkers() &#123;\n    // 根据numKubernetesWorkers来开启worker\n    for i := 0; i &lt; numKubernetesWorkers; i++ &#123;\n        startRequestProcessingWorker(\n            r.podInterfaces,\n            r.configMapInterfaces,\n            strconv.Itoa(i),\n            r.workerChan,\n            r.workerReady,\n            r.failures,\n        )\n    &#125;\n&#125;\n\n\nstartRequestProcessingWorker\n\n// master/internal/rm/kubernetesrm/request_workers.go\nfunc startRequestProcessingWorker(\n    podInterfaces map[string]typedV1.PodInterface,\n    configMapInterfaces map[string]typedV1.ConfigMapInterface,\n    id string,\n    in &lt;-chan interface&#123;&#125;,\n    ready readyCallbackFunc,\n    failures chan&lt;- resourcesRequestFailure,\n) *requestProcessingWorker &#123;\n    syslog := logrus.New().WithField(\"component\", \"kubernetesrm-worker\").WithField(\"id\", id)\n    r := &amp;requestProcessingWorker&#123;\n        podInterfaces:       podInterfaces,\n        configMapInterfaces: configMapInterfaces,\n        failures:            failures,\n        syslog:              syslog,\n    &#125;\n    // 接受请求并处理\n    go r.receive(in, ready)\n    return r\n&#125;\n\n\nreceive\n\nfunc (r *requestProcessingWorker) receive(in &lt;-chan interface&#123;&#125;, ready readyCallbackFunc) &#123;\n    go ready(\"\")\n    for msg := range in &#123;\n        switch msg := msg.(type) &#123;\n        case createKubernetesResources:\n            // 创建资源事件\n            r.receiveCreateKubernetesResources(msg)\n            go ready(keyForCreate(msg))\n        case deleteKubernetesResources:\n            r.receiveDeleteKubernetesResources(msg)\n            go ready(\"\")\n        default:\n            errStr := fmt.Sprintf(\"unexpected message %T\", msg)\n            r.syslog.Error(errStr)\n            panic(errStr)\n        &#125;\n    &#125;\n&#125;\n\n\n\nreceiveCreateKubernetesResources,到这里完成整个资源的创建\n\n// master/internal/rm/kubernetesrm/request_workers.go\n\nfunc (r *requestProcessingWorker) receiveCreateKubernetesResources(\n    msg createKubernetesResources,\n) &#123;\n    r.syslog.Debugf(\"creating configMap with spec %v\", msg.configMapSpec)\n    // 创建configmap\n    configMap, err := r.configMapInterfaces[msg.podSpec.Namespace].Create(\n        context.TODO(), msg.configMapSpec, metaV1.CreateOptions&#123;&#125;)\n    if err != nil &#123;\n        r.syslog.WithError(err).Errorf(\"error creating configMap %s\", msg.configMapSpec.Name)\n        r.failures &lt;- resourceCreationFailed&#123;podName: msg.podSpec.Name, err: err&#125;\n        return\n    &#125;\n    r.syslog.Infof(\"created configMap %s\", configMap.Name)\n\n    r.syslog.Debugf(\"launching pod with spec %v\", msg.podSpec)\n    // 创建pod\n    pod, err := r.podInterfaces[msg.podSpec.Namespace].Create(\n        context.TODO(), msg.podSpec, metaV1.CreateOptions&#123;&#125;,\n    )\n    if err != nil &#123;\n        r.syslog.WithError(err).Errorf(\"error creating pod %s\", msg.podSpec.Name)\n        r.failures &lt;- resourceCreationFailed&#123;podName: msg.podSpec.Name, err: err&#125;\n        return\n    &#125;\n    r.syslog.Infof(\"created pod %s\", pod.Name)\n&#125;\n\nSchedule\n将定期处理reqlist中的任务\n\n// New returns a new ResourceManager, which communicates with\n// and submits work to a Kubernetes apiserver.\nfunc New(\n    db *db.PgDB,\n    rmConfigs *config.ResourceManagerWithPoolsConfig,\n    taskContainerDefaults *model.TaskContainerDefaultsConfig,\n    opts *aproto.MasterSetAgentOptions,\n    cert *tls.Certificate,\n) (*ResourceManager, error) &#123;\n/*\n\n*/\n        go func() &#123;\n            t := time.NewTicker(podSubmissionInterval)\n            defer t.Stop()\n            for range t.C &#123;\n                // 这里处理retlist\n                rp.Schedule()\n            &#125;\n        &#125;()\n&#125;\n\n\nSchedule\n\nfunc (k *kubernetesResourcePool) Schedule() &#123;\n    k.mu.Lock()\n    defer k.mu.Unlock()\n\n    if k.reschedule &#123;\n        // 调度等待的任务\n        k.schedulePendingTasks()\n    &#125;\n    k.reschedule = false\n&#125;\n\n\n\nschedulePendingTasks\n\nfunc (k *kubernetesResourcePool) schedulePendingTasks() &#123;\n    // 遍历 reqList中的所有任务\n    for it := k.reqList.Iterator(); it.Next(); &#123;\n        req := it.Value()\n        group := k.groups[req.JobID]\n        if group == nil &#123;\n            k.syslog.Warnf(\"schedulePendingTasks cannot find group for job %s\", req.JobID)\n            continue\n        &#125;\n        if !k.reqList.IsScheduled(req.AllocationID) &#123;\n            if maxSlots := group.MaxSlots; maxSlots != nil &#123;\n                if k.slotsUsedPerGroup[group]+req.SlotsNeeded > *maxSlots &#123;\n                    continue\n                &#125;\n            &#125;\n            // 分配资源\n            k.assignResources(req)\n        &#125;\n    &#125;\n&#125;\n\n\n\nassignResources\n\nfunc (k *kubernetesResourcePool) assignResources(\n    req *sproto.AllocateRequest,\n) &#123;\n\n/*\n    分配资源逻辑\n*/\n\n    allocations := sproto.ResourceList&#123;&#125;\n    for _, rs := range resources &#123;\n        allocations[rs.Summary().ResourcesID] = rs\n        k.allocationIDToContainerID[req.AllocationID] = rs.containerID\n        k.containerIDtoAllocationID[rs.containerID.String()] = req.AllocationID\n    &#125;\n\n    assigned := sproto.ResourcesAllocated&#123;\n        ID:                req.AllocationID,\n        Resources:         allocations,\n        JobSubmissionTime: req.JobSubmissionTime,\n    &#125;\n    // 添加套reqList中已分配列表中\n    k.reqList.AddAllocationRaw(req.AllocationID, &amp;assigned)\n    // 发布分配完成的消息\n    rmevents.Publish(req.AllocationID, assigned.Clone())\n\n&#125;\n","tags":["ai","go"]},{"title":"docker部署frp内网穿透","url":"/2023/11/27/docker%E9%83%A8%E7%BD%B2frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/","content":"frp是一个国人开发的内网穿透工具\n\n\n\n\nfrp是cs架构，访问frps(服务端)就可以访问部署在内网的frpc(客户端)\n\n服务端version: '3.8'\n\nservices:\n  frps:\n    image: snowdreamtech/frps\n    container_name: frps\n    restart: always\n    network_mode: \"host\"\n    volumes:\n      - /etc/frp/:/etc/frp/\n\n# frps.toml\nbindPort = 7000\nauth.token = \"&lt;密码>\"\n\n# 报表\nwebServer.addr= \"0.0.0.0\"\nwebServer.port = 7500\nwebServer.user = \"admin\"\nwebServer.password = \"&lt;密码>\"\n\n\n启动之后可以通过7500端口访问报表\n\n客户端version: '3.8'\n\nservices:\n  frpc:\n    image: snowdreamtech/frpc:latest\n    container_name: frpc\n    restart: always\n    network_mode: \"host\"\n    volumes:\n      - /data/frp/:/etc/frp/\n\nserverAddr = \"&lt;服务器地址>\"\nserverPort = 7000\n\nauth.token = \"&lt;服务端认证token>\"\n\nwebServer.port = 7400\nwebServer.user = \"admin\"\nwebServer.password = \"&lt;密码>\"\n\n[[proxies]]\nname = \"ssh\"\ntype = \"tcp\"\nlocalIP = \"127.0.0.1\"\nlocalPort = 22\nremotePort = 6000 # 服务端远程访问的端口\n\n参考资料https://gofrp.org/zh-cn/docs/\n","tags":["网络"]},{"title":"docker镜像分析工具dive","url":"/2020/09/29/docker%E9%95%9C%E5%83%8F%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7dive/","content":"通过可视化分析docker镜像\n\n\n安装MacOsbrew install dive\n\n其他平台查看官方文档:https://github.com/wagoodman/dive#installation\n介绍一般我们查看镜像可以使用docker inspect命令查看镜像的信息\n使用dive &lt;image:tage&gt;来查看一个镜像，默认tag为latest没有镜像则会下载\n\n如图之所示左边显示阶段和执行的命令，右边是文件系统，&lt;tab&gt;键切换到右边的文件系统，↑↓键则启动光标\n","tags":["docker"]},{"title":"gitlab升级之后gitlab-page404","url":"/2022/08/13/gitlab%E5%8D%87%E7%BA%A7%E4%B9%8B%E5%90%8Egitlab-page404/","content":"记一次升级gitlab导致的gitlab page 404\n\n\n起因公司gitlab每年都会升级上个大版本的最后一个小版本，按着计划本次从gitlab 13.12.12 升级到14.10.5，按着官方的升级计划升级到14.10.5是:13.12.15 &#x3D;&gt; 14.0.12 &#x3D;&gt; 14.3.6 &#x3D;&gt; 14.6.2 &gt; 14.9.5 -&gt; 14.10.5像往常一样准备所需要的安装包,发通知坐等夜晚十点的升级，开始了毕业之后最难查的bug。。。\n经过当晚十点按计划先升级到13.12.15,打开用gitab page部署的wiki,查看之前写的gitlab升级，备份了一下gitlab.rb和gitlab-secrets.json，在熟悉的yum确认界面下了y，gitlab开始执行升级随后在等了几分钟之后显示升级成功，打开gitlab先开始是502过会就正常了\n于是继续升级到gitlab14这个大版本，同样执行了升级命令，然后报错检查未过查看报错大概意思是unicorn[xxx]参数不再支持unicorn，于是搜索了下gitlab14不在支持unicorn改为puma,夜晚不太想查对应的字段且这些都是优化项于是直接注释了，再次执行yum install xxx但是还是报错。然后又试了几次还是报这个错误，仔细看了配置文件unicorn的配置已经被我注释了，为啥还是报这个错了呢，稍微思索了一会突然想到了gitlab.rb这个配置文件修改之后要执行gitlab-ctl reconfigure才生效，执行之后再次升级果然没有报错了同样的等了几分钟之后界面打开了。14版本新增了后台数据库迁移任务在迁移任务没没有完成再升级下个把版本会报错的，所以赶紧使用管理员账号看下后台任务，发现才完成2个还有10来个任务且进度很慢果然和预期的一样是没法一下子升级到目标版本的，遂检查下git clone git push等功能,发现没问题\n准备睡觉但是好巧不求随手点了下打开的wiki，然后wiki就404了。。。开始以为是刚升级完导致的，于是手动刷新了几次还是404此时心里有点纳闷，我都没更改gitlab的配置为什么会404，然后冷静一下首先此问题不是大问题gitlab重要功能没问题，然后对可能出问题的地方进行了一下分析，我只是注释了unicorn的一些配置其次14版本的后台字段升级会不会影响，于是查找了unicorn对应puma配置，修改完成之后重新reconfigure一下，重新打开wiki界面发现还是404。。。那么此时就有可能是14版本的后台迁移任务导致的，查看了下升级任务才玩跑2个第三个还很慢估计得第二天了，但是心里隐隐约约觉得这俩应该没啥关系，于是试探性的查看了下gitlab的nginx日志，发现里面有301返回的日志，此时企信群里有同事已经再说为啥gitlab page打不开了(真卷当时都11点多了)，我没有找到原因就没回他，于是继续顺着301这条线索找，\n开始怀疑是gitlab-pages服务返回了个301于是gitlab-ctl tail gitlab-pages查看了下gilab-page服务的日志，然后发现没日志。。。查看配置文件原来gitlab-pges的日志文件修改了,于是去了日志目录，从我升级之后日志就没了此时怀疑是gitlab-page服务是不是升级之后有问题。变在浏览器刷新wiki变来查看ngixn和pages日志，随后有报错502，在nginx日志显示是connection refused大概意思,发现解析出来的locahost是ipv6的而gitlab-page是监听的ipv4，手动调用下确实，于是将gitlab-page改为监听ipv6测试一下界面还是404，随后一番操作时候发现和gitlab监听的地址关系不大，又改回监听ipv4，用curl测试curl localhost:8090有响应。百思不得其解。在晚上搜索了一番也么结果。此时已经凌晨一点半左右，于是没办法只能先睡觉了，在床上依然很纳闷，我明明没动gitlab-page相关配置为啥会出问题。\n第二天上午到公司之后和同事交流了一下问题，她也加入处理问题。在企信群里通知所有人gitlab-page有问题我们再看了，并建立一个相关故障群同步处理情况。做完这些之后我们就开始了处理，她按我提供的情况查了一下未发现问题，唯一的有价值的是gitlab-page升级之后存储方式有变化变成了zipfs，需要迁移。于是执行了一下迁移返回的job是0，也就是说不需要迁移，此时我们查看了gitlab的架构图，觉得可能是nginx的问题，我们一起看了nginx的配置文件没发现啥问题，加上这个nginx文件是gitlab.rb生成的。\n\n中午吃完饭之后，下午我俩分工，我重新部署一套环境来对比，她继续调查gitlab-page的问题。我咋经过一波折腾时候成功在测试环境上访问到了我做测试的page界面，而她那边发现的更多，比如https://docs.gitlab.com/14.10/ee/administration/pages/index.html#wildcard-domains-with-tls-supporthttps导致的在经过一波操作测试还是不行，还有这个issues下的所有方法https://gitlab.com/gitlab-org/gitlab/-/issues/331699我们都是尝试了还是404，有点沮丧，此时一下午过得差不多了快要下班了。于是我想先回复一下业务，用go写一个web服务先临时替代一下gitlab-page。但是同事觉得直接用最简单的nginx好了，这里说下gitlab的page其实就是返回一个有规则的目录里的配置文件,于是尝试用nginx实现这个规则发现不行，同时领导也过来问了问帮我们叫来了另一个同事来写ningx，折腾了一个小时之后我们放弃了于是先下班了。满脑子都是404\n第二天上午到公司之后继续折腾那个nginx规则，同事则去培训了，然后发现不是不自动跳转就是找不到文件404，在快到中午时我放弃了，使用go配置gin实现了一下，很快就实现了除了一些路径稍微还有些问题，中午吃饭完之后三下午五除二就搞定了，将端口改为8080其他的和gitlab-page规则一致，自己测量下久违的界面出来了终于不是404😄，在群里发了通告。就开始处理问题了。在经过一波分析之后我还是觉得要不是nginx有问题要不就是gitlab-page有问题亦或者是认证有问题导致的跳转异常，于是我查看了gitalb-page的源码发现认证实际上在page上也有处理，于是直接停止了gitlab-page服务，手动在用命令行执行gitlab-page，在此测试发现gitlab-page的日志压根就没日志。那么这就可以百分之百确认是nginx的问题了，将nginx中转发gitlab-page的cat /var/opt/gitlab/nginx/conf/gitlab-pages.conf的proxy_pass http://localhost:8090;这一行注释，在他的前面增加了return 200 &quot;test&quot;来确认请求到了这里，测试之后浏览器并没后返回”test”字样，那么可以确认没有转发到这里！于是琢磨为啥流量没到这里。同时通过网页f12查看第一个请求，其响应了301发现响应服务器为nginx，于是使用301作为关键字在/var/opt/gitlab/nginx/conf查找，发现了下面这一句\ngrep -r -n \"301\" *\ngitlab-http.conf:43:    return 301 https://git.example.com:443$request_uri;\n\n于是发现这个是80跳转到443的配置，既常见的访问80自动跳转到http的443配置，但是这行监听的是gitlab的配置不是gitlab page的配置，但是我还是注释了此行进行测试，在浏览器测试之后发现界面由gitlab的404变成了nginx的404，也就是说这行对本不是由他处理的gitab page产生了作用\n\n但是这只是解释了为啥跳转没解释路由问题，查看了gitla page的配置文件时发现一个奇怪的配置：\nserver_name  ~^(?&lt;group>.*)\\.page\\.example\\.com$;\n\n这推测意思是只要是group这个组的并且下都是为此gitlab-page匹配的由gitlab-page这个配置文件的内容来处理，我开始以为这个group是nginx的变量之类的gitlab通过某种方式传入到了nginx当中，导致nginx没有匹配到此，所以我就问了公司比较熟悉nginx的同事,在咨询了他时候他说这个group其实是变量赋值，也就说说.page前面的字符串给了group这个变量，遂邀请他来我工位上一起看下，简单说了下我的发现他看了下配置文件，我复盘了一下我的发现。 随后他说将在page的配置文件上添加一下listen参数，将信将疑的问他会是这个导致的吗，他说他遇到ipv4和ipv6监听不一致导致的路由失效，于是添加了下\nlisten 192.168.1.1:80;\nlisten [::]:80;\n\n添加完成之后重启，发现还是这个样子。他说估计是缓存，于是我换了个浏览器，久违的界面终于出来。不再是那个讨厌的404了\n结果由于nginx的配置是生成的所以需要修改gitlab.rb的配置才行只需要将page的nignx也监听ipv6即可\npages_nginx['listen_addresses'] = ['192.168.1.1', '[::]']\n\n疑问\n为什么老版本没有这个问题？\n\n参考资料gitlab架构\n","tags":["gitlab","故障处理"]},{"title":"gomod使用私有仓库","url":"/2023/04/11/gomod%E4%BD%BF%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/","content":"在工作中我们有些mod是放在gitlab中的且一般是有认证的这里记录下解决办法\n\n\n私有仓库设置\n都可以通过,来设置多个\n\n\n告诉go那些仓库是私有仓库\n\ngo env -w GOPRIVATE=\"git@git.example.com\"\n\n\n告诉go私有仓库不走goproxy代理\n\ngo env -w GONOPROXY=\"git.example.com\"\n\n\n告诉go这个仓库的不用验证CA\n\ngo env -w GOINSECURE=\"git.example.com\"\n\n\n设置不做校验的仓库\n\ngo env -w GONOSUMDB=\"git.example.com\"\n\n使用gitlab token认证\n原理其实就是替换下git的链接将普通的链接替换成可以认证的链接\n\ntoken在gitlab的项目–&gt;设置–&gt;访问令牌，添加一个只读的即可\n\n\n# 将go默认访问的替换成通过token认证的链接以达到认证的目的\ngit config --global url.\"https://oauth2:$TOKEN@git.example.com/lib/utils.git\".insteadOf \"https://git.example.com/lib/utils.git\"\n\n使用gitlab ssh认证\n这里将https的请求换成ssh请求，需要注意的是本地的公钥需要提前加入到gitalb中\n\ngit config --global url.\"git@git.example.com:lib/utils.git\".insteadOf \"https://git.example.com/lib/utils.git\"\n\n# 另一种写法\ngit config --global url.\"ssh://git@git.example.com:lib/utils.git\".insteadOf \"https://git.example.com/lib/utils.git\"\n","tags":["gomod"]},{"title":"gitlab部署配置","url":"/2023/11/23/gitlab%E9%83%A8%E7%BD%B2%E7%BB%B4%E6%8A%A4/","content":"gitlab是一个功能非常强大的私有化git仓库\n\n\n部署\nyum安装\n\nyum install -y curl policycoreutils-python openssh-server\nsystemctl enable sshd\nsystemctl start sshd\ncurl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh | sudo bash\nyum install -y gitlab-ce\n\n备份设置定时备份\ncrobjob设置定时备份\n\n0 23 * * * /opt/gitlab/bin/gitlab-backup create SKIP=builds,artifacts,lfs,terraform_state\n\n设置备份保留时间\n保留三天\n\ngitlab_rails['backup_keep_time'] = 259200\n\n备份到挂载在本地的存储\n修改配置文件，&#x2F;mnt&#x2F;nfs为nfs挂载点\n\ngitlab_rails['backup_upload_connection'] = &#123;\n  :provider => 'Local',\n  :local_root => '/mnt/nfs'\n&#125;\n\ngitlab_rails['backup_upload_remote_directory'] = 'gitlab-backups'\n\n\n执行gitlab-ctl reconfigure生效\n\n还原sudo gitlab-ctl stop puma\nsudo gitlab-ctl stop sidekiq\n# Verify\nsudo gitlab-ctl status\n\ngitlab-backup restore BACKUP=1684312462_2023_05_17_14.9.5\n\n升级\n查看升级计划 https://docs.gitlab.com/ee/update/index.html#upgrade-paths,新版本有个工具来确定升级计划https://gitlab-com.gitlab.io/support/toolbox/upgrade-path/\n\n根据升级计划下载中间版本和目标版本的二进制文件https://packages.gitlab.com/gitlab/gitlab-ce\n\ngitlab各个版本发行说明https://about.gitlab.com/releases/categories/releases/\n\n\n注意14版本以上增加了后台迁移任务，后台迁移任务未跑完成时升级会报错 https://docs.gitlab.com/ee/update/index.html#batched-background-migrations\n\n下载并安装\n\nwget --content-disposition https://packages.gitlab.com/gitlab/gitlab-ce/packages/el/7/gitlab-ce-13.12.12-ce.0.el7.x86_64.rpm/download.rpm\nyum -y install gitlab-ce-13.12.12-ce.0.el7.x86_64.rpm\n\nrunner\nhttps://docs.gitlab.com/runner/install/linux-repository.html\n\n安装对应系统的源头\n\n\n# apt\ncurl -L \"https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh\" | sudo bash\n\n# apt\ncurl -L \"https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh\" | sudo bash\n\n\n安装gitlab runner\n\n# apt\nsudo apt-get install gitlab-runner\n\n# yum\nsudo yum install gitlab-runner\n\n\n注册 管理区域-&gt;cicd-&gt;runenr添加一个，然后会出现类似gitlab-runner register --url http://gitlab.naturelr.cc  --token glrt-94ZmmuqybyyGdPW_kbSJ\n\n启动，将注册的时候出现的命令执行一下，会询问你一些信息一步一步操作即可\n\n\ngitalb-pages\n设置pages的地址，不要和gitlab的域名一致，dns是是泛域名解析到gitlab服务器\n\npages_external_url \"http://pages.example.com/\"\ngitlab_pages['enable'] = true\n\n实例pages:\n  stage: deploy\n  script:\n    - mkdir .public\n    - cp -r * .public\n    - mv .public public\n  artifacts:\n    paths:\n      - public\n  only:\n\n&lt;!DOCTYPE html>\n&lt;html lang=\"en\">\n&lt;head>\n    &lt;meta charset=\"UTF-8\">\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    &lt;title>Document&lt;/title>\n&lt;/head>\n&lt;body>\n    &lt;h1>测试&lt;/h1>\n&lt;/body>\n&lt;/html>\n","tags":["gitlab"]},{"title":"go环境变量","url":"/2020/09/14/go%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","content":"go 有很多的环境变量记录一下常用的变量\n\n\n查看环境变量\ngo env 查看所有变量\n\ngo env xxx 查看执行环境变量\n\ngo help environment 查看各个环境变量的作用\n\n\n修改环境变量\ngo 1.13以上推荐使用 go env -w NAME&#x3D;VALUE 来设置环境变量\n\ngo env -w 设置的变量根据os.UserConfigDir()返回的值来确定存在哪\n\nLinux在$HOME&#x2F;.config\nDarwin在$HOME&#x2F;Library&#x2F;Application Support\nWindows在%AppData%\n\n\ngo 1.13以下使用export NAME&#x3D;VALUE 写profile来设置，如.bashrc,.zshrc等\n\n\n常用变量说明\n\n\n环境变量\n说明\n默认\n备注\n\n\n\nGOROOT\ngo的安装位置\n&#x2F;usr&#x2F;local&#x2F;bin\n-\n\n\nGOARCH\n架构类型\n当前机器架构类型\n-\n\n\nGOOS\n编译出文件的类型\n当前系统\n通过改变GOOS来设置交叉编译\n\n\nGOPATH\ngo的项目存放目录\n$HOME&#x2F;go\n在没使用gomod的时候安装的代码就存放在此\n\n\nGOBIN\ngo instlal安装的文件目录\n-\n一般将此目录加入PATH,export PATH=$PATH:$GOBIN&gt;$HOME/.zshrc\n\n\nGO111MODULE\ngo mod 开关\n自动\n-\n\n\nGOPROXY\ngo mod的代理地址\n-\nhttps://goproxy.cn,https://mirrors.aliyun.com/goproxy/,https://goproxy.io,direct\n\n\n","tags":["go"]},{"title":"go资源内嵌embed","url":"/2021/03/19/go%E8%B5%84%E6%BA%90%E5%86%85%E5%B5%8Cembed/","content":"Go官方在1.16版本发布了官方内嵌资源到二进制的功能，使得部署更加简单\n\n\n\n在开发web的时候往往会有一些web文件，而部署的时候需要部署一个二进制还要部署web文件比较繁琐，在go1.16之前也有很多包实现了内嵌资源文件到二进制中如https://github.com/gobuffalo/packr，而如今go官方实现了这个特性\n\n基本用法package main\n\nimport (\n\t_ \"embed\"\n\t\"fmt\"\n)\n\n//go:embed Dockerfile\nvar f string\n\nfunc main() &#123;\n\tfmt.Println(f)\n&#125;\n\n上面的例子就是将当前目录的dockerfile内容内嵌到变量f中,编译之后即使这个文件不存在也能打印出内容\n嵌入文件夹package main\n\nimport (\n    \"embed\"\n    \"fmt\"\n    \"path/filepath\"\n)\n\n//go:embed foo\nvar fs embed.FS\n\nfunc main() &#123;\n    files, err := fs.ReadDir(\"foo\")\n    if err != nil &#123;\n        fmt.Println(err)\n    &#125;\n    for _, file := range files &#123;\n        d, _ := fs.ReadFile(filepath.Join(\"foo\", file.Name()))\n        if err != nil &#123;\n            fmt.Println(err)\n        &#125;\n        fmt.Println(\"文件名:\", file.Name(), \"内容:\", string(d))\n    &#125;\n&#125;\n\n\n上面的代码将目录下的foo目录内嵌到fs这个变量中，然后打印出这个文件夹里文字的名字和内容\n\n$ tree foo \nfoo\n├── test\n└── test2\n\n0 directories, 2 files\n\n# 编译\n$ go build -o test .\n\n# 执行\n$ ./test                \n文件名: test 内容: hahah\n文件名: test2 内容: testest\n\n注意\n路径默认是从mod的目录为根目录\n会忽略”.“开头和”_“开头的文件\n不管是win还是linux都使用”&#x2F;“\n支持匹配如，//go:embed foo/*.yaml\n可以同时导入多个目录 如//go:embed foo test\n\n参考资料https://www.cnblogs.com/apocelipes/p/13907858.html\n","tags":["go"]},{"title":"haproxy使用","url":"/2023/08/22/haproxy%E4%BD%BF%E7%94%A8/","content":"haproxy是一个负载均衡程序支持L4和L7,和ipvs不同的是他的实现在用户空间\n\n\n安装yum install -y haproxy\nsystemc start haproxy\n\n配置\n配置的路径为/etc/haproxy/haproxy.cfg\n\nglobal为全局配置\n\ndefaults则为默认配置\n\nfrontend backend listen其中frontend和backend配合完成一个代理比较灵活，listen则比较方便直接能定义监听相关信息和后端地址\n\n\nglobal # 全局配置\n    log         127.0.0.1 local2\n\n    chroot      &#x2F;var&#x2F;lib&#x2F;haproxy\n    pidfile     &#x2F;var&#x2F;run&#x2F;haproxy.pid\n    maxconn     4000 # 最大连接数\n    user        haproxy\n    group       haproxy\n    daemon # 以daemon方式运行\n\n    # turn on stats unix socket\n    stats socket &#x2F;var&#x2F;lib&#x2F;haproxy&#x2F;stats\ndefaults # 默认参数\n    mode                    http # 定义模式，http为7层，tcp则为4层\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0&#x2F;8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n\nfrontend  main *:5000 # 定义前端\n    acl url_static       path_beg       -i &#x2F;static &#x2F;images &#x2F;javascript &#x2F;stylesheets # acl设置7层路径前缀匹配\n    acl url_static       path_end       -i .jpg .gif .png .css .js # acl设置7层路径后缀匹配，还有正则匹配\n\n    use_backend static          if url_static # 符合url_static这个规则则使用static这个后端\n    default_backend             app # 默认后端app\n\nbackend static\n    balance     roundrobin\n    server      static 127.0.0.1:4331 check\n\nbackend app # 后端app\n    balance     roundrobin # 代理算法\n    server  app1 10.7.112.201:80 check # 定义后端地址有很多个，check开启了健康检查\n\nlisten stats    #定义监控页面，通过浏览器可以查看haproxy状态\n    bind *:1080                   # 绑定端口1080\n    stats refresh 30s             # 每30秒更新监控数据\n    stats uri &#x2F;stats              # 访问监控页面的uri\n    stats realm HAProxy Stats     # 监控页面的认证提示\n    stats auth admin:admin        # 监控页面的用户名和密码\n\n参考资料https://www.cnblogs.com/f-ck-need-u/p/8502593.html#1-5-acl\n","tags":["网络","负载均衡"]},{"title":"helm使用","url":"/2021/07/07/helm%E4%BD%BF%E7%94%A8/","content":"helm是cncf基金会下的一个云原生管理程序\n\n\n\nhelm2和helm3有些区别，helm3去掉了服务端，本文主要是使用helm3\n\n安装macos\nbrew install helm\n\n脚本安装\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\n\n应用搜索应用在hub中搜索\nhelm search hub &lt;应用>\n\n在本地的repo中搜索\nhelm search repo &lt;应用>\n\n安装应用helm install &lt;名字> &lt;仓库>\n\n# 指定ns下安装\nhelm install &lt;名字> &lt;仓库> --namespace &lt;namespace>\n\n\n指定ns并创建\n\nhelm install &lt;名字> &lt;仓库>  --namespace &lt;namespace> --create-namespace \n\n\n查看helm chart的values文件\n\nhelm show values &lt;仓库> > values.yaml\n\n\n通过values来设置参数\n\nhelm install &lt;名字> &lt;仓库> --namespace &lt;namespace> --create-namespace -f values.yaml\n\n\n通过命令行来设置参数,–set参数可以有多个用户指定多个参数，其指定的参数就是values里的\n\nhelm install &lt;名字> &lt;仓库> --namespace &lt;namespace> --create-namespace --set &lt;key>:&lt;value>\n\n\n–dry-run参数不执行安装可以将要安装yaml打印出来\n\nhelm install &lt;名字> &lt;仓库> --namespace &lt;namespace>  --create-namespace --debug --dry-run > resource.yaml\n\n查看应用显示当前ns下\nhelm list\n\n显示当前ns下\nhelm list -n &lt;namespace>\n\n显示所有ns\nhelm list -A\n\n升级应用获取安装时的设置值\nhelm get values &lt;应用> > tmp.yaml\n\n升级配置或者版本\nhelm upgrade &lt;应用> &lt;应用仓库> -f tmp.yaml\n\n升级指定版本\nhelm upgrade &lt;应用> &lt;应用仓库> --version vx.y.z\n\n例子\nhelm get values cilium > tmp.yaml\nhelm upgrade cilium cilium/cilium -f tmp.yaml\n\n回滚应用helm rollback &lt;应用>\n\n卸载应用helm uninstall &lt;名字>\n\n下载应用包将在本地生成一个包里面是这个应用得chart文件\nhelm fetch &lt;应用仓库>\n\n本地直接生成模版\n常常用在离线安装或者本地开发当中\n\nhelm template &lt;解压后chart包> -f &lt;values文件>\n\n仓库操作添加仓库helm repo add &lt;仓库地址>\n\n查看仓库helm repo list\n\n升级仓库helm repo update\n\n删除仓库helm remove &lt;仓库名字>\n\n参考资料http://blog.naturelr.cchttps://helm.sh/docs\n","tags":["k8s","helm"]},{"title":"hexo搭建博客","url":"/2020/09/16/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/","content":"记录一下用hexo搭建博客的过程和一些坑\n\n\n安装 Node.jsbrew install node\n\n具体看nodeJS基本使用,当前版本的hexo在node14中会有告警，建议安装12并使用nvm管理node版本\n安装 Hexonpm install hexo-cli -g\n\n初始化Hexomkdir blog &amp;&amp;cd blog # 创建文件夹并进入\nhexo init            # 初始化 hexo\n\n这个时候执行hexo g &amp;&amp; hexo s就可以使用localhost:4000打开一个blog，此时这个主题是默认的\n配置Hexo\nblog根目录目录下的_config.yaml是hexo的配置文件，自定义的相关设置需要修改此文件\n\n安装 Next主题npm install hexo-theme-next\n\n配置Next主题\n配置主题为next，在_config.yaml中查找theme并修改为next\n配置文件中有详细的说明不再细说\n\nHexo-adminhexo admin 是一个可以直接在网页上写文章且实时预览的插件\n执行npm install --save hexo-admin安装,访问http://localhost:4000/admin\n","tags":["hexo"]},{"title":"ipset","url":"/2021/07/28/ipset/","content":"ipset是iptables的一个扩展，可以动态的修改规则的地址\n\n\n\n主要用户存储网络，端口号，ip地址以及mac地址，然后在iptables中调用此模块,有点像是存储网络信息的数据库\n\n安装yum install ipset\n\n基本操作ipset的操作比较简单\n显示集合ipset list &lt;集合名字>\n\n增加集合ipset create &lt;集合名字> &lt;集合类型>\n\n删除集合# 删除指定集合\nipset destroy &lt;集合名字>\n\n# 删除所有\nipset destroy\n\n增加条目ipset add &lt;集合名字> &lt;条目>\n\n删除条目ipset del &lt;集合名字> &lt;条目>\n\n保存规则ipset save > ipset.bak\n\n还原规则ipset restore &lt; ipset.bak\n\n参考资料https://ipset.netfilter.org\n","tags":["网络"]},{"title":"iptables","url":"/2020/12/03/iptables/","content":"Iptables是大多数发型版本中支持的防火墙\n\n\n\niptables是个前端其真正的后端是linux的netfilter框架,一些发行版略有区别，centos8中则使用nftables来代替\n\n链\n在linuxn内核中的五个钩子(hook)，iptable中还可以自定义链，自定义只能被默认链引用才能使用\n\n\nINPUT 发送到用户空间的钩子\nOUTPUT 从用户空间发发出的钩子\nPREROUTING 路由前的钩子\nFORWARD 转发的钩子\nPOSTROUTING 路由后的钩子\n\n表\n一些相近功能规则的组\n\n\nfilter表：负责过滤功能，内核模块：iptables_filter\nnat表：网络地址转换功能；内核模块：iptable_nat\nmangle表：拆解报文，做出修改并重新封装的功能；内核模块：iptable_mangle\nraw表：关闭nat表上启用的连接追踪机制；iptable_raw\n\n链表关系\n\n以流量的视角来看\n\n\n\n全局来看,原地址\n\n\n查看规则\n命令说明\n-L 列出规则,L后面可也接受指定链\n-v 可以查看更多的信息\n-n 不对地址做名称反解 直接显示原来的IP地址\n-t 执行表名，默认为filter表\n–line-numbers 显示规则序列号,缩写为–line\n-x 精确数值\n\n返回说明\n红色部分：\nchain：链名，括号里的policy默认策略这里是drop\npackets：默认策略匹配到的包的数量\nbytes：当前链默认策略匹配到的所有包的大小总和\n\n\n绿色部分：\nbytes:对应匹配到的报文包的大小总和\ntarget:规则对应的target，往往表示规则对应的”动作”，即规则匹配成功后需要采取的措施\nprot:表示规则对应的协议，是否只针对某些协议应用此规则\nopt:表示规则对应的选项\nin:表示数据包由哪个接口(网卡)流入\nout:表示数据包由哪个接口(网卡)流出\nsource:表示规则对应的源头IP或网段\ndestination:表示规则对应的目标IP或网段\n\n\n黄色部分：规则序列号\n\n查看所有规则查看所有链所有表的规则\niptables --line-numbers -nvL\n\n查看指定规则查看指定表，默认链\n# iptables --line-numbers -nvL -t &lt;表>\niptables --line-numbers -nvL -t nat\n\n查看INPUT链的nat表\n# iptables --line-numbers -nvL &lt;链> -t &lt;表>\n# iptables --line-numbers -nv -L &lt;链> -t &lt;表>\niptables --line-numbers -nv -L INPUT -t nat\n\n查看INPUT链的nat表的序列号是3的规则\n# iptables --line-numbers -nvL &lt;链> 3-t &lt;表>\n# iptables --line-numbers -nv -L &lt;链> 3 -t &lt;表>\niptables --line-numbers -nv -L INPUT 3 -t nat\n\n增加规则\niptables是自上而下匹配规则的所以顺序很重要 -A 尾部增加 -I 头部增加 后面加上序列号则是指定序列号位置\n\n尾部增加规则在 filter表INPUT链中尾部增加一条丢弃从192.168.1.1发送过来数据的规则\n# iptables -t &lt;表名&gt; -A &lt;链名&gt; &lt;匹配条件&gt; -j &lt;动作&gt;\niptables -t filter -A INPUT -s 192.168.1.1 -j DROP\n\n头部增加规则在 filter表INPUT链中头部增加一条丢弃从192.168.1.2发送过来数据的规则\n# iptables -t &lt;表名> -I &lt;链名> &lt;匹配条件> -j &lt;动作>\niptables -t filter -I INPUT -s 192.168.1.2 -j DROP\n\n指定位置增加规则在 filter表INPUT链中指定位置增加一条丢弃从192.168.1.3发送过来数据的规则\n# iptables -t &lt;表名> -I &lt;链名> &lt;规则序号>  &lt;匹配条件> -j &lt;动作>\niptables -t filter -I INPUT  3 -s 192.168.1.2 -j DROP\n\n修改规则将序列号为2的规则的动作修改为accept\n# iptables -t &lt;表名> -R &lt;链名> &lt;规则序号> &lt;原本的匹配条件> -j &lt;动作>\niptables -t filter -R INPUT 2 -s 192.168.1.146 -j ACCEPT\n\n修改默认规则将INPUT链默认策略设置为DROP,注意不要在生产环境执行此规则会断网\n# iptables -t &lt;表> -P &lt;链> &lt;动作>\n# iptables  -P &lt;链> &lt;动作> 表可省略\niptables -t filter -P INPUT DROP\n\n删除规则按照规则序号删除规则# iptables -t &lt;表名&gt; -D &lt;链名&gt; &lt;规则序号&gt;\niptables -t filter -D INPUT 3\n\n按照具体的匹配条件与动作删除规则# iptables -t &lt;表名&gt; -D &lt;链名&gt; &lt;匹配条件&gt; -j &lt;动作&gt;\niptables -t filter -D INPUT -s 192.168.1.2 -j DROP\n\n删除所有规则\n谨慎操作！！！\n\n清除filter表\n# iptables -t &lt;表名&gt; -F\niptables -t filter -F\n\n处理动作\n处理动作在iptables中被称为target，动作也可以分为基本动作和扩展动作默认动作如下\n\n\nACCEPT：允许数据包通过\nDROP：丢弃数据包,客户端会等待\nREJECT：拒绝数据包通过，客户端会立即发现拒绝\nSNAT：源地址转换\nDNAT：目标地址转换\nMASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上\nREDIRECT：在本机做端口映射\nLOG：在&#x2F;var&#x2F;log&#x2F;messages文件中记录日志信息，然后传给下一条\n\n日志\n日志模块一般用于调试\n\n# 开启日志\niptables -A INPUT -j LOG\n\n# 设置日志级别\niptables -A INPUT -s 192.168.1.0/24 -j LOG --log-level 7\n\n# 在日志加上前缀\niptables -A INPUT -s 192.168.1.0/24 -j LOG --log-prefix \"iptables log: \"\n\n条件匹配\n如果有多个规则，则规则之间是与关系\n\n源地址匹配\n-s 用于匹配报文的源地址,可以同时指定多个源地址，每个IP之间用逗号隔开，也可以指定为一个网段。\n\n# 丢弃从192.168.1.1和192.168.1.2发送过来的数据\niptables -t filter -I INPUT -s 192.168.1.1,192.168.1.2 -j DROP\n# 允许从192.168.1.0/24发送过来的数据\niptables -t filter -I INPUT -s 192.168.1.0/24 -j ACCEPT\n# 允许除了192.168.1.0/24发送过来的数据\niptables -t filter -I INPUT ! -s 192.168.1.0/24 -j ACCEPT\n\n目的地址匹配\n-d 用于匹配报文的目标地址,和源地址匹配一样可以同时指定多个目标地址，每个IP之间用逗号隔开，也可以指定为一个网段。\n\n# 丢弃发送到192.168.1.1和192.168.1.2的数据\niptables -t filter -I OUTPUT -s 192.168.1.1,192.168.1.2 -j DROP\n# 允许发送数据到192.168.1.0/24\niptables -t filter -I INPUT -s 192.168.1.0/24 -j ACCEPT\n# 允许发送除了192.168.1.0/24数据\niptables -t filter -I INPUT ! -s 192.168.1.0/24 -j ACCEPT\n\n端口匹配\n-p 匹配报文的协议类型,可以匹配的协议类型tcp、udp、icmp等\n\n# 允许发送数据到192.168.1.1的tcp协议\niptables -t filter -I INPUT -p tcp -s 192.168.1.1 -j ACCEPT\n# 允许发送数据到192.168.1.1的udp协议\niptables -t filter -I INPUT ! -p udp -s 192.168.1.1 -j ACCEPT\n\n网卡流入匹配\n-i 匹配报文是从哪个网卡接口流入本机的，由于匹配条件只是用于匹配报文流入的网卡，所以在OUTPUT链与POSTROUTING链中不能使用此选项。\n\n#\niptables -t filter -I INPUT -p icmp -i enp0s3 -j DROP\n#\niptables -t filter -I INPUT -p icmp ! -i enp0s3 -j DROP\n\n网卡流出匹配\n-o 匹配报文将要从哪个网卡接口流出本机，于匹配条件只是用于匹配报文流出的网卡，所以在INPUT链与PREROUTING链中不能使用此选项。\n\n# 丢弃从enp0s3网卡流出的icmp协议\niptables -t filter -I OUTPUT -p icmp -o enp0s3 -j DROP\n# 丢弃除enp0s3网卡流出的icmp协议\niptables -t filter -I OUTPUT -p icmp ! -o enp0s3 -j DROP\n\n自定义链\n在规则比较多时方便管理规则，自定义链需要被默认链引用才能生效\n\n创建自定义链\n创建一个filter表名字叫TEST的链\n\n# iptables -t &lt;表> -N &lt;链名>\niptables -t filter -N TEST\n\n引用自定义链\n引用的时候只需要将动作使用自定义链名 在默认链INPUT中插入一条目标端口是80协议是tcp动作是自定义链TEST的规则\n\niptables -t filter -I INPUT -p tcp --dport 80 -j TEST\n\n重命名自定义链\n将TEST自定义链改为TEST2\n\n# iptables -E &lt;原自定义链名> &lt;目标自定义链名>\niptables -E TEST TEST2\n\n删除自定义链\n删除TEST2的自定义链名\n\n# iptables -X &lt;自定义链名>\niptables -X TEST2\n\n扩展模块\niptables支持使用扩展模块来进行功能的扩展\n\ncomment\n注释模块，顾名思义对规则进行说明\n\n-m comment --comment \"comment\" \n\niptables -t filter -I INPUT -s 192.168.1.0/24 -m comment --comment \"xxxx\" -j ACCEPT\n\nmultiport\n多端口模块，可以设置一条规则匹配多个端口\n\n-m multiport --dports &lt;端口号>,&lt;端口号>\n\nipset\n可以一条规则匹配ipset里面的地址\n\n-m set --match-set &lt;ipset名字>\n\nmark\n标记流量，需要注意的时候这个标记只在本地标记流量出去之后就没了\n\n# 设置标签\n-j MARK --set-xmark 0x8000/0x8000\n\n# 匹配标签并丢弃\n-m MARK --mark 0x8000/0x8000 -j DROP\n\n\n–set-xmark value[&#x2F;mask]  mask和value做异或运算\n–set-mark value[&#x2F;mask]   mask和value做或运算\n–and-mark bits           和nfmark做与运算\n–or-mark bits            和nfmark做或运算\n–xor-mark bits           和nfmark做异或运算\n\nconntrack\n连接跟踪模块可以根据连接的状态匹配\n\n-m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n\n\nNEW 新建连接\nESTABLISHED 已经连接\nRELATED 相关连接\nINVALID 无效连接\nUNTRACKED 人为设置的\n\n参考 https://www.cnblogs.com/saolv/p/13096965.html\n规则保存导出和还原保存规则\ncentos7以上没有安装iptables-services\n\n# 安装iptables-services\nyum install -y iptables-services\n\n# 保存规则\nservice iptables save\n\n导出规则iptables-restore > iptable\n\n还原规则iptables-save &lt; iptable\n\n帮助iptables -j &lt;target> -help\n\n参考资料朱双印个人博客https://www.frozentux.net/iptables-tutorial/iptables-tutorial.html\n","tags":["linux","网络"]},{"title":"ipvs","url":"/2022/11/07/ipvs/","content":"ipvs是个4层负载均衡器，常常用于服务的高可用\n\n\n\nipvs已经合并到linux内核当中，用户层面使用ipvsadm\n\n安装ipvsadmyum install ipvsadm\n\n原理\n整个动作在内核态完成,跳过了一些链\n术语\n\n\n缩写\n全写\n说明\n\n\n\nCIP\nClient IP\n客户端ip\n\n\nVIP\nVirtual IP\n虚拟ip\n\n\nDIP\nDirector Server IP\n负载均衡ip\n\n\nRIP\nReal Servier IP\n真正的后端服务ip\n\n\nDS\nDirector Server\n部署负载均衡的服务器\n\n\nRS\nReal Server\n后端服务器\n\n\n三种模式\n\n\n类型\nIP\n\n\n\nCIP\n10.23.18.81\n\n\nVIP\n10.23.59.162\n\n\nDS\n10.23.12.87\n\n\nRS1\n10.23.197.23\n\n\nRS2\n10.23.9.198\n\n\nNAT\n本质是个dnat\n流量出入都经过DR\n来回流量都从dr过dr会成为瓶颈\n同一个网段时RS的默认网关需要指向DS,且本网段的路由需要删除不然不会通过默认路由到DR\n\n\nNAT部署\n在DS配置规则\n\necho 1 >/proc/sys/net/ipv4/ip_forward\n\nexport VIP=10.23.59.162\nexport RS1=10.23.197.23\nexport RS2=10.23.9.198\n\nipvsadm -A -t $VIP:80 -s rr\nipvsadm -a -t $VIP:80 -r $RS1 -m\nipvsadm -a -t $VIP:80 -r $RS2 -m\n\n\n查看规则\n\n[root@10-23-12-87 ~]# ipvsadm -L -n\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.23.59.162:80 rr\n  -> 10.23.9.198:80               Masq    1      0          2\n  -> 10.23.197.23:80              Masq    1      0          1\n\n\n在RS上部署一个httpd用于判断访问到哪那台机器\n\nyum -y install httpd &amp;&amp; systemctl start httpd\necho \"i am rs $HOSTNAME\" > /var/www/html/index.html\n\n\n此时在rs上curl 10.23.59.162这个vip会轮询访问，此时cip是自己\n\n[root@10-23-12-87 ~]# curl 10.23.59.162\ni am rs 10-23-9-198\n[root@10-23-12-87 ~]# curl 10.23.59.162\ni am rs 10-23-197-23\n[root@10-23-12-87 ~]#\n\n同一个网络说明client此时并不知道vip这个地址在哪,所以需要在dr上绑定vip,让隔壁的邻居知道vip的mac\n且由于vip和rs的地址都在同一个网段默认会通过二层直接到client,未经过dr的nat，导致client不认回包而被丢弃因此需要在rs上设置默认网关为dr\n\ndr执行\n\nip addr add 10.23.59.162 dev eth0\n\n\nRS上执行,将默认路由指向dr\n\nexport DS=10.23.12.87\n\n# 设置默认路由\nip route replace default via $DS\n# 删除路由\nip r del 10.23.0.0/16 dev eth0 proto kernel scope link src 10.23.197.23\n\n流量转发路径\n通过抓包我可以看到client请求vip之后，dr接受到会将vip替换成rip然后发送给rsrs收到之后因为会路由设置会发送给dr，dr将src地址再改回vip\n\nDR\nrs和ds需要在一个二层中\n\ndr模式中客户端请求vip流量从ds通过修改mac地址来达到负载均衡\n\n由于没有修改ip地址所以rs上需要添加vip到lo或者dummy类型的网口上，不然rs发现请求的ip不在本机就会被丢弃\n\n由于rs的lo或者dummy的网卡上配置的有vip为了防止rs响应vip的请求，所以需要修改arp配置\n\n不支持端口映射\n\n\nDR部署\nDS配置\n\nexport VIP=10.23.59.162\nexport RS1=10.23.197.23\nexport RS2=10.23.9.198\n\nip link add vip type dummy\nip addr add $VIP dev vip\n\nipvsadm -A -t $VIP:80 -s rr\nipvsadm -a -t $VIP:80 -r $RS1:80 -g\nipvsadm -a -t $VIP:80 -r $RS2:80 -g\n\n\n两个RS配置\n\n# 部署http服务用于区分是否负载均衡\nyum -y install httpd &amp;&amp; systemctl start httpd\necho \"i am rs $HOSTNAME\" > /var/www/html/index.html\n\n# 配置arp\necho 1 >/proc/sys/net/ipv4/conf/all/arp_ignore\necho 2 >/proc/sys/net/ipv4/conf/all/arp_announce\n\nexport VIP=10.23.59.162\n\n# 配置vip网卡(用dummy和lo都可以)\nip link add vip type dummy\nip addr add $VIP dev vip\n\n\nClient\n\n添加路由，原则来说因为在同一个交换机中直接通过二层,但是有些vpc的子网不是通过vpc创建的ip不会转发\n也可以查看云厂商的文档如vip或者辅助ip等来作为vip\nexport VIP=10.23.59.162\nexport DR=10.23.12.87\n\nip r add $VIP/32 via $DR dev eth0\n\n\n测试\n\n[root@10-23-18-81 ~]# curl 10.23.59.162\ni am rs 10-23-197-23\n[root@10-23-18-81 ~]# curl 10.23.59.162\ni am rs 10-23-9-198\n\n流量抓包\n\n图中可看出dr将mac地址换成类似rs的mac地址\n隧道(IPIP)隧道(IPIP)部署\nDR配置\n\nexport VIP=10.23.59.162\nexport RS1=10.23.197.23\nexport RS2=10.23.9.198\n\nipvsadm -A -t $VIP:80 -s rr\nipvsadm -a -t $VIP:80 -r $RS1:80 -i\nipvsadm -a -t $VIP:80 -r $RS2:80 -i\n\n\nRS\n\n# 加载内部模块\nmodprobe ipip\n\nexport VIP=10.23.59.162\n# 将vip添加到ipip隧道网卡\nip addr add $VIP dev tunl0\nip link set tunl0 up\n\n# 修改内核参数\necho \"1\" >/proc/sys/net/ipv4/conf/tunl0/arp_ignore\necho \"2\" >/proc/sys/net/ipv4/conf/tunl0/arp_announce\n\necho \"1\" >/proc/sys/net/ipv4/conf/all/arp_ignore\necho \"2\" >/proc/sys/net/ipv4/conf/all/arp_announce\n\necho \"0\" > /proc/sys/net/ipv4/conf/tunl0/rp_filter\necho \"0\" > /proc/sys/net/ipv4/conf/all/rp_filter\n\nIPIP流量分析\n\n由于实验环境在同一个网段所以需要对arp响应进行处理\n内核参数内核参数的文档\nhttps://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt\narp_ignore设置使用那个网卡的mac和ip用来请求arp,即arp请求的src_mac和src_ip\n\n0(默认):将任何网卡的地址响应出去，而不关系该ip是否在接受的网卡上\n1:只响应目标ip接受网卡的地址\n2:只响应目标ip接受网卡的地址，且需要在同网段\n3:请求的地址作用域为host的不响应，只有作用域为global和link的才响应\n4-7:保留\n8:任何arp都不响应\n\n在dr模式中每个rs上都配置了vip的地址如果不设置arp_ignore为1则会响应vip的arp请求使客户端的请求绕过了ds直接到了rs\narp_announce设置使用那个网卡的mac和ip用来响应arp请求,既arp响应的dst_mac和dst_ip\n\n0:使用任何配置在本接口上的地址响应\n1:尽量避免使用不属于该发送网卡子网的本地地址作为发送arp请求的源IP地址\n2:忽略IP数据包的源IP地址，选择该发送网卡上最合适的本地地址作为arp请求的源IP地址\n\nrp_filter反向路径过滤\n\n0:不校验\n1(默认):严格的校验，每个数据包都进行校验，校验反向路径通过特定的接口是否是最佳路径，如不是则丢弃\n2:宽松模式。只校验通过任意接口是否可达，如果不通则丢弃\n取conf&#x2F;{all,interface}&#x2F;rp_filter中的最大值\n\n负载均衡算法\nrr（轮询）\nwrr（权重）\nlc（最后连接）\nwlc（权重）\nlblc（本地最后连接）\nlblcr（带复制的本地最后连接）\ndh（目的地址哈希）\nsh（源地址哈希）\nsed（最小期望延迟）\nnq（永不排队）\n\nipvsadm常用命令# 查看规则\nipvsadm -L\n\n# 查看指定规则\nipvsadm -L -t 10.0.0.1:80\n\n# 查看链接\nipvsadm -l -c\n\n# 清理所有规则\nipvsadm -c\n\n# 清空计数器\nipvsadm -Z\n\n# 添加一个虚拟服务器，算法为轮询\nipvsadm -A -t 10.0.0.1:80 -s rr\n\n# 删除一个虚拟服务,同时删除RS\nipvsadm -D -t 10.0.0.1:80\n\n# 修改一个服务，将算法修改为wlc\nipvsadm -E -t 10.0.0.1:80 -s wlc\n\n# 添加一个RS,nat模式\nipvsadm -a -t 10.0.0.1:80 -r 192.168.32.129:80 -m\n\n# 添加一个RS,路由模式,权重为3\nipvsadm -a -t 10.0.0.1:80 -r 192.168.32.129:80 -g -w 3\n\n# 添加一个RS,ipip隧道模式\nipvsadm -a -t 10.0.0.1:80 -r 192.168.32.129:80 -i\n\n# 修改rs 将此rs的模式改为ipip权重为2\nipvsadm -e -t 10.0.0.1:80 -r 192.168.32.129:80 -i -w 2\n\n# 删除一个RS\nipvsadm -d -t 10.0.0.1:80 -r 192.168.32.129:80\n\n# 查看转发情况\nipvsadm -L -n -c\n\n# 保存配置\nipvsadm -S -n >ipvs.conf\n\n# 读取配置\nipvsadm -R &lt; ipvs.conf\n\n参考资料https://www.cnblogs.com/laolieren/p/lvs_explained.htmlhttps://www.cnblogs.com/klb561/p/9215667.htmlhttps://www.jianshu.com/p/734640384fdahttps://www.cnblogs.com/f-ck-need-u/p/8472744.html\n","tags":["网络","负载均衡"]},{"title":"ip命令基本使用","url":"/2021/05/02/ip%E5%91%BD%E4%BB%A4%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"ip 命令是linux中常用的网络配置命令\n\n\n\nip命令是iproute2包中的命令\n\n安装\n一般发行版再带ip命令\n\n# macos\nbrew install iproute2mac\n\n# ubuntu\napt install iproute2\n\n# centos\nyum install iproute2\n\n设备(device)\n主要是配置OSI模型中的第二层数据链路层\n\n查看设备# 显示所有\nip link show\n\n详细显示\nip -s link show\n\n操作设备# 开启网卡\nip link set ens33 up\n\n# 关闭网卡\nip link set ens33 down\n\n# 开启网卡的混合模式\nip link set ens33 promisc on\n\n# 关闭网卡的混个模式\nip link set ens33 promisc offi\n\n# 设置网卡队列长度\nip link set ens33 txqueuelen 1200\n\n# 设置网卡最大传输单元\nip link set ens33 mtu 1400\n\n# 修改名字\nip link set ens33 name eth0\n\n# 修改网卡的MAC地址\nip link set ens33 address aa:aa:aa:aa:aa:aa\n\n# 将 ens33 连接到vbr0网桥\nip link set ens33 master vbr0\n\nIP相关配置查看IP# 显示所有IP地址\nip address\n# 简写\nip addr \n\n# 显示指定网卡的IP\nip addr ens\n\n# 详细显示指定网卡的IP\nip -s addr ens33\n\n2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 00:0c:29:d9:89:c8 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.32.132/24 brd 192.168.32.255 scope global noprefixroute ens33\n       valid_lft forever preferred_lft forever\n    inet6 fe80::20c:29ff:fed9:89c8/64 scope link\n       valid_lft forever preferred_lft forever\n\n\nbroadcast：设定广播位址，如果设定值是 + 表示让系统自动计算；\nlabel：该设备的别名，例如eth0:0；\nscope：这个设备的领域，默认global，通常是以下几个大类：\nglobal：允许来自所有来源的连线；\nsite：仅支持IPv6 ，仅允许本主机的连接；\nlink：仅允许本设备自我连接；\nhost：仅允许本主机内部的连接；\n\n\n\n增加IP# 设置ens33网卡IP地址192.168.1.1\nip addr add 192.168.1.1/24 dev ens33 \n\n删除IP # 删除ens33网卡IP地址\nip addr del 192.168.1.1/24 dev ens33\n\n路由相关配置查看路由# 显示系统路由\nip route show\n\n# 简写\nip r\n\ndefault via 192.168.32.2 dev ens33 proto static metric 100                      \n169.254.0.0/16 dev ens33 scope link metric 1000                                 \n172.16.1.0/24 dev docker0 proto kernel scope link src 172.16.1.1                \n192.168.32.0/24 dev ens33 proto kernel scope link src 192.168.32.132 metric 100 \n192.168.49.0/24 dev br-e6a94a27c143 proto kernel scope link src 192.168.49.1    \n192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 linkdown  \n\n# 显示vip这个路由表的路由\nip route show table vip\n\n# 查看某个地址走那条路由\nip route get 114.114.114.114\n\n\nproto：此路由的路由协定，主要有redirect,kernel,boot,static,ra等，其中kernel是直接由核心判断自动设定。\nscope：路由的范围，主要是link，是与本设备有关的直接连接。\n\n增加&#x2F;修改路由# 设置192.168.1.0网段的网关为192.168.1.1数据走eth0接口\nip route add 192.168.1.0/24 via 192.168.1.1 dev eth0\n\n# 设置默认网关为192.168.1.1\nip route add default via 192.168.1.1 dev eth0\n\n删除路由# 删除192.168.1.0网段的网关\nip route del 192.168.1.0/24\n\n# 删除默认路由\nip route del default\n\n# 删除路由\nip route delete 192.168.1.0/24 dev eth0 \n\n网络命名空间查看ip netns\nip netns show\n\n增加# 增加一个叫test的网络命名空间\nip netns add test\n\n删除# 删除一个叫test的网络命名空间\nip netns del test\n\n操作# 在 test ns 下执行ip addr\nip netns exec test ip addr\n\n参考资料https://wangchujiang.com/linux-command/c/ip.htmlhttps://www.jianshu.com/p/7466862382c4\n","tags":["网络"]},{"title":"juicefs基本使用","url":"/2024/05/05/juicefs%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"juicefs高性能分布式文件系统，能将s3，webdav等协议挂到文件系统中和其他接口，采用换数据和数据分离设计\n\n\n\n安装\njuicefs需要fuse，没有安装需要安装\n\n默认安装到 &#x2F;usr&#x2F;local&#x2F;bin\n\n\ncurl -sSL https://d.juicefs.com/install | sh -\n\n基本使用创建文件系统\n使用sqlite3作为元数据存储,名字叫dlfs\n后端使用文件系统，目录为$HOME/.juicefs/local，这也是默认的参数\n\njuicefs format --storage file --bucket $HOME/.juicefs/local sqlite3://dlfs.db dlfs\n# 等效\njuicefs format sqlite3://dlfs.db dlfs\n# 2024/05/05 00:11:26.689029 juicefs[41492] &lt;INFO>: Meta address: sqlite3://dlfs.db [interface.go:497]\n# 2024/05/05 00:11:26.690663 juicefs[41492] &lt;INFO>: Data use file:///home/debian/.juicefs/local/dlfs/ [format.go:471]\n# 2024/05/05 00:11:27.808629 juicefs[41492] &lt;INFO>: Volume is formatted as &#123;\n#   \"Name\": \"dlfs\",\n#   \"UUID\": \"7df826c4-9850-4591-99cd-1b58261a21c4\",\n#   \"Storage\": \"file\",\n#   \"Bucket\": \"/home/debian/.juicefs/local/\",\n#   \"BlockSize\": 4096,\n#   \"Compression\": \"none\",\n#   \"EncryptAlgo\": \"aes256gcm-rsa\",\n#   \"TrashDays\": 1,\n#   \"MetaVersion\": 1,\n#   \"MinClientVersion\": \"1.1.0-A\",\n#   \"DirStats\": true\n# &#125; [format.go:508]\n\n挂载一个卷\n挂载到当前目录下的jfs,默认是前台的\n\njuicefs mount sqlite3://dlfs.db ./jfs\n# 2024/05/05 00:15:55.623202 juicefs[41499] &lt;INFO>: Meta address: sqlite3://dlfs.db [interface.go:497]\n# 2024/05/05 00:15:55.632156 juicefs[41499] &lt;INFO>: Data use file:///home/debian/.juicefs/local/dlfs/ [mount.go:605]\n# 2024/05/05 00:15:55.632923 juicefs[41499] &lt;INFO>: Disk cache (/home/debian/.juicefs/cache/7df826c4-9850-4591-99cd-1b58261a21c4/): capacity (102400 MB), free ratio (10%), max pending pages (15) [disk_cache.go:114]\n# 2024/05/05 00:15:55.887736 juicefs[41499] &lt;INFO>: Create session 1 OK with version: 1.1.2+2024-02-04.8dbd89a [base.go:494]\n# 2024/05/05 00:15:55.890264 juicefs[41499] &lt;INFO>: Prometheus metrics listening on 127.0.0.1:9567 [mount.go:160]\n# 2024/05/05 00:15:55.891592 juicefs[41499] &lt;INFO>: Mounting volume dlfs at ./jfs ... [mount_unix.go:269]\n# 2024/05/05 00:15:55.893449 juicefs[41499] &lt;WARNING>: setpriority: permission denied [fuse.go:431]\n# 2024/05/05 00:15:56.149422 juicefs[41499] &lt;INFO>: OK, dlfs is ready at ./jfs [mount_unix.go:48]\n\n\n-d参数则为后台\n\njuicefs mount -d sqlite3://dlfs.db ./jfs\n# 2024/05/05 00:17:01.165052 juicefs[41512] &lt;INFO>: Meta address: sqlite3://dlfs.db [interface.go:497]\n# 2024/05/05 00:17:01.167909 juicefs[41512] &lt;INFO>: Data use file:///home/debian/.juicefs/local/dlfs/ [mount.go:605]\n# 2024/05/05 00:17:01.168481 juicefs[41512] &lt;INFO>: Disk cache (/home/debian/.juicefs/cache/7df826c4-9850-4591-99cd-1b58261a21c4/): capacity (102400 MB), free ratio (10%), max pending pages (15) [disk_cache.go:114]\n# 2024/05/05 00:17:01.670775 juicefs[41512] &lt;INFO>: OK, dlfs is ready at ./jfs [mount_unix.go:48]\n\n\n开机自动挂载 –update-fstab\n\n卸载挂载点juicefs umount ./jfs\n\n\n实时统计\n\njuicefs profile ./jfs\n\n\n查看状态\n\njuicefs status sqlite3://dlfs.db ./jfs/\n\n\n查看信息\n\njuicefs info ./jfs/\n# ./jfs/ :\n#   inode: 1\n#   files: 1\n#    dirs: 1\n#  length: 0 Bytes\n#    size: 8.00 KiB (8192 Bytes)\n#    path: /\n\n\n调试模式\n\njuicefs debug ./jfs/\n\njuicefs summary ./jfs/\n# ./jfs/: 2                      257.0/s\n# ./jfs/: 8.0 KiB (8192 Bytes)   1.5 MiB/s\n# +------+---------+------+-------+\n# | PATH |   SIZE  | DIRS | FILES |\n# +------+---------+------+-------+\n# | /    | 8.0 KiB |    1 |     1 |\n# | xxxx | 4.0 KiB |    0 |     1 |\n# +------+---------+------+-------+\n\n\n性能测试\n\njuicefs bench ./jfs\n\n使用webdav和mysql\n创建一个文件系统，元数据选择mysql存储选择webdav\n\njuicefs format \\\n--storage webdav \\\n--bucket http://home.naturelr.cc:6089/dav/nas/zxz \\\n--access-key &lt;webdav账号> \\\n--secret-key &lt;webdav密码> \\ \nmysql://&lt;mysql账号>:&lt;mysql密码>@(&lt;mysql地址>:3306)/&lt;mysql数据库> \\\njuicefs\n\n\n挂载目录\n\njuicefs mount \"mysql://&lt;mysql账号>:&lt;mysql密码>@(&lt;mysql地址>:3306)/juicefs\" &lt;挂载路径>\n\n参考资料https://mp.weixin.qq.com/s/Uar9hlaLdlAL6IIhMWNotwhttps://juicefs.com/docs/zh/community/introduction/\n","tags":["存储"]},{"title":"k8s-apiserver中loopback证书源码阅读","url":"/2023/07/09/k8s-apiserver%E4%B8%ADloopback%E8%AF%81%E4%B9%A6%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","content":"apiserver因为loopback证书过期导致一些功能无法使用,\n\n\napiserver启动的时候会生成一个loopback证书，该证书默认只有一年有效期,k8s官方解释说应该每年升级或者重启一次,issues\n但在实际场景当中不能没事就重启或升级apiserver\n生成证书和回环客户端\n入口\n\n.&#x2F;cmd&#x2F;kube-apiserver&#x2F;main.go\n\n然后跳转\n\n./cmd/kube-apiserver/app/server.go\n\n// NewAPIServerCommand creates a *cobra.Command object with default parameters\nfunc NewAPIServerCommand() *cobra.Command &#123;\n    s := options.NewServerRunOptions()\n...\n    return Run(completedOptions, genericapiserver.SetupSignalHandler())\n&#125;\n\n// Run runs the specified APIServer.  This should never exit.\nfunc Run(completeOptions completedServerRunOptions, xstopCh &lt;-chan struct&#123;&#125;) error &#123;\n    // To help debugging, immediately log version\n    klog.Infof(\"Version: %+v\", version.Get())\n\n    klog.InfoS(\"Golang settings\", \"GOGC\", os.Getenv(\"GOGC\"), \"GOMAXPROCS\", os.Getenv(\"GOMAXPROCS\"), \"GOTRACEBACK\", os.Getenv(\"GOTRACEBACK\"))\n\n    server, err := CreateServerChain(completeOptions, stopCh)\n    if err != nil &#123;\n        return err\n    &#125;\n\n    prepared, err := server.PrepareRun()\n    if err != nil &#123;\n        return err\n    &#125;\n\n    return prepared.Run(stopCh)\n&#125;\n\n// CreateServerChain creates the apiservers connected via delegation.\nfunc CreateServerChain(completedOptions completedServerRunOptions, stopCh &lt;-chan struct&#123;&#125;) (*aggregatorapiserver.APIAggregator, error) &#123;\n    // 回环证书在此创建\n    kubeAPIServerConfig, serviceResolver, pluginInitializer, err := CreateKubeAPIServerConfig(completedOptions)\n...\n\nfunc CreateKubeAPIServerConfig(s completedServerRunOptions) (\n    *controlplane.Config,\n    aggregatorapiserver.ServiceResolver,\n    []admission.PluginInitializer,\n    error,\n) &#123;\n...\n    genericConfig, versionedInformers, serviceResolver, pluginInitializers, admissionPostStartHook, storageFactory, err := buildGenericConfig(s.ServerRunOptions, proxyTransport)\n    if err != nil &#123;\n        return nil, nil, nil, err\n    &#125;\n...\n&#125;\n...\n\n// BuildGenericConfig takes the master server options and produces the genericapiserver.Config associated with it\nfunc buildGenericConfig(\n    s *options.ServerRunOptions,\n    proxyTransport *http.Transport,\n) (\n    genericConfig *genericapiserver.Config,\n    versionedInformers clientgoinformers.SharedInformerFactory,\n    serviceResolver aggregatorapiserver.ServiceResolver,\n    pluginInitializers []admission.PluginInitializer,\n    admissionPostStartHook genericapiserver.PostStartHookFunc,\n    storageFactory *serverstorage.DefaultStorageFactory,\n    lastErr error,\n) &#123;\n    genericConfig = genericapiserver.NewConfig(legacyscheme.Codecs)\n    genericConfig.MergedResourceConfig = controlplane.DefaultAPIResourceConfigSource()\n\n    if lastErr = s.GenericServerRunOptions.ApplyTo(genericConfig); lastErr != nil &#123;\n        return\n    &#125;\n\n    // 将生成的回环客户端赋值给genericConfig\n    if lastErr = s.SecureServing.ApplyTo(&amp;genericConfig.SecureServing, &amp;genericConfig.LoopbackClientConfig); lastErr != nil &#123;\n        return\n    &#125;\n...\n&#125;\n...\n\n./k8s.io/apiserver/pkg/server/options/serving_with_loopback.go\n\nfunc (s *SecureServingOptionsWithLoopback) ApplyTo(secureServingInfo **server.SecureServingInfo, loopbackClientConfig **rest.Config) error &#123;\n    if s == nil || s.SecureServingOptions == nil || secureServingInfo == nil &#123;\n        return nil\n    &#125;\n...\n\n    // 将正式放到SNICerts,给http服务使用\n    (*secureServingInfo).SNICerts = append([]dynamiccertificates.SNICertKeyContentProvider&#123;certProvider&#125;, (*secureServingInfo).SNICerts...)\n    secureLoopbackClientConfig, err := (*secureServingInfo).NewLoopbackClientConfig(uuid.New().String(), certPem) // 使用生成的证书创建一个reset客户端\n    switch &#123;\n    // if we failed and there's no fallback loopback client config, we need to fail\n    case err != nil &amp;&amp; *loopbackClientConfig == nil:\n        (*secureServingInfo).SNICerts = (*secureServingInfo).SNICerts[1:]\n        return err\n\n    // if we failed, but we already have a fallback loopback client config (usually insecure), allow it\n    case err != nil &amp;&amp; *loopbackClientConfig != nil:\n\n    default:\n        *loopbackClientConfig = secureLoopbackClientConfig // 传回结构体\n    &#125;\n\n./k8s.io/client-go/util/cert/cert.go\n\n// GenerateSelfSignedCertKey creates a self-signed certificate and key for the given host.\n// Host may be an IP or a DNS name\n// You may also specify additional subject alt names (either ip or dns names) for the certificate.\nfunc GenerateSelfSignedCertKey(host string, alternateIPs []net.IP, alternateDNS []string) ([]byte, []byte, error) &#123;\n    return GenerateSelfSignedCertKeyWithFixtures(host, alternateIPs, alternateDNS, \"\")\n&#125;\n\n\nfunc GenerateSelfSignedCertKeyWithFixtures(host string, alternateIPs []net.IP, alternateDNS []string, fixtureDirectory string) ([]byte, []byte, error) &#123;\n    validFrom := time.Now().Add(-time.Hour) // valid an hour earlier to avoid flakes due to clock skew\n    maxAge := time.Hour * 24 * 365         // one year self-signed certs # 这里就是控制证书过期的时间\n\n    baseName := fmt.Sprintf(\"%s_%s_%s\", host, strings.Join(ipsToStrings(alternateIPs), \"-\"), strings.Join(alternateDNS, \"-\"))\n    certFixturePath := filepath.Join(fixtureDirectory, baseName+\".crt\")\n    keyFixturePath := filepath.Join(fixtureDirectory, baseName+\".key\")\n    if len(fixtureDirectory) > 0 &#123;\n        cert, err := ioutil.ReadFile(certFixturePath)\n        if err == nil &#123;\n            key, err := ioutil.ReadFile(keyFixturePath)\n            if err == nil &#123;\n                return cert, key, nil\n            &#125;\n            return nil, nil, fmt.Errorf(\"cert %s can be read, but key %s cannot: %v\", certFixturePath, keyFixturePath, err)\n        &#125;\n        maxAge = 100 * time.Hour * 24 * 365 // 100 years fixtures\n    &#125;\n...\n    caTemplate := x509.Certificate&#123;\n        SerialNumber: big.NewInt(1),\n        Subject: pkix.Name&#123;\n            CommonName: fmt.Sprintf(\"%s-ca@%d\", host, time.Now().Unix()),\n        &#125;,\n        NotBefore: validFrom,\n        NotAfter:  validFrom.Add(maxAge),\n\n        KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,\n        BasicConstraintsValid: true,\n        IsCA:                  true,\n    &#125;\n...\n\n\n\n到此位置生成的证书和回环客户端完成，其中回环客户端复制给了controlplane.Config.LoopbackClientConfig,证书给了controlplane.Config.SNICerts\n\n使用证书\n使用证书的地方为\n\nstaging/src/k8s.io/apiserver/pkg/server/genericapiserver.go\n\nfunc (s preparedGenericAPIServer) Run(stopCh &lt;-chan struct&#123;&#125;) error &#123;\n...\n    stoppedCh, listenerStoppedCh, err := s.NonBlockingRun(stopHttpServerCh, shutdownTimeout)\n    if err != nil &#123;\n        return err\n    &#125;\n...\n&#125;\n\n\nfunc (s preparedGenericAPIServer) NonBlockingRun(stopCh &lt;-chan struct&#123;&#125;, shutdownTimeout time.Duration) (&lt;-chan struct&#123;&#125;, &lt;-chan struct&#123;&#125;, error) &#123;\n...\n    if s.SecureServingInfo != nil &amp;&amp; s.Handler != nil &#123;\n        var err error\n        stoppedCh, listenerStoppedCh, err = s.SecureServingInfo.Serve(s.Handler, shutdownTimeout, internalStopCh)\n        if err != nil &#123;\n            close(internalStopCh)\n            close(auditStopCh)\n            return nil, nil, err\n        &#125;\n    &#125;\n...\n    s.RunPostStartHooks(stopCh) //启动之前注册的hook\n    if _, err := systemd.SdNotify(true, \"READY=1\\n\"); err != nil &#123;\n        klog.Errorf(\"Unable to send systemd daemon successful start message: %v\\n\", err)\n    &#125;\n&#125;\n\n\n继续跳转到server()\n\nfunc (s *SecureServingInfo) Serve(handler http.Handler, shutdownTimeout time.Duration, stopCh &lt;-chan struct&#123;&#125;) (&lt;-chan struct&#123;&#125;, &lt;-chan struct&#123;&#125;, error) &#123;\n    tlsConfig, err := s.tlsConfig(stopCh) // 这里配置http的证书\n    if err != nil &#123;\n        return nil, nil, err\n    &#125;\n&#125;\n\nfunc (s *SecureServingInfo) tlsConfig(stopCh &lt;-chan struct&#123;&#125;) (*tls.Config, error) &#123;\n    // 创建了基本的tls.config\n    tlsConfig := &amp;tls.Config&#123;\n        // Can't use SSLv3 because of POODLE and BEAST\n        // Can't use TLSv1.0 because of POODLE and BEAST using CBC cipher\n        // Can't use TLSv1.1 because of RC4 cipher usage\n        MinVersion: tls.VersionTLS12,\n        // enable HTTP2 for go's 1.7 HTTP Server\n        NextProtos: []string&#123;\"h2\", \"http/1.1\"&#125;,\n    &#125;\n... \n    // 创建了一个动态证书控制器\n    if s.ClientCA != nil || s.Cert != nil || len(s.SNICerts) > 0 &#123;\n        dynamicCertificateController := dynamiccertificates.NewDynamicServingCertificateController(\n            tlsConfig,\n            s.ClientCA,\n            s.Cert,\n            s.SNICerts,\n            nil, // TODO see how to plumb an event recorder down in here. For now this results in simply klog messages.\n        )\n...\n        for _, sniCert := range s.SNICerts &#123;\n            sniCert.AddListener(dynamicCertificateController)\n            if controller, ok := sniCert.(dynamiccertificates.ControllerRunner); ok &#123;\n                    // runonce to try to prime data.  If this fails, it's ok because we fail closed.\n                    // Files are required to be populated already, so this is for convenience.\n                if err := controller.RunOnce(ctx); err != nil &#123; //\n                    klog.Warningf(\"Initial population of SNI serving certificate failed: %v\", err)\n                &#125;\n            go controller.Run(ctx, 1) // 同步证书\n        \n            &#125;\n        &#125;\n...\n\ntlsConfig.GetConfigForClient = dynamicCertificateController.GetConfigForClient // 设置了这个参数之后，接受到https请求之后会调用这个\n...\n&#125;\n\n\n使用客户端\n回环证书在很多地方回到CreateServerChain这里\n\n// 这里已经有调用了\n    apiExtensionsConfig, err := createAPIExtensionsConfig(*kubeAPIServerConfig.GenericConfig, kubeAPIServerConfig.ExtraConfig.VersionedInformers, pluginInitializer, completedOptions.ServerRunOptions, completedOptions.MasterCount,\n        serviceResolver, webhook.NewDefaultAuthenticationInfoResolverWrapper(kubeAPIServerConfig.ExtraConfig.ProxyTransport, kubeAPIServerConfig.GenericConfig.EgressSelector, kubeAPIServerConfig.GenericConfig.LoopbackClientConfig, kubeAPIServerConfig.GenericConfig.TracerProvider)) // TODO\n    if err != nil &#123;\n        return nil, err\n    &#125;\n\n...\n\n\n最多为hook使用，在http服务启动之后前面注册的hook就开始执行其中传入了回环证书\n\n// RunPostStartHooks runs the PostStartHooks for the server\nfunc (s *GenericAPIServer) RunPostStartHooks(stopCh &lt;-chan struct&#123;&#125;) &#123;\n    s.postStartHookLock.Lock()\n    defer s.postStartHookLock.Unlock()\n    s.postStartHooksCalled = true\n\n    context := PostStartHookContext&#123;\n        LoopbackClientConfig: s.LoopbackClientConfig, //使用了回环\n        StopCh:               stopCh,\n    &#125;\n\n    for hookName, hookEntry := range s.postStartHooks &#123; // 将前面注册的hook全部启动\n        go runPostStartHook(hookName, hookEntry, context)\n    &#125;\n&#125;\n\n问题\n一个服务多个证书，其实就是通过tsl.Config.GetConfigForClient来实现\n\n为什么要loopback,从代码来看apiserve本身也需要请求一个资源，比如校验参数的正确性,如果不请求自己就需要从新写一套从etcd获取的逻辑，这样就逻辑重复了\n\n除了一些零散的调用主要是通过AddPostStartHookOrDie注册的hook在启动后调用\n\n\n参考资料https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzg2NTU3NjgxOA==&amp;action=getalbum&amp;album_id=2958341226519298049&amp;scene=173&amp;from_msgid=2247488299&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect\nhttps://github.com/kubernetes/kubernetes/issues/86552\n","tags":["k8s"]},{"title":"k8s-service网络","url":"/2022/12/14/k8s-service%E7%BD%91%E7%BB%9C/","content":"k8s service是对一组pod进行抽象的方法\n\n\n\npod并不是永久的随时可以销毁的，那么他ip也会变的，这样的话就需要一种方法来作为pod的”前端“来转发到pod上去\n\n实现方式service的ip是虚拟的他的具体实现程序是kube-proxy\n\n如图，流量被service给负载到后端当中，对于用户来说只要访问service即可\nkub-proxy有目前主要有四种\n\n用户空间: 早期的方案，所有策略都在用户空间效率比较差\niptables: iptables在内核空间，主要通过nat实现，由于iptables是一个一个匹配所有规则多时效果延迟比较大\nipvs: 和iptables类似只不过使用了ipvs\nebpf: 最新的技术速度快效率高，但对内核版本要求比较高\n\n目前主流使用iptables和ipvs，所以主要说iptables和ipvs\n\nkube-proxy使用apiVersion: v1\nkind: Service\nmetadata:\n  name: foo\nspec:\n  selector: # 选择需要负载到pod\n    app: foo\n  ports:\n  - port: 80 # 服务的端口\n    targetPort: 80 # pod的端口\n  type: ClusterIP # 类型，默认ClusterIP\n\n将上面的保存为foo.yaml然后执行kubectl apply -f foo.yaml即可创建一个svc\n类型k8s的服务类型拥有很多种，根据实际情况选择\nClusterIP默认的类型,创建一个虚拟的ip并将选择器选择的pod的ip作为这个虚拟ip的后端\nNodePort和ClusterIP基本一致，但是会将端口映射到所有集群中所有的节点上,端口范围默认是3000以上\napiVersion: v1\nkind: Service\nmetadata:\n  name: foo\nspec:\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: foo\n  type: NodePort\n\nHeadless和ClusterIP基本一致，只是没有虚拟ip同时失去了lb的功能,kube-proxy不会处理此svc且dns返回对应ep的所有地址，常用于服务发现\napiVersion: v1\nkind: Service\nmetadata:\n  name: foo\nspec:\n  selector: \n    app: foo\n  clusterIP: None # 指定为none\n  ports:\n  - port: 80 \n    targetPort: 80 \n  type: ClusterIP\n\nLoadBalancer这个类型一般只有云服务商只能使用，创建这个服务的同时在云服务商的lb服务商上创建了一个实例\napiVersion: v1\nkind: Service\nmetadata:\n  name: foo-loadbalancer\nspec:\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    nodePort: 80\n    port: 80\n    protocol: TCP\n    targetPort: 800\n  selector:\n    app: foo\n  sessionAffinity: None\n  type: LoadBalancer\n\nExternalName外部的服务对内部的一个别名，比如web的域名是10.23.83.9.sslip.io，则在集群中可以使用web来访问10.23.83.9.sslip.io,\n没有选择器,kube-proxy不会创建规则,仅在dns层面完成\nexternalName为dns地址如果是ip的话dns无法解析,而且如果是http(s)将可能无法访问，因为你访问的是a,而对应的服务只接受b域名\n解决方法使用类似&lt;sslip.io&gt;这种域名来通过域名动态解析到ip上\napiVersion: v1\nkind: Service\nmetadata:\n  name: web\nspec:\n  externalName: 10.23.83.9.sslip.io\n  ports:\n  - name: tcp\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  sessionAffinity: None\n  type: ExternalName\n\n外部IP可以将一个外部地址导入到集群内部对应的服务里,在本地请求\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: cdebug\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 80\n  externalIPs:\n    - 1.1.1.1\n\n\n使用效果如下\n\n[root@10-23-141-183 ~]# kubectl get svc my-service\nNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nmy-service   ClusterIP   172.17.4.228   1.1.1.1       80/TCP    19h\n[root@10-23-141-183 ~]# curl 1.1.1.1\nClientAddr: 10.23.141.183:52416\nClientReqPath: /\nClientReqMeth: GET\nServerHostName: cdebug-77cc4fc98f-rv9hn\nServerAddr: 10.23.8.140\n[root@10-23-141-183 ~]#                                                                                 \n\n指定后端Endpoints\n可以手动创建可以svc同名的ep来指定一个svc的对应的ep，不需要selector字段且ports名字要一致\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-external-service\nspec:\n  ports:\n    - name: http # 和下面的ep的name要一致\n      protocol: TCP\n      port: 80\n      targetPort: 80\n---\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: my-external-service\nsubsets:\n  - addresses:\n      - ip: 1.1.1.1\n    ports:\n      - port: 80\n        protocol: TCP\n        name: http # 和上面的svc的port名字要一致\n\n流量策略流量策略主要解决在拥有众多ep的服务在转发流量时有些pod距离访问的node比较远导致延迟增大\n目前拥有2种策略，cluster和local\n\ncluster(默认):会将流量转发到所有节点的pod当中,但有可能pod在其他节点或者其他地域上会导致延迟\n\nlocal: 只会将流量转发本地的pod上，不会转发到其他node上的pod,拥有较好的性能\n\n\n\ninternalTrafficPolicy\n主要针对pod访问svc的策略\n\nexternalTrafficPolicy\n针对外面通过node port访问集群的svc\n\n实现iptables规则分析PREROUTING\nPREROUTING链的nat表是所有svc的入口,进入的流量都会到KUBE-SERVICES这条自定义链\n\nroot@minikube:~# iptables -nvL PREROUTING -t nat\nChain PREROUTING (policy ACCEPT 1 packets, 60 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n   57  3492 KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */\n    2   192 DOCKER_OUTPUT  all  --  *      *       0.0.0.0/0            192.168.65.2        \n   57  3420 DOCKER     all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL\n\n\n在KUBE-SERVICES中就是我们定义的svc对应的规则,来源地址是任何目标地址是clusterIP时,匹配对应svc的自定义链\n其中KUBE-NODEPORTS是处理nodePort类型的规则\n\nroot@minikube:~# iptables -nvL KUBE-SERVICES -t nat \nChain KUBE-SERVICES (2 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  *      *       0.0.0.0/0            10.96.0.1            /* default/kubernetes:https cluster IP */ tcp dpt:443\n    0     0 KUBE-SVC-JD5MR3NA4I4DYORP  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153\n    0     0 KUBE-SVC-TCOU7JCQXEZGVUNU  udp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns cluster IP */ udp dpt:53\n    0     0 KUBE-SVC-ERIFXISQEP7F7OF4  tcp  --  *      *       0.0.0.0/0            10.96.0.10           /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53\n    0     0 KUBE-SVC-ZZYI5KMAZUYAMTQ6  tcp  --  *      *       0.0.0.0/0            10.98.178.225        /* default/cdebug cluster IP */ tcp dpt:80\n 1043 62580 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL\n\n\n在KUBE-SVC-ZZYI5KMAZUYAMTQ6这条自定义链中定义了具体的nat地址,当原地址不是10.244.0.0&#x2F;16目标是clusterIP时会进入一个打标签的自定义链\n下面2条是每一条分别对应svc的ep,进入第一条的几率是50%通过random,实现了负载均衡\n10.244.0.0&#x2F;16是kube-proxy的clusterCIDR设置,作用是区别流量是否是pod的流量以用来直接访问svc参考https://blog.csdn.net/shida_csdn/article/details/104334372\n\nroot@minikube:~# iptables -nvL KUBE-SVC-ZZYI5KMAZUYAMTQ6 -t nat\nChain KUBE-SVC-ZZYI5KMAZUYAMTQ6 (2 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.244.0.0/16        10.98.178.225        /* default/cdebug cluster IP */ tcp dpt:80\n    0     0 KUBE-SEP-36ZFR6ZLFG6NGI5P  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/cdebug -> 172.17.0.4:80 */ statistic mode random probability 0.50000000000\n    0     0 KUBE-SEP-4E32UMZN7V2DQATS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/cdebug -> 172.17.0.5:80 */\n\n\n在KUBE-MARK-MASQ是个标记的链他会将流量打上0x4000标签,该流量将在PODTROUTNG链中的KUBE-POSTROUTING被执行snat\n\nroot@minikube:~# iptables -nvL KUBE-MARK-MASQ -t nat\nChain KUBE-MARK-MASQ (8 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000\n\n\n这里对来源是172.17.0.4(ep)的进行打标签，同时进行了DNAT到了ep\n\nroot@minikube:~# iptables -nvL KUBE-SEP-36ZFR6ZLFG6NGI5P -t nat\nChain KUBE-SEP-36ZFR6ZLFG6NGI5P (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-MARK-MASQ  all  --  *      *       172.17.0.4           0.0.0.0/0            /* default/cdebug */\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/cdebug */ tcp to:172.17.0.4:80\n\n\nKUBE-NODEPORTS下是存放nodePort的规则,打标签之后进入了svc\n\nroot@minikube:~# iptables -nvL KUBE-NODEPORTS -t nat\nChain KUBE-NODEPORTS (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-EXT-ZZYI5KMAZUYAMTQ6  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/cdebug */ tcp dpt:32753\n\n\n具体内容和KUBE-SERVICES里一样的规则复用了每个svc创建的链\n\nroot@minikube:~# iptables -nvL KUBE-EXT-ZZYI5KMAZUYAMTQ6  -t nat\nChain KUBE-EXT-ZZYI5KMAZUYAMTQ6 (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    2   120 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* masquerade traffic for default/cdebug external destinations */\n    2   120 KUBE-SVC-ZZYI5KMAZUYAMTQ6  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n\nINPUT\n这里主要实现了一些防火墙规则\n\nroot@minikube:~# iptables -nvL INPUT -t filter\nChain INPUT (policy ACCEPT 279K packets, 37M bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 647K   39M KUBE-PROXY-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes load balancer firewall */\n  75M   10G KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes health check service ports */\n 647K   39M KUBE-EXTERNAL-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */\n  75M   10G KUBE-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0\n\nroot@minikube:~# iptables -nvL  KUBE-PROXY-FIREWALL -t filter \nChain KUBE-PROXY-FIREWALL (3 references)\n pkts bytes target     prot opt in     out     source               destination         \n\nroot@minikube:~# iptables -nvL  KUBE-NODEPORTS -t filter\nChain KUBE-NODEPORTS (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n\nroot@minikube:~# iptables -nvL  KUBE-EXTERNAL-SERVICES -t filter\nChain KUBE-EXTERNAL-SERVICES (2 references)\n pkts bytes target     prot opt in     out     source               destination\n\n\n这个将丢弃标签为0x8000的流量还有源头不是127.0.0.0/8目标是127.0.0.0/8状态是RELATED,ESTABLISHED,DNAT的流量\n\nroot@minikube:~# iptables -nvL KUBE-FIREWALL -t filter\nChain KUBE-FIREWALL (2 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       all  --  *      *      !127.0.0.0/8          127.0.0.0/8          /* block incoming localnet connections */ ! ctstate RELATED,ESTABLISHED,DNAT\n    0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes firewall for dropping marked packets */ mark match 0x8000/0x8000\n\nFORWARDroot@minikube:~# iptables -nvL FORWARD -t filter\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 KUBE-PROXY-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes load balancer firewall */\n    0     0 KUBE-FORWARD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */\n    0     0 KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */\n    0     0 KUBE-EXTERNAL-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes externally-visible service portals */\n    0     0 DOCKER-USER  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n    0     0 DOCKER-ISOLATION-STAGE-1  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED\n    0     0 DOCKER     all  --  *      docker0  0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     all  --  docker0 !docker0  0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     all  --  docker0 docker0  0.0.0.0/0            0.0.0.0/0  \n\nroot@minikube:~# iptables -nvL  KUBE-FORWARD  -t filter\nChain KUBE-FORWARD (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate INVALID\n    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding rules */ mark match 0x4000/0x4000\n    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes forwarding conntrack rule */ ctstate RELATED,ESTABLISHED\n\nOUTPUT\n这里继续会有一个KUBE-SERVICES这个链用于处理节点上访问svc\n\nroot@minikube:~# iptables -nvL OUTPUT -t nat\nChain OUTPUT (policy ACCEPT 3995 packets, 240K bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 748K   45M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */\n  130  8116 DOCKER_OUTPUT  all  --  *      *       0.0.0.0/0            192.168.65.2        \n 499K   30M DOCKER     all  --  *      *       0.0.0.0/0           !127.0.0.0/8          ADDRTYPE match dst-type LOCAL\n\nroot@minikube:~# iptables -nvL OUTPUT -t filter\nChain OUTPUT (policy ACCEPT 368K packets, 49M bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 748K   45M KUBE-PROXY-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes load balancer firewall */\n 748K   45M KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ctstate NEW /* kubernetes service portals */\n  75M 9999M KUBE-FIREWALL  all  --  *      *       0.0.0.0/0            0.0.0.0/0           \n\nPOSTROUTING\n整个流程中最后一个地点，这里主要对前面打了0x4000/0x4000的标签进行snat\n\nroot@minikube:~# iptables -nvL POSTROUTING -t nat   \nChain POSTROUTING (policy ACCEPT 4112 packets, 247K bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 748K   45M KUBE-POSTROUTING  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes postrouting rules */\n    0     0 MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0           \n    0     0 DOCKER_POSTROUTING  all  --  *      *       0.0.0.0/0            192.168.65.2\n\nroot@minikube:~# iptables -nvL KUBE-POSTROUTING -t nat\nChain KUBE-POSTROUTING (1 references)\n pkts bytes target     prot opt in     out     source               destination         \n 4253  255K RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000\n    0     0 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000\n    0     0 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ random-fully\n\n概览\n\n流程图\n\n\n集群外访问NodePort集群外部一台服务器(10.23.83.9)通过nodePort请求集群的cdebug这个服务\ncdebug服务详情\n❯ k get svc -o wide\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR\ncdebug       NodePort    172.17.102.133   &lt;none>        80:32577/TCP   5d2h   app=cdebug\nkubernetes   ClusterIP   172.17.0.1       &lt;none>        443/TCP        5d2h   &lt;none>\n\n❯ k get po -l app=cdebug -o wide\nNAME                      READY   STATUS    RESTARTS   AGE    IP            NODE            NOMINATED NODE   READINESS GATES\ncdebug-77cc4fc98f-rv9hn   1/1     Running   0          5d1h   10.23.8.140   10.23.142.106   &lt;none>           &lt;none>\n\n❯ kubectl get po nginx -o wide\nNAME    READY   STATUS    RESTARTS   AGE   IP              NODE            NOMINATED NODE   READINESS GATES\nnginx   1/1     Running   0          22h   10.23.246.131   10.23.142.106   &lt;none>           &lt;none>\n\n\n这是在请求的node上抓包\n\n发送\n10.23.83.9:36848 > 10.23.142.106:32577\n10.23.142.106:36848 > 10.23.8.140:80\n\n返回\n10.23.8.140:80 > 10.23.142.106:36848\n10.23.142.106:32577 > 10.23.83.9:36848\n\n\n可以看到客户端请求集群的nodePort，iptables目的地址改为对应的pod地址,源地址则是nodePort的地址,所以pod不知道真正的客户端是谁即pod看到的请求来自请求的nodePort所在的node这里做了\n\n在netfilter流向,这里充当了一个路由器防火墙\n\n\n集群内节点访问NodePort和集群外访问NodePort差不多,最终pod的看到的源地址为访问所在的node的地址\nPOD访问SVC发送10.23.246.131:38906 &gt; 172.17.102.133:8010.23.246.131:38906 &gt; 10.23.8.140:80\n响应10.23.8.140:80    &gt; 10.23.246.131:38906172.17.102.133:80 &gt; 10.23.246.131:38906\n\n路径和访问NodePort差不都只不过没有做SNAT只做了DNAT,也就是说集群POD访问SVC是可以知道客户端的真实地址\nkube-proxy修改为ipvs\n去人内核模块是否有ipvs\n\nlsmos|grep ip_vs\n\n\n根据情况是否安装ipvsadm管理工具\n\nyum install ipvsadm\n\n\nkube-proxy配置文件中mode,改为ipvs即可切换为ipvs,如果是ds运行的话需要重启pod\n\nipvs总体来说ipvs相比iptables要简单一些\n\n创建了一个名为kube-ipvs0的dummy类型网卡，该网卡上会有所有CluserIP的地址,用于让该ip的流量进入协议栈，不然网卡发现本地没该地址流量被丢弃\n\niptables上会通过创建一些规则,使用ipset提升性能\n\n创建ipvs转发规则,使用nat模式\n\n\n分析\n一个svc叫cdebug详情如下\n\n# kubectl get svc cdebug\nNAME     TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\ncdebug   NodePort   172.17.102.133   &lt;none>        80:32577/TCP   25d\n# kubectl get ep  cdebug\nNAME     ENDPOINTS          AGE\ncdebug   10.23.169.107:80   25d\n\n网卡\n节点上的kube-ipvs0网卡上可以看到这个svc的ClusterIP\n\n# ip -s addr show kube-ipvs0 |grep 172.17.102.133\n    inet 172.17.102.133/32 scope global kube-ipvs0\n\nipvs规则\n同时ipvs的规则中rs为svc对应的ep也就是pod的地址转发规则为rr\n\n# ipvsadm -Ln -t 172.17.102.133:80\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  172.17.102.133:80 rr\n  -> 10.23.169.107:80             Masq    1      0          0\n\niptable规则\nipvs下依然使用了iptables\n使用ipset模块来过滤添加标记,并在POSTROUTING上做snat\n和iptables模式一样分别在OUTPUT和PREROUTING上调用了KUBE-SERVICES\nKUBE-SERVICES和iptables模式相比没有了负载均衡功能(交给了ipvs)\nipvs模式吧所有的流量都做snat\n\n# iptables -t nat -nvL KUBE-SERVICES\nChain KUBE-SERVICES (2 references)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 KUBE-LOAD-BALANCER  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* Kubernetes service lb portal */ match-set KUBE-LOAD-BALANCER dst,dst\n    0     0 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* Kubernetes service cluster ip + port for masquerade purpose */ match-set KUBE-CLUSTER-IP src,dst\n    0     0 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* Kubernetes service external ip + port for masquerade and filter purpose */ match-set KUBE-EXTERNAL-IP dst,dst\n    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* Kubernetes service external ip + port for masquerade and filter purpose */ match-set KUBE-EXTERNAL-IP dst,dst PHYSDEV match ! --physdev-is-in ADDRTYPE match src-type !LOCAL\n    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* Kubernetes service external ip + port for masquerade and filter purpose */ match-set KUBE-EXTERNAL-IP dst,dst ADDRTYPE match dst-type LOCAL\n   39  2148 KUBE-NODE-PORT  all  --  *      *       0.0.0.0/0            0.0.0.0/0            ADDRTYPE match dst-type LOCAL\n    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            match-set KUBE-CLUSTER-IP dst,dst\n    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            match-set KUBE-LOAD-BALANCER dst,dst\n\n如何选择当svc比较多的时候选择ipvs比较好，否则iptables\n参考https://docs.ucloud.cn/uk8s/userguide/kubeproxy_mode\n参考资料https://kubernetes.io/zh/docs/concepts/services-networking/service/https://draveness.me/kubernetes-service/\n","tags":["k8s","网络"]},{"title":"k8s中创建8ks-vcluster","url":"/2022/12/19/k8s%E4%B8%AD%E5%88%9B%E5%BB%BA8ks-vcluster/","content":"vcluster是一个kink(k8s in k8s)程序即在k8s里创建k8s,和ns相比主要可以随便安装crd等资源且可以获得类似完整的集群体验\n\n\n安装\nmacos\n\ncurl -L -o vcluster \"https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-darwin-arm64\" &amp;&amp; sudo install -c -m 0755 vcluster /usr/local/bin &amp;&amp; rm -f vcluster\n\n\n其他的https://www.vcluster.com/docs/getting-started/setup\n\n原理\n其主要原理是在宿主集群中启动了一个轻量k8s-k3s,然后将k3s的端口通过宿主集群的svc暴漏,在虚拟集群中创建的资源通过同步到宿主集群中由宿主集群来创建具体的pod\n常用操作\n创建集群,创建完成之后会在当前的kubeconfig中添加虚拟机集群的context\n\nvcluster create &lt;集群名字&gt;\n\n\n查看所有集群\n\nvcluster list\n\n\n删除集群\n\nvcluster list &lt;集群名字>\n\n\n连接集群\n\nvcluster connect &lt;集群名字>\n\n参考资料https://www.vcluster.com/docs/what-are-virtual-clusters\n","tags":["k8s","部署"]},{"title":"k8s中安装gitlab-runner","url":"/2021/12/17/k8s%E4%B8%AD%E5%AE%89%E8%A3%85gitlab-runenr/","content":"记录下在k8s中安装gitlab-runner\n\n\n获取注册token\n全局runner：管理员界面-&gt;概览—&gt;runner-&gt;左上角(&#x2F;admin&#x2F;runners)\n\n组runner：组界面-&gt;设置-&gt;CI&#x2F;CD-&gt;展开runner(&#x2F;groups&#x2F;&lt;组名&gt;&#x2F;-&#x2F;settings&#x2F;ci_cd)\n\n项目runner：项目界面-&gt;设置-&gt;CI&#x2F;CD-&gt;展开runner(&lt;组名&gt;&#x2F;&lt;项目名&gt;&#x2F;-&#x2F;settings&#x2F;ci_cd)\n\n\n添加helmc仓库helm repo add gitlab https://charts.gitlab.io\n\n解压chart包\n解压他的包为了得到完整的values.yaml，这个文件里面说的很详细的一些配置\n\nhelm pull gitlab/gitlab-runner\ntar -xvf gitlab-runner-0.35.3.tgz\n\n修改参数\n修改gitlabUrl的地址为你的gitlab地址\n\n配置runnerRegistrationToken为你的token\n\n配置tags字段，可以在在选择性\n\n增加权限，这里直接给所有权限\n\n\nrbac:\n  create: true\n  rules: \n   - resources: [\"*\"]\n     verbs: [\"*\"]\n   - apiGroups: [\"\"]\n     resources: [\"*\"]\n     verbs: [\"*\"]\n\n安装gitlabhelm install --namespace gitlab gitlab-runner -f values.yaml gitlab/gitlab-runner \n\nhelm upgrade --namespace gitlab -f values.yaml gitlab-runner gitlab&#x2F;gitlab-runner\n\n参考资料https://docs.gitlab.com/runner/install/kubernetes.html\n","tags":["cicd"]},{"title":"k8s使用nfs持久化数据","url":"/2022/07/16/k8s%E4%BD%BF%E7%94%A8nfs%E6%8C%81%E4%B9%85%E5%8C%96%E6%95%B0%E6%8D%AE/","content":"nfs是们常用的远程存储，这里记录下k8s安装nfs\n\n\n部署nfs服务器安装nfs工具\n\nyum -y install nfs-utils\nsystemctl start nfs &amp;&amp; systemctl enable nfs\n\n创建nfs的目录\nmkdir -p /data/nfs/ &amp;&amp; chmod -R 777 /data/nfs\n\n# 设置共享目录\necho \"/data/nfs *(rw,no_root_squash,sync)\" >> /etc/exports\n# 应用配置\nexportfs -r\n# 查看配置\nexportfs\n\n启动nfs服务\nsystemctl restart rpcbind &amp;&amp; systemctl enable rpcbind\nsystemctl restart nfs &amp;&amp; systemctl enable nfs\n\n# 查看 RPC 服务的注册状况\nrpcinfo -p localhost\n\n# 测试一下\nshowmount -e 192.168.32.133\n\nk8s安装nfs驱动官方仓库https://github.com/kubernetes-csi/csi-driver-nfs\n官方默认的镜像在国内是无法访问，需要转储到国内的仓库里，建议找台香港的机器或者科学上网\n\n\nregistry.k8s.io/sig-storage/csi-provisioner:v3.2.0\nregistry.k8s.io/sig-storage/livenessprobe:v2.7.0\nregistry.k8s.io/sig-storage/csi-node-driver-registrar:v2.5.1\ngcr.io/k8s-staging-sig-storage/nfsplugin:canary\n\n在线安装curl -skSL https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/deploy/install-driver.sh | bash -s master --\n\n本地安装git clone https://github.com/kubernetes-csi/csi-driver-nfs.git\ncd csi-driver-nfs\n./deploy/install-driver.sh master local\n\n等待所有pod running\nkubectl -n kube-system get pod  |grep nfs\n\n部署存储类对象cat &lt;&lt;EOF > nfs-cs.yml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs\nparameters:\n  server: 192.168.32.133 # nfs服务器地址\n  share: /data/nfs # nfs共享的目录\nprovisioner: nfs.csi.k8s.io\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nEOF\n\nkubectl apply -f nfs-cs.yml\n\n# 将nfs-csi 设置为默认存储类\nkubectl patch storageclass nfs-csi -p '&#123;\"metadata\": &#123;\"annotations\":&#123;\"storageclass.kubernetes.io/is-default-class\":\"true\"&#125;&#125;&#125;'\n\n测试部署静态pv\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-nfs\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: nfs-csi\n  mountOptions:\n    - nfsvers=3\n  csi:\n    driver: nfs.csi.k8s.io\n    readOnly: false\n    volumeHandle: unique-volumeid  # make sure it's a unique id in the cluster\n    volumeAttributes:\n      server: 192.168.32.133\n      share: /data/nfs\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: pvc-nfs-static\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\n  volumeName: pv-nfs\n  storageClassName: nfs-csi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: test\n      mountPath: /data\n  volumes:\n  - name: test\n    persistentVolumeClaim:\n      claimName: pvc-nfs-static\n\n参考资料https://github.com/kubernetes-csi/csi-driver-nfs\n","tags":["k8s","存储"]},{"title":"k8s动态准入控制","url":"/2021/06/17/k8s%E5%8A%A8%E6%80%81%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/","content":"准入控制是k8s中用来提供安全控制的一个控制器，而动态控制则是用户定制的安全策略\n\n\n种类\n动态准入控制分为两种，分别为Mutating，Validating\n\nMutatingMutating主要为修改性质的，在api调用完成之后k8s会根据ValidatingWebhookConfiguration中的条件发送给配置的webhook服务，webhook服务根据业务逻辑进行修改，比如说大名鼎鼎的istio的Sidecar注入就是于此\nValidatingValidating主要为验证性质的，主要看是不是符合条件集群要求，比方说为了高可用不允许设置副本数为1的类型为deployment的请求\n架构\n下图所显的是api请求的流程\n\n编写webhook创建证书\n创建证书的的程序很多比较出名的是openssl，这里我们使用rancher提供的一个自动生成证书的脚本\n\n1. 将下面的脚本保存为create_self-signed-cert.sh#!/bin/bash -e\n\nhelp ()\n&#123;\n    echo  ' ================================================================ '\n    echo  ' --ssl-domain: 生成ssl证书需要的主域名，如不指定则默认为www.rancher.local，如果是ip访问服务，则可忽略；'\n    echo  ' --ssl-trusted-ip: 一般ssl证书只信任域名的访问请求，有时候需要使用ip去访问server，那么需要给ssl证书添加扩展IP，多个IP用逗号隔开；'\n    echo  ' --ssl-trusted-domain: 如果想多个域名访问，则添加扩展域名（SSL_TRUSTED_DOMAIN）,多个扩展域名用逗号隔开；'\n    echo  ' --ssl-size: ssl加密位数，默认2048；'\n    echo  ' --ssl-cn: 国家代码(2个字母的代号),默认CN;'\n    echo  ' 使用示例:'\n    echo  ' ./create_self-signed-cert.sh --ssl-domain=www.test.com --ssl-trusted-domain=www.test2.com \\ '\n    echo  ' --ssl-trusted-ip=1.1.1.1,2.2.2.2,3.3.3.3 --ssl-size=2048 --ssl-date=3650'\n    echo  ' ================================================================'\n&#125;\n\ncase \"$1\" in\n    -h|--help) help; exit;;\nesac\n\nif [[ $1 == '' ]];then\n    help;\n    exit;\nfi\n\nCMDOPTS=\"$*\"\nfor OPTS in $CMDOPTS;\ndo\n    key=$(echo $&#123;OPTS&#125; | awk -F\"=\" '&#123;print $1&#125;' )\n    value=$(echo $&#123;OPTS&#125; | awk -F\"=\" '&#123;print $2&#125;' )\n    case \"$key\" in\n        --ssl-domain) SSL_DOMAIN=$value ;;\n        --ssl-trusted-ip) SSL_TRUSTED_IP=$value ;;\n        --ssl-trusted-domain) SSL_TRUSTED_DOMAIN=$value ;;\n        --ssl-size) SSL_SIZE=$value ;;\n        --ssl-date) SSL_DATE=$value ;;\n        --ca-date) CA_DATE=$value ;;\n        --ssl-cn) CN=$value ;;\n    esac\ndone\n\n# CA相关配置\nCA_DATE=$&#123;CA_DATE:-3650&#125;\nCA_KEY=$&#123;CA_KEY:-cakey.pem&#125;\nCA_CERT=$&#123;CA_CERT:-cacerts.pem&#125;\nCA_DOMAIN=cattle-ca\n\n# ssl相关配置\nSSL_CONFIG=$&#123;SSL_CONFIG:-$PWD/openssl.cnf&#125;\nSSL_DOMAIN=$&#123;SSL_DOMAIN:-'www.rancher.local'&#125;\nSSL_DATE=$&#123;SSL_DATE:-3650&#125;\nSSL_SIZE=$&#123;SSL_SIZE:-2048&#125;\n\n## 国家代码(2个字母的代号),默认CN;\nCN=$&#123;CN:-CN&#125;\n\nSSL_KEY=$SSL_DOMAIN.key\nSSL_CSR=$SSL_DOMAIN.csr\nSSL_CERT=$SSL_DOMAIN.crt\n\necho -e \"\\033[32m ---------------------------- \\033[0m\"\necho -e \"\\033[32m       | 生成 SSL Cert |       \\033[0m\"\necho -e \"\\033[32m ---------------------------- \\033[0m\"\n\nif [[ -e ./$&#123;CA_KEY&#125; ]]; then\n    echo -e \"\\033[32m ====> 1. 发现已存在CA私钥，备份\"$&#123;CA_KEY&#125;\"为\"$&#123;CA_KEY&#125;\"-bak，然后重新创建 \\033[0m\"\n    mv $&#123;CA_KEY&#125; \"$&#123;CA_KEY&#125;\"-bak\n    openssl genrsa -out $&#123;CA_KEY&#125; $&#123;SSL_SIZE&#125;\nelse\n    echo -e \"\\033[32m ====> 1. 生成新的CA私钥 $&#123;CA_KEY&#125; \\033[0m\"\n    openssl genrsa -out $&#123;CA_KEY&#125; $&#123;SSL_SIZE&#125;\nfi\n\nif [[ -e ./$&#123;CA_CERT&#125; ]]; then\n    echo -e \"\\033[32m ====> 2. 发现已存在CA证书，先备份\"$&#123;CA_CERT&#125;\"为\"$&#123;CA_CERT&#125;\"-bak，然后重新创建 \\033[0m\"\n    mv $&#123;CA_CERT&#125; \"$&#123;CA_CERT&#125;\"-bak\n    openssl req -x509 -sha256 -new -nodes -key $&#123;CA_KEY&#125; -days $&#123;CA_DATE&#125; -out $&#123;CA_CERT&#125; -subj \"/C=$&#123;CN&#125;/CN=$&#123;CA_DOMAIN&#125;\"\nelse\n    echo -e \"\\033[32m ====> 2. 生成新的CA证书 $&#123;CA_CERT&#125; \\033[0m\"\n    openssl req -x509 -sha256 -new -nodes -key $&#123;CA_KEY&#125; -days $&#123;CA_DATE&#125; -out $&#123;CA_CERT&#125; -subj \"/C=$&#123;CN&#125;/CN=$&#123;CA_DOMAIN&#125;\"\nfi\n\necho -e \"\\033[32m ====> 3. 生成Openssl配置文件 $&#123;SSL_CONFIG&#125; \\033[0m\"\ncat > $&#123;SSL_CONFIG&#125; &lt;&lt;EOM\n[req]\nreq_extensions = v3_req\ndistinguished_name = req_distinguished_name\n[req_distinguished_name]\n[ v3_req ]\nbasicConstraints = CA:FALSE\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment\nextendedKeyUsage = clientAuth, serverAuth\nEOM\n\nif [[ -n $&#123;SSL_TRUSTED_IP&#125; || -n $&#123;SSL_TRUSTED_DOMAIN&#125; ]]; then\n    cat >> $&#123;SSL_CONFIG&#125; &lt;&lt;EOM\nsubjectAltName = @alt_names\n[alt_names]\nEOM\n    IFS=\",\"\n    dns=($&#123;SSL_TRUSTED_DOMAIN&#125;)\n    dns+=($&#123;SSL_DOMAIN&#125;)\n    for i in \"$&#123;!dns[@]&#125;\"; do\n      echo DNS.$((i+1)) = $&#123;dns[$i]&#125; >> $&#123;SSL_CONFIG&#125;\n    done\n\n    if [[ -n $&#123;SSL_TRUSTED_IP&#125; ]]; then\n        ip=($&#123;SSL_TRUSTED_IP&#125;)\n        for i in \"$&#123;!ip[@]&#125;\"; do\n          echo IP.$((i+1)) = $&#123;ip[$i]&#125; >> $&#123;SSL_CONFIG&#125;\n        done\n    fi\nfi\n\necho -e \"\\033[32m ====> 4. 生成服务SSL KEY $&#123;SSL_KEY&#125; \\033[0m\"\nopenssl genrsa -out $&#123;SSL_KEY&#125; $&#123;SSL_SIZE&#125;\n\necho -e \"\\033[32m ====> 5. 生成服务SSL CSR $&#123;SSL_CSR&#125; \\033[0m\"\nopenssl req -sha256 -new -key $&#123;SSL_KEY&#125; -out $&#123;SSL_CSR&#125; -subj \"/C=$&#123;CN&#125;/CN=$&#123;SSL_DOMAIN&#125;\" -config $&#123;SSL_CONFIG&#125;\n\necho -e \"\\033[32m ====> 6. 生成服务SSL CERT $&#123;SSL_CERT&#125; \\033[0m\"\nopenssl x509 -sha256 -req -in $&#123;SSL_CSR&#125; -CA $&#123;CA_CERT&#125; \\\n    -CAkey $&#123;CA_KEY&#125; -CAcreateserial -out $&#123;SSL_CERT&#125; \\\n    -days $&#123;SSL_DATE&#125; -extensions v3_req \\\n    -extfile $&#123;SSL_CONFIG&#125;\n\necho -e \"\\033[32m ====> 7. 证书制作完成 \\033[0m\"\necho\necho -e \"\\033[32m ====> 8. 以YAML格式输出结果 \\033[0m\"\necho \"----------------------------------------------------------\"\necho \"ca_key: |\"\ncat $CA_KEY | sed 's/^/  /'\necho\necho \"ca_cert: |\"\ncat $CA_CERT | sed 's/^/  /'\necho\necho \"ssl_key: |\"\ncat $SSL_KEY | sed 's/^/  /'\necho\necho \"ssl_csr: |\"\ncat $SSL_CSR | sed 's/^/  /'\necho\necho \"ssl_cert: |\"\ncat $SSL_CERT | sed 's/^/  /'\necho\n\necho -e \"\\033[32m ====> 9. 附加CA证书到Cert文件 \\033[0m\"\ncat $&#123;CA_CERT&#125; >> $&#123;SSL_CERT&#125;\necho \"ssl_cert: |\"\ncat $SSL_CERT | sed 's/^/  /'\necho\n\necho -e \"\\033[32m ====> 10. 重命名服务证书 \\033[0m\"\necho \"cp $&#123;SSL_DOMAIN&#125;.key tls.key\"\ncp $&#123;SSL_DOMAIN&#125;.key tls.key\necho \"cp $&#123;SSL_DOMAIN&#125;.crt tls.crt\"\ncp $&#123;SSL_DOMAIN&#125;.crt tls.crt\n\n\n2. 然后执行下面的命令./create_self-signed-cert.sh --ssl-domain=admission-example.admission-example.svc.cluster.local  --ssl-trusted-domain=admission-example,admission-example.admission-example.svc -ssl-trusted-ip=127.0.0.1\n\n3. 会在目录里生成一套证书和秘钥\n.key的为秘钥\n.crt为域名的证书\ncsr文件为证书申请文件\nca开头的为根证书和秘钥\n\n编写yaml文件\n编写MutatingWebhookConfiguration和ValidatingWebhookConfiguration\n\napiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: mutating-example\n  labels:\n    app: admission-example\nwebhooks:\n  - name: admission-example.naturelr.cc\n    clientConfig:\n      service:\n        name: admission-example\n        namespace: admission-example\n        path: \"/mutate\"\n        port: 8080\n      # 证书进行base64编码\n      caBundle: &#123;&#123;CA&#125;&#125;\n    rules:\n      - operations: [ \"CREATE\" ]\n        apiGroups: [\"apps\", \"\"]\n        apiVersions: [\"v1\"]\n        resources: [\"deployments\",\"services\"]\n    admissionReviewVersions: [\"v1\", \"v1beta1\"]\n    sideEffects: None\n    # 只有ns上拥有admission-webhook-example: enabled才生效\n    namespaceSelector:\n      matchLabels:\n        admission-webhook-example: enabled\n---\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: validation-example\n  labels:\n    app: admission-example\nwebhooks:\n  - name: admission-example.naturelr.cc\n    clientConfig:\n      service:\n        name: admission-example\n        namespace: admission-example\n        path: \"/validate\"\n        port: 8080\n      caBundle: &#123;&#123;CA&#125;&#125;\n    rules:\n      - operations: [ \"CREATE\" ]\n        apiGroups: [\"apps\", \"\"]\n        apiVersions: [\"v1\"]\n        resources: [\"deployments\",\"services\"]\n    admissionReviewVersions: [\"v1\", \"v1beta1\"]\n    sideEffects: None\n    namespaceSelector:\n      matchLabels:\n        admission-webhook-example: enabled\n\n开发webhook\n开发上面定义的两个接口validate，mutate\n\n监听的端口和上面配置的端口一直，且使用创建的证书\n...\n  http.HandleFunc(\"/validate\", validate)\n  http.HandleFunc(\"/mutate\", mutate)\n  http.HandleFunc(\"/ping\", func(w http.ResponseWriter, r *http.Request) &#123;\n    fmt.Fprintln(w, \"pong\")\n  &#125;)\n\n  svr := http.Server&#123;\n    Addr:         \":8080\",\n    ReadTimeout:  time.Minute,\n    WriteTimeout: time.Minute,\n  &#125;\n  go func() &#123;\n    if *key == \"\" || *cert == \"\" &#123;\n      fmt.Println(\"http服务启动成功\")\n      if err := svr.ListenAndServe(); err != nil &#123;\n        log.Fatalln(err)\n      &#125;\n    &#125;\n    fmt.Println(\"https服务启动成功\")\n    if err := svr.ListenAndServeTLS(*cert, *key); err != nil &#123;\n      log.Fatalln(err)\n  &#125;()  \n    &#125;\n\n\n实现mutate的部分，我们需要给满足条件的deployment和service添加一个名为admission-example.naturelr.cc/status&quot;: &quot;test&quot;的注解这里和使用kubectl操作上很像只不过由代码返回给k8s\n\nfunc mutate(w http.ResponseWriter, r *http.Request) &#123;\n  // 请求结构体\n  qar := admissionv1.AdmissionReview&#123;&#125;\n  _, _, err := serializer.NewCodecFactory(runtime.NewScheme()).UniversalDeserializer().Decode(body, nil, &amp;qar)\n  checkErr(err)  \n  type patchOperation struct &#123;\n    Op    string      `json:\"op\"`\n    Path  string      `json:\"path\"`\n    Value interface&#123;&#125; `json:\"value,omitempty\"`\n  &#125;  \n  p := patchOperation&#123;\n    Op:    \"add\",\n    Path:  \"/metadata/annotations\",\n    Value: map[string]string&#123;\"admission-example.naturelr.cc/status\": \"test\"&#125;,\n  &#125;\n  patch, err := json.Marshal([]patchOperation&#123;p&#125;)\n  checkErr(err)\n\n  // 返回给k8s的消息\n  are := &amp;admissionv1.AdmissionReview&#123;\n    TypeMeta: apimetav1.TypeMeta&#123;\n      APIVersion: qar.APIVersion,\n      Kind:       qar.Kind,\n    &#125;,\n    Response: &amp;admissionv1.AdmissionResponse&#123;\n      Allowed: true,\n      Patch:   patch,\n      PatchType: func() *admissionv1.PatchType &#123;\n        pt := admissionv1.PatchTypeJSONPatch\n        return &amp;pt\n      &#125;(),\n      UID: qar.Request.UID,\n    &#125;,\n  &#125;\n\n  resp, err := json.Marshal(are)\n  checkErr(err)\n  fmt.Println(\"响应:\", string(resp))\n  w.WriteHeader(200)\n  w.Write(resp)\n&#125;\n\n\nvalidate中主要验证service和deployment中标签是否有admission字段如果就没有则拒绝访问\n\nfunc validate(w http.ResponseWriter, r *http.Request) &#123;\n    // 请求结构体\n  qar := admissionv1.AdmissionReview&#123;&#125;\n  _, _, err := serializer.NewCodecFactory(runtime.NewScheme()).UniversalDeserializer().Decode(body, nil, &amp;qar)\n  checkErr(err\n  // 处理逻辑 从请求的结构体判断是是否满足条件\n  var  availableLabels map[string]string\n  \n  requiredLabels := \"admission\"\n  var errMsg error\n  switch qar.Request.Kind.Kind &#123;\n  case \"Deployment\":\n    var deploy appsv1.Deployment\n    if err := json.Unmarshal(qar.Request.Object.Raw, &amp;deploy); err != nil &#123;\n      log.Println(\"无法解析格式:\", err)\n      errMsg = err\n    &#125;\n    availableLabels = deploy.Labels\n  case \"Service\":\n    var service corev1.Service\n    if err := json.Unmarshal(qar.Request.Object.Raw, &amp;service); err != nil &#123;\n      log.Println(\"无法解析格式:\", err)\n      errMsg = err\n    &#125;\n    availableLabels = service.Labels\n  default:\n    msg := fmt.Sprintln(\"不能处理的类型：\", qar.Request.Kind.Kind)\n    log.Println(msg)\n    errMsg = errors.New(msg)\n  &#125;\n\n  var status *apimetav1.Status\n  var allowed bool\n  if _, ok := availableLabels[requiredLabels]; !ok || errMsg != nil &#123;\n    msg := \"不符合条件\"\n    if err != nil &#123;\n        msg = fmt.Sprintln(errMsg)\n    &#125;\n  &#125;\n  status = &amp;apimetav1.Status&#123;\n      Message: msg,\n      Reason:  apimetav1.StatusReason(msg),\n      Code:    304,\n    &#125;\n    allowed = false\n  &#125; else &#123;\n    Message: \"通过\",\n    status = &amp;apimetav1.Status&#123;\n     Reason:  \"通过\",\n     Code:    200,\n    &#125;\n    allowed = true\n  &#125;\n\n  // 返回给k8s的消息\n  are := &amp;admissionv1.AdmissionReview&#123;\n    TypeMeta: apimetav1.TypeMeta&#123;\n      APIVersion: qar.APIVersion,\n      Kind:       qar.Kind,\n    &#125;,\n    Response: &amp;admissionv1.AdmissionResponse&#123;\n      Allowed: allowed,\n      Result:  status,\n      UID:     qar.Request.UID,\n    &#125;,\n  &#125;\n\n  resp, err := json.Marshal(are)\n  checkErr(err)\n  fmt.Println(\"响应:\", string(resp))\n  w.WriteHeader(200)\n  w.Write(resp)\n\n完整项目在https://github.com/NatureLR/admission-example\n测试验证\n在打了admission-webhook-example: enabled标签下的ns中随便创建一个应用会发现被拒绝\n在给deployment打上了设定的标签之后就可以创建了，且deployment多了一个注解\n\n参考资料https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/\n","tags":["k8s"]},{"title":"k8s节点管理","url":"/2020/01/17/k8s%E8%8A%82%E7%82%B9%E7%AE%A1%E7%90%86/","content":"节点管理\n\n\n查看节点#查看节点基本信息\nkubectl get nodes\n\n#查看节点详细信息\nkubectl get nodes &lt;节点名字> -o wide\nkubectl describe nodes &lt;节点名字>\n\n节点调度#停止向此节点调度\nkubectl cordon &lt;节点名字>\n\n#将此节点上的所有容器驱逐到其他节点\nkubectl drain &lt;节点名字>\n\n#恢复向此节点调度pod\nkubectl uncordon &lt;节点名字>\n\n标签#打标签\nkubectl label nodes &lt;节点名字> &lt;标签key>=&lt;标签val>  \n\n#删除节点标签\nkubectl label nodes &lt;节点名字> &lt;标签key>- \n\n删除节点# 驱逐节点上的pod\nkubectl drain &lt;节点> --delete-local-data --force --ignore-daemonsets\n\n# 删除节点\nkubectl delete nodes &lt;节点>\n","tags":["k8s"]},{"title":"karmada安装和基本使用","url":"/2024/04/01/karmada%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"karmada是k8s的一个多集群实现方案\n\n\n\n安装\n下载二进制客户端文件https://github.com/karmada-io/karmada/releases\n\nwget https://github.com/karmada-io/karmada/releases/download/v1.9.0/karmadactl-linux-amd64.tgz\n\n\n也可以安装 kubectl的插件\n\nkubectl krew install karmada\n\n\n安装karmadactl到集群中\n\nkarmadactl init \n\n\n安装完成之后默认会在目录/etc/karmada/添加这个虚拟集群的相关文件\n\n多集群成员push模式添加成员\n添加集群push模式，–kubeconfig决定要加入的集群\n\nkarmadactl --kubeconfig /etc/karmada/karmada-apiserver.config join cluster1  --cluster-kubeconfig=$HOME/.kube/config\n\n查看集群kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters\n\npull模式添加成员\n获取注册命令和凭据\n\nkarmadactl  token create --print-register-command --kubeconfig /etc/karmada/karmada-apiserver.config\n# karmadactl register 10.7.143.254:32443 --cluster-name cluster2 --token po8les.a05eqne2hqwy8gly --discovery-token-ca-cert-hash sha256:3bfb29c846092b61af5bb51a47a88bd52ed834d8468158a7fb341180d5a3bc74\n\n删除集群kubectl --kubeconfig /etc/karmada/karmada-apiserver.config delete cluster cluster2\n\n应用分发\n首先查看下有那些集群\n\nkubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters\n# NAME         VERSION    MODE   READY   AGE\n# cluster1     v1.24.12   Push   True    10d\n# kubernetes   v1.24.12   Pull   True    9d\n\n\n部署分发策略\n\n# propagationpolicy.yaml\napiVersion: policy.karmada.io/v1alpha1\nkind: PropagationPolicy\nmetadata:\n  name: example-policy # The default namespace is `default`.\nspec:\n  resourceSelectors:\n    - apiVersion: apps/v1\n      kind: Deployment\n      name: nginx # If no namespace is specified, the namespace is inherited from the parent object scope.\n  placement:\n    clusterAffinity:\n      clusterNames:\n        - cluster1\n        - kubernetes\n\n\n部署上面的分发策略\n\nkubectl --kubeconfig /etc/karmada/karmada-apiserver.config apply -f propagationpolicy.yaml\n\n\n其中一个集群中部署一个测试的nginx\n\nkubectl create deployment nginx --image nginx\n\n\n在karmada中也创建一个测试的\n\nkubectl --kubeconfig /etc/karmada/karmada-apiserver.config create deployment nginx --image nginx\n\n\n在第二个集群中可以看到同步过来了，此时同步完成\n\nkubectl --kubeconfig=./2.yaml get po\n# NAME                    READY   STATUS    RESTARTS   AGE\n# nginx-8f458dc5b-w526c   1/1     Running   0          5m43s\n\n\n当我们删除karmada中的nginx\n\nkubectl --kubeconfig /etc/karmada/karmada-apiserver.config delete deploy\n# deployment.apps \"nginx\" deleted\n\n# 集群1的资源还在\nkubectl get po\n# NAME                    READY   STATUS    RESTARTS   AGE\n# nginx-8f458dc5b-t6z4k   1/1     Running   0          7m35s\n\n# 集群2的资源不在了\nkubectl --kubeconfig=./2.yaml get po\n# No resources found in default namespace.\n\n\n将集群1的测试资源也删除\n\nkubectl delete deploy nginx\n# deployment.apps \"nginx\" deleted\n\n\n我们只在karmada中部署nginx测试\n\nkubectl --kubeconfig /etc/karmada/karmada-apiserver.config create deployment nginx --image nginx\n#deployment.apps/nginx created\n\n# 可以看到 ready是2/1 因为2个集群都同步了\nkubectl --kubeconfig /etc/karmada/karmada-apiserver.config get deploy  \n# NAME    READY   UP-TO-DATE   AVAILABLE   AGE\n# nginx   2/1     2            2           15s\n\n# 使用karmadactl查看\nkarmadactl --kubeconfig=/etc/karmada/karmada-apiserver.config  get po\n# NAME                     CLUSTER      READY   STATUS    RESTARTS   AGE\n# nginx-74f96fcc5c-zphtd   cluster1     1/1     Running   0          6m6s\n# nginx-8f458dc5b-mkpx9    kubernetes   1/1     Running   0          11m\n\n\n从上面的现象可以看到同步的时候karmada集群中必须有需要同步资源,当成员集群已经有了该资源则在删除的时候不会删除\n\n覆盖应用\n覆盖主要用于重写一些属性\n\n\n保存下面的覆盖策略为overridepolicy.yaml\n\n# overridepolicy.yaml\napiVersion: policy.karmada.io/v1alpha1\nkind: OverridePolicy\nmetadata:\n  name: example\nspec:\n  resourceSelectors:\n    - apiVersion: apps/v1\n      kind: Deployment\n      name: nginx\n      labelSelector:\n        matchLabels:\n          app: nginx\n  overrideRules:\n    - targetCluster:\n        clusterNames:\n          - cluster1\n      overriders:\n        imageOverrider:\n          - component: Tag\n            operator: replace\n            value: '1.20'\n\n\napply这个策略到karmada集群\n\nkubectl --kubeconfig /etc/karmada/karmada-apiserver.config apply -f overridepolicy.yaml\n\n\n查看集群1的nginx已经被改为1.20版本了\n\nkubectl  get deploy -o wide \n# NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES       SELECTOR\n# nginx   1/1     1            1           6m12s   nginx        nginx:1.20   app=nginx\n\n参考资料https://karmada.io/zh/docs\n","tags":["k8s","多集群","karmada"]},{"title":"kibana总是创建index-pattern","url":"/2022/01/13/kibana%E6%80%BB%E6%98%AF%E5%88%9B%E5%BB%BAindex-pattern/","content":"在kibana界面点击创建index pattern失败一直让创建index pattern\n\n\n\n今天在做升级修复log4j时升级之后打开kibana界面创建index pattern总是创建不出来，将es删除重建也不行，看日志也没发现一些错误，于是就想是不是kibana的问题，于是重启kibana和删除es中一些kibana的索引解决了\n\n1.删除es索引# 先查看所有索引\ncurl 127.0.0.1:9200/_cat/indices?v\n\n可能还有其他类似.kibana_1之类的我也删除了\n#删除kibana的索引\ncurl -XDELETE 127.0.0.1:9200/.kibana?pretty\n\n2.重启kibana容器kubectl rollout restart deployment kibana\n","tags":["kibana"]},{"title":"knative-serving","url":"/2024/05/15/knative-serving/","content":"knative是2018年开源的一个Serverless，和普通的k8s原生工作负载相比可以做到0pod，在收到请求时再拉起pod\n\n\n架构\nserving主要负责提供服务，快速部署容器基于istio处理路由，只是缩容到0\n\n\n\n流量图\n\n\n安装部署安装istio\n从https://github.com/istio/istio/releases下载istioctl\n\n安装istio到集群\n\n\nistioctl install -y\n\n安装serving\n需要注意是否兼容的k8s版本\n\n\n安装serving\n\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.14.0/serving-crds.yaml\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.14.0/serving-core.yaml\n\n\n网络层\n\nkubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.14.0/net-istio.yaml\n\n\n由于镜像在gcr上可能无法启动，可以实现拉取到国内的一些仓库上\n\nkubectl -n knative-serving get po        \n# NAME                                    READY   STATUS    RESTARTS   AGE\n# activator-58f4bb476b-l7sjw              1/1     Running   0          3m3s\n# autoscaler-84f556f484-xzwff             1/1     Running   0          3m3s\n# controller-858485bfbd-jtrpk             1/1     Running   0          3m3s\n# net-istio-controller-56d6d4f8ff-s7z5l   1/1     Running   0          2m\n# net-istio-webhook-bfd966868-mjcxh       1/1     Running   0          2m\n# webhook-6655bf6cbd-ktfv8                1/1     Running   0          3m3s\n\n基本使用\n部署一个服务\n\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\nspec:\n  template:\n    metadata:\n      labels:\n        app: helloworld-go\n      annotations:\n        autoscaling.knative.dev/target: \"10\"\n    spec:\n      containers:\n        - image: registry.cn-hangzhou.aliyuncs.com/knative-sample/helloworld-go:160e4dc8\n          ports:\n            - name: http1\n              containerPort: 8080\n          env:\n            - name: TARGET\n              value: \"World\"\n\n\n查看服务\n\nkubectl get ksvc \n# NAME            URL                                              LATESTCREATED         LATESTREADY           READY   REASON\n# helloworld-go   http://helloworld-go.default.svc.cluster.local   helloworld-go-00001   helloworld-go-00001   True    \n\n\n在集群中任意一个pod中访问，出现hello world 则表示测试成功，仔细观察在没有请求之后则pod会被删除\n\ncurl http://helloworld-go.default.svc.cluster.local\n# Hello World!\n\n外部访问配置\n在配置域名之后默认就开启了外部访问\n\n\n集群外部访问需要配置外部域名,删除默认的_example,然后添加外部的域名,如下yaml示例\n生产中要将这个域名解析到isito的ingress gateway\n同时可以通过标签选择对那些服务生效\n\napiVersion: v1\ndata:\n  naturelr.cc: \"\"\nkind: ConfigMap\nmetadata:\n  name: config-domain\n  namespace: knative-serving\n\n\n查看knative服务可以发现url发生了变化\n\nk get ksvc                             \n# NAME            URL                                        LATESTCREATED         LATESTREADY           READY   REASON\n# helloworld-go   http://helloworld-go.default.naturelr.cc   helloworld-go-00001   helloworld-go-00001   True    \n\n\n访问测试,这里使用nodeport代替\n\ncurl -H \"Host: helloworld-go.default.naturelr.cc\" http://192.168.49.2:31733\n# Hello World!\n\nCR之间的关系链kubectl tree ksvc helloworld-go\n# NAMESPACE  NAME                                                               READY  REASON  AGE\n# default    Service/helloworld-go                                              True           30m\n# default    ├─Configuration/helloworld-go                                      True           30m\n# default    │ └─Revision/helloworld-go-00001                                   True           30m\n# default    │   ├─Deployment/helloworld-go-00001-deployment                    -              30m\n# default    │   │ └─ReplicaSet/helloworld-go-00001-deployment-7b8765df6b       -              30m\n# default    │   │   └─Pod/helloworld-go-00001-deployment-7b8765df6b-zqrt5      True           6s \n# default    │   ├─Image/helloworld-go-00001-cache-user-container               -              30m\n# default    │   └─PodAutoscaler/helloworld-go-00001                            True           30m\n# default    │     ├─Metric/helloworld-go-00001                                 True           30m\n# default    │     └─ServerlessService/helloworld-go-00001                      True           30m\n# default    │       ├─Endpoints/helloworld-go-00001                            -              30m\n# default    │       │ └─EndpointSlice/helloworld-go-00001-qbm6x                -              30m\n# default    │       ├─Service/helloworld-go-00001                              -              30m\n# default    │       └─Service/helloworld-go-00001-private                      -              30m\n# default    │         └─EndpointSlice/helloworld-go-00001-private-vhvfq        -              30m\n# default    └─Route/helloworld-go                                              True           30m\n# default      ├─Ingress/helloworld-go                                          True           30m\n# default      │ ├─VirtualService/helloworld-go-ingress                         -              30m\n# default      │ └─VirtualService/helloworld-go-mesh                            -              30m\n# default      └─Service/helloworld-go                                          -              30m\n\n灰度\n发布v1版本(上面创建的需要删除)\n\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\nspec:\n  template:\n    metadata:\n      labels:\n        app: helloworld-go-v1\n      annotations:\n        autoscaling.knative.dev/target: \"10\"\n      name: helloworld-go-v1\n    spec:\n      containers:\n        - image: registry.cn-hangzhou.aliyuncs.com/knative-sample/helloworld-go:160e4dc8\n          ports:\n            - name: http1\n              containerPort: 8080\n          env:\n            - name: TARGET\n              value: \"World v1\"\n\n\n发布v2版本\n\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: helloworld-go\nspec:\n  template:\n    metadata:\n      labels:\n        app: helloworld-go-v2\n      annotations:\n        autoscaling.knative.dev/target: \"10\"\n      name: helloworld-go-v2 # 必须要以—为版本分隔符\n    spec:\n      containers:\n        - image: registry.cn-hangzhou.aliyuncs.com/knative-sample/helloworld-go:160e4dc8\n          ports:\n            - name: http1\n              containerPort: 8080\n          env:\n            - name: TARGET\n              value: \"World v2\" # 区分v1\n  traffic: # 灰度流量百分比\n    - tag: v1\n      revisionName: helloworld-go-v1\n      percent: 20\n    - tag: v2\n      revisionName: helloworld-go-v2\n      percent: 80\n    - tag: latest\n      latestRevision: true\n      percent: 0\n\n\n请求效果\n\n# root@d5ce4828c4d3:/# curl -H \"Host:  helloworld-go.default.naturelr.cc\" http://192.168.49.2:31733\n# Hello World v2!\n# root@d5ce4828c4d3:/# curl -H \"Host:  helloworld-go.default.naturelr.cc\" http://192.168.49.2:31733\n# Hello World v1!\n# root@d5ce4828c4d3:/# curl -H \"Host:  helloworld-go.default.naturelr.cc\" http://192.168.49.2:31733\n# Hello World v2!\n# root@d5ce4828c4d3:/# curl -H \"Host:  helloworld-go.default.naturelr.cc\" http://192.168.49.2:31733\n# Hello World v1!\n\n伸缩配置\n流量模式分为恐慌模式和稳定模式\n\n稳定模式是以60s为一个周期统计平均并发数，且达到目标并发数的2倍则触发\n恐慌模式是以6s为一个周期统计平均并发数\n\n\n全局配置在这里修改\n\n\nk -n knative-serving  get cm config-autoscaler\n\n\n最大伸缩和最小伸缩,以及目标\n\nspec:\n  template:\n    metadata:\n      autoscaling.knative.dev/target: \"10\"\n      autoscaling.knative.dev/minScale: \"2\"\n      autoscaling.knative.dev/maxScale: \"10\"\n\n参考资料https://knative.dev/docs/https://knative-sample.com/\n","tags":["knative","serverless"]},{"title":"kubeadm修改证书过期时间","url":"/2022/08/19/kubeadm%E4%BF%AE%E6%94%B9%E8%AF%81%E4%B9%A6%E8%BF%87%E6%9C%9F%E6%97%B6%E9%97%B4/","content":"kubeadm搭建的集群证书默认ca是时间，其他组件的证书是一年如果一年没有执行升级的操作就会过期\n\n\n\n所以一劳永逸直接修改kubeadm证书时间\n\n下载源码git clone -b v1.27.3 https://github.com/kubernetes/kubernetes.git\n\n修改证书时间\nca 证书\n\ncode ./staging/src/k8s.io/client-go/util/cert/cert.go\n\nNewSelfSignedCACert这个函数的NotAfter字段\n// NewSelfSignedCACert creates a CA certificate\nfunc NewSelfSignedCACert(cfg Config, key crypto.Signer) (*x509.Certificate, error) &#123;\n  now := time.Now()\n  tmpl := x509.Certificate&#123;\n    SerialNumber: new(big.Int).SetInt64(0),\n    Subject: pkix.Name&#123;\n      CommonName:   cfg.CommonName,\n      Organization: cfg.Organization,\n    &#125;,\n    DNSNames:              []string&#123;cfg.CommonName&#125;,\n    NotBefore:             now.UTC(),\n    NotAfter:              now.Add(duration365d * 10).UTC(), // 需要修改的地方\n    KeyUsage:              x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign,\n    BasicConstraintsValid: true,\n    IsCA:                  true,\n  &#125;\n\n  certDERBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;tmpl, &amp;tmpl, key.Public(), key)\n  if err != nil &#123;\n    return nil, err\n  &#125;\n  return x509.ParseCertificate(certDERBytes)\n&#125;\n\n\nloopback证书\n\nfunc GenerateSelfSignedCertKeyWithFixtures(host string, alternateIPs []net.IP, alternateDNS []string, fixtureDirectory string) ([]byte, []byte, error) &#123;\n  validFrom := time.Now().Add(-time.Hour) // valid an hour earlier to avoid flakes due to clock skew\n  maxAge := time.Minute * 10// one year self-signed certs 这里需要修改\n\n  baseName := fmt.Sprintf(\"%s_%s_%s\", host, strings.Join(ipsToStrings(alternateIPs), \"-\"), strings.Join(alternateDNS, \"-\"))\n  certFixturePath := filepath.Join(fixtureDirectory, baseName+\".crt\")\n  keyFixturePath := filepath.Join(fixtureDirectory, baseName+\".key\")\n...\n\n\n组件证书\n\ncode ./cmd/kubeadm/app/constants/constants.go\n\nCertificateValidity这个变量\n// CertificateValidity defines the validity for all the signed certificates generated by kubeadm\nCertificateValidity = time.Hour * 24 * 365 // 需要修改的地方\n\n重新编译make all WHAT=cmd/kubeadm GOFLAGS=-v\n\nmake all WHAT=cmd/kube-apiserver GOFLAGS=-v\n\n编译好的二进制文件中在_output/bin/\n建议先使用yum等工具安装官方的kubeadm之后进行二进制替换\n参考资料https://blog.51cto.com/legehappy/4895615\n","tags":["k8s","kubeadm","证书"]},{"title":"kubeadm搭建高可用集群","url":"/2021/09/04/kubeadm%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/","content":"kubeadm是官方的一个用来管理k8s集群的工具\n\n\n\nkubeadm，虽然是官方的是使用起来也不是很方便，他需要在每个节点上进行安装，在大规模的时候需要借助其他工具\n\n环境信息说明\n4台2c2g虚拟机,官方要求最少2c4g但是我的机器没这么高配置，如果仅仅是学习的话够用了\n系统为centos7\nlb方案为了方便使用hosts文件，生产环境请使用lvs,haproxy,nginx等方案\n默认为最新版本\n\n节点初始化\n所有节点无论master和node\n\n设置主机名字和PS1为主机IP\n为了方便统一设置主机名为ip地址\n\necho 'export PS1=\"[\\u@\\H \\W]\\$ \"' >> .bashrc\n\nIP=$(ip addr show $(ip route |grep default |awk '&#123;print$5&#125;') |grep -w inet |awk -F '[ /]+' '&#123;print $3&#125;')\nhostnamectl set-hostname $IP\n\n关闭swap交换分区# 临时关闭\nswapoff -a\n\n# 永久关闭\nsed -ri 's/.*swap.*/#&amp;/' /etc/fstab\n\n关闭selinuxsetenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disable/g' /etc/selinux/config\n\n关闭防火墙systemctl stop firewalld &amp;&amp; systemctl disable firewalld\n\n同步时间ntpdate cn.pool.ntp.org\n\nyum源默认源很慢，改为阿里云的\n修改centos7源为阿里云curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo\n\n修改centos7 epel源为阿里云curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\n\n安装k8s源官方的国内不可用，使用阿里云的\ncat &lt;&lt;EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n安装CRI安装dockerk8s 1.24之后无法直接支持docker,需要安装cri-dockerd\n\n\n# 添加配置文件\n\ncat &lt;&lt;EOF > /etc/docker/daemon.json \n&#123;\n    \"oom-score-adjust\": -1000,\n    \"log-driver\": \"json-file\",\n    \"log-opts\": &#123;\n        \"max-size\": \"100m\",\n        \"max-file\": \"3\"\n    &#125;,\n    \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n    \"live-restore\": true,\n    \"max-concurrent-downloads\": 10,\n    \"max-concurrent-uploads\": 10,\n    \"registry-mirrors\": [\"http://hub-mirror.c.163.com\",\"https://registry.docker-cn.com\",\"https://docker.mirrors.ustc.edu.cn\"],\n    \"storage-driver\": \"overlay2\",\n    \"storage-opts\": [\n        \"overlay2.override_kernel_check=true\"\n    ]\n&#125;\nEOF\n\n# 安装docker\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n\n# 重启docker\nsystemctl restart docker &amp;&amp; systemctl enable docker\n\n安装containerd# 加载内核模块\ncat &lt;&lt; EOF > /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n\nmodprobe overlay\nmodprobe br_netfilter\n\n# 使用阿里的源\nwget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n\nyum install containerd.io-1.6.6-3.1.el7.x86_64.rpm\n\n安装k8s组件yum install -y kubelet kubeadm kubectl\nsystemctl enable kubelet &amp;&amp; systemctl start kubelet\n\n# ipvs模式推荐安装\nyum install -y ipvsadm\n\n安装master# 执行master节点初始化\nkubeadm init \\\n    --control-plane-endpoint \"k8s-api:6443\" \\\n    --upload-certs \\\n    --image-repository registry.aliyuncs.com/google_containers \\\n    --pod-network-cidr=172.16.1.0/16 \\\n    --v=6\n\n# 初始化完成之后会打印出加入集群的命令\n\n\n加入集群的命令可以使用kubeadm重新获取,参考后面kubeadm\n其他两个master节点kubeadm join k8s-api:6443 --token iq5o5t.8mtwj9117qhed25p \\\n       --discovery-token-ca-cert-hash sha256:95fda448e3cb56303efc3bccbc785e000c3124a9a045ff2ed33c854cb9ee3108 \\\n       --control-plane --certificate-key f075fe20e799440297bf9bd48942134da1c95f1c00ef94d7d208a2a66ce87bda\n\n安装工作节点kubeadm join k8s-api:6443 --token iq5o5t.8mtwj9117qhed25p \\\n        --discovery-token-ca-cert-hash sha256:95fda448e3cb56303efc3bccbc785e000c3124a9a045ff2ed33c854cb9ee3108\n\ncni\nk8s支持很多cni，这里使用了最简单的flannel\n\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\n清理master上的污点\n默认安装完成之后master是有污点的\n\nkubectl taint nodes &lt;节点名字> node-role.kubernetes.io/master:NoSchedule-\n\n\n新版本叫control-plane\n\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\n\nmetrics-server\nmetrics-server提供了最基础的metrics手机，使用kubectl top和hpa时需要他，当然也可以使用kube-prometheus代理\n\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\ningress\ningress官方只是定义了crd，具体实现由第三方实现，这里使用了常见的nginx-ingreses\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/baremetal/deploy.yaml\n\n# 使用helm\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm install ingress-nginx ingress-nginx/ingress-nginx\n\ndashboardkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml\n\n删除清理\nmaster要保持奇数！\n\n\n驱逐节点上的pod\n\nkubectl drain &lt;节点> --delete-local-data --force --ignore-daemonsets\n\n\n删除节点\n\nkubectl delete &lt;节点>\n\n\n在要删除的节点上执行\n\nkubeadm reset\n\n\n清理iptables规则\n\niptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X\n\n\n如果使用了ipvs模式\n\nipvsadm -C\n\n\n清理安装目录和文件\n\nrm -rf ~/.kube\nrm -rf /opt/cni\nrm -rf /etc/cni\nrm -rf /etc/kubernetes\nrm -rf /var/etcd # master节点才有\n\n\n卸载组件\n\nyum remove kube*\n\n\n重启\n\nreboot\n\n升级版本\nk8s升级版本最大不能跨越两个次版本，其版本通过二进制的版本来确定要通过kubeadm去每个节点上执行\n\nmaster节点yum -y update kubeadm kubelet kubectl\n\n# 验证版本\nkubeadm version\n\n# 查看升级计划\nkubeadm upgrade plan\n\n# 执行升级\nsudo kubeadm upgrade apply v1.y.x\n\n# 其他的master\nsudo kubeadm upgrade node\n\n工作节点\n驱逐节点上pod\n\nkubectl drain &lt;节点> --delete-local-data --force --ignore-daemonsets\n\n\n升级节点\n\nyum update -y kubelet\n\nsystemctl restart kubelet\n\n\n恢复节点\n\nkubectl uncordon &lt;节点>\n\n其他\n查看cni是不是需要根据版本升级\ndashboard等k8s应用升级\n\nkubeadm常用命令# 打印默认的初始化配置\nkubeadm config print init-defaults > kubeadm-config.yaml\n\n# 打印默认的初始化配置,巴罗了kubeetl组件\nkubeadm config print init-defaults --component-configs KubeletConfiguration\n\n# 使用配置文件来初始化集群\nkubeadm init --config kubeadm-config.yaml\n\n# 查看所需要的镜像列表\nkubeadm config images list\n\n# 下载默认配置的镜像\nkubeadm config images pull\n\n# 由于国内无法访问gcr.io，可以指定仓库，这里使用了阿里的镜像\nkubeadm config images pull --image-repository registry.aliyuncs.com/google_containers --kubernetes-version latest\n\n# 获取key\nkubeadm init phase upload-certs --upload-certs\n\n# 获取加入节点的命令\nkubeadm token create --print-join-command --ttl 0\n\n# 将获取的key组合成添加master的命令\nkubeadm join k8s-api:6443 \n--token &lt;token> \\\n--discovery-token-ca-cert-hash &lt;cert>\\\n--control-plane \\\n--certificate-key &lt;key> \\\n--v=6\n\n# 加入master的简便命令\nkubeadm token create --print-join-command --certificate-key $(kubeadm init phase upload-certs --upload-certs|tail -1)\n\n# kubeadm init 和 kubeadm join 如果cpu配置太低可以使用下面的参数忽略\n--ignore-preflight-errors=Mem,NumCPU\n\n# 查看证书时间\nkubeadm certs check-expiration\n\n# 证书续期\nkubeadm certs renew all\n\n参考资料https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/https://kubernetes.io/docs/setup/production-environment/container-runtimes/#docker\n","tags":["k8s","kubeadm"]},{"title":"kubebuilder创建webhook","url":"/2025/05/03/kubebuilder%E5%88%9B%E5%BB%BAwebhook/","content":"kubebuilder可以快速的为k8s创建一个webhook脚手架\n\n\n1.初始化项目\n初始化kubebuilder\n\nkubebuilder init --repo github.com/naturelr/lxcfs-admission-webhook --domain github.com\n\n2.创建webhook\n注意 mutating webhook和validating webhook确定之后是不能修改的\n\n\nmutating webhook\n\nkubebuilder create webhook --group core --version v1 --kind Pod --defaulting \n\n\nvalidating webhook\n\nkubebuilder create webhook --group core --version v1 --kind Pod --programmatic-validation\n\n\nmutating webhook和validating webhook\n\nkubebuilder create webhook --group core --version v1 --kind Pod --programmatic-validation --defaulting\n\n\n在config/default/kustomization.yaml删除下面的注释来支持certmanager\n\nresources:\n# [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER'. 'WEBHOOK' components are required.\n- ../certmanager\n\n\n在config/default/kustomization.yaml删除下面的注释的来支持webhook\n\nreplacements:\n- source: # Uncomment the following block if you have any webhook\n    kind: Service\n    version: v1\n    name: webhook-service\n    fieldPath: .metadata.name # Name of the service\n  targets:\n    - select:\n        kind: Certificate\n        group: cert-manager.io\n        version: v1\n        name: serving-cert\n      fieldPaths:\n        - .spec.dnsNames.0\n        - .spec.dnsNames.1\n      options:\n        delimiter: '.'\n        index: 0\n        create: true\n- source:\n    kind: Service\n    version: v1\n    name: webhook-service\n    fieldPath: .metadata.namespace # Namespace of the service\n  targets:\n    - select:\n        kind: Certificate\n        group: cert-manager.io\n        version: v1\n        name: serving-cert\n      fieldPaths:\n        - .spec.dnsNames.0\n        - .spec.dnsNames.1\n      options:\n        delimiter: '.'\n        index: 1\n        create: true\n\n\n如果是mutating webhook(–defaulting)就删除下面的注释\n\nreplacements:\n- source: # Uncomment the following block if you have a DefaultingWebhook (--defaulting )\n    kind: Certificate\n    group: cert-manager.io\n    version: v1\n    name: serving-cert\n    fieldPath: .metadata.namespace # Namespace of the certificate CR\n  targets:\n    - select:\n        kind: MutatingWebhookConfiguration\n      fieldPaths:\n        - .metadata.annotations.[cert-manager.io/inject-ca-from]\n      options:\n        delimiter: '/'\n        index: 0\n        create: true\n- source:\n    kind: Certificate\n    group: cert-manager.io\n    version: v1\n    name: serving-cert\n    fieldPath: .metadata.name\n  targets:\n    - select:\n        kind: MutatingWebhookConfiguration\n      fieldPaths:\n        - .metadata.annotations.[cert-manager.io/inject-ca-from]\n      options:\n        delimiter: '/'\n        index: 1\n        create: true\n\n\n如果是validating webhook则将下面注释开头的内容注释删除\n\n# - source: # Uncomment the following block if you have a ValidatingWebhook (--programmatic-validation)\n\n为webhook添加ns选择器\n将下面的yaml的补丁放到config/default/webhook_webhook_patch.yaml中\n\napiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: mutating-webhook-configuration\nwebhooks:\n  - name: mpod-v1.kb.io\n    namespaceSelector:\n      matchLabels:\n         lxcfs-injection: enabled\n\n\n同时在config/default/kustomization.yaml中的patches字段添加下面的内容\n\npatches:\n- path: webhook_webhook_patch.yaml\n  target:\n    kind: MutatingWebhookConfiguration\n    name: mutating-webhook-configuration\n\n3.编写webhook代码\n在internal/webhook/v1/pod_webhook.go中写逻辑，其中+kubebuilder开头的注释为代码生成器\n\n修改+kubebuilder需要执行下面的命令来生成一些代码\n\n\nmake manifests\n\n4.编译镜像\n设置镜像仓库地址，在makefile中第一行设置docker镜像地址\n\n# makefile\nIMG ?= controller:latest\n\n\n编译docker镜像\n\nmake docker-build \nmake docker-push\n\n\n多架构镜像,这会直接上传到镜像仓库，在makefiel中的PLATFORMS变量可以调整需要的平台\n\nmake docker-buildx\n\n\n如果没有cr的话没有api目录导致编译报错注释掉dockefile中这句话就可以了\n\n#COPY api/ api/\n\n5.部署测试\n部署到集群中\n\nmake deploy\n\n\n从集群中卸载\n\nmake undeploy\n\n\n生成部署的yaml,如果是webhook需要添加--force\n\nmake build-installer IMG=naturelr/lxcfs-admission-webhook:latest --force\n\n\n生成helm文件,\n\nkubebuilder edit --plugins=helm/v1-alpha\n\n参考资料https://book.kubebuilder.io\n","tags":["k8s","kubebuilder"]},{"title":"kubebuilder扩展k8s","url":"/2021/11/02/kubebuilder%E6%89%A9%E5%B1%95k8s/","content":"kubebuilder是个专门用于开发k8s的框架\n\n\n\nk8s有很多资源如deployment,cronjob等资源，这些资源的行为则由位于controller-manager中的各个资源控制器来实现逻辑,\n\n安装在https://github.com/kubernetes-sigs/kubebuilder/releases下载合适的二进制文件并放入path中\n术语\nGV: Api Group和Version\nAPI Group 是相关API功能的集合，\n每个 Group 拥有一或多个Versions\n\n\nGVK: Group Version Kind\n每个GV都包含很多个api 类型，称之为Kinds,不同Version同一个Kinds可能不同\n\n\nGVR: Group Version Rsource\nResource 是 Kind 的对象标识，一般来Kind和Resource 是1:1 的,但是有时候存在 1:n 的关系，不过对于Operator来说都是 1:1 的关系\n\n\n\napiVersion: apps/v1 # 这个是 GV，G 是 apps，V 是 v1\nkind: Deployment    # 这个就是 Kind\nsepc:               # 加上下放的 spec 就是 Resource了\n\n根据GVK K8s就能找到你到底要创建什么类型的资源，根据你定义的Spec创建好资源之后就成为了Resource，也就是GVR。GVK&#x2F;GVR就是K8s资源的坐标，是我们创建&#x2F;删除&#x2F;修改&#x2F;读取资源的基础\n类似这样的关系&#x2F;group&#x2F;version&#x2F;kind\n示例项目初始化完整代码:https://github.com/NatureLR/code-example/tree/master/operator\n需求背景\n我们在部署服务的时候经常需要同时部署deployment和svc这样很复杂，于是自定义一个资源叫appx，让appx来创建svc和deployment\n\n初始化文件夹在项目文件夹下执行\nkubebuilder init --repo github.com/naturelr/code-example/operator --domain naturelr.cc --skip-go-version-check\n\n这个时候目录下会产生一些文件\n├── Dockerfile # 编译docker镜像\n├── Makefile # 编译部署相关的脚本，常用功能都在里面\n├── PROJECT # 项目说明\n├── config # 这个目录都是一些需要安装到集群的文件\n│   ├── default # 默认配置\n│   ├── manager # crd文件\n│   ├── prometheus # 监控相关的如ServiceMonitor\n│   └── rbac # rbac文件\n├── go.mod\n├── go.sum\n├── hack\n│   └── boilerplate.go.txt\n└── main.go\n\n6 directories, 24 files\n\n创建api模板执行下面的命令，创建api，期间会问你是不是需要创建Resource和Controller，这里我们都选y\nkubebuilder create api --group appx --version v1 --kind Appx\n\n完成之后多了一些目录\n.\n├── Dockerfile\n├── Makefile\n├── PROJECT\n├── api\n│   └── v1 # 我们自定义的api\n├── bin\n│   └── controller-gen # 生成文件的程序\n├── config\n├── controllers\n│   ├── appx_controller.go # 控制逻辑写在这\n│   └── suite_test.go # 测试用例\n├── go.mod\n├── go.sum\n├── hack\n│   └── boilerplate.go.txt\n└── main.go\n\n12 directories, 10 files\n\n实现定义字段在api/v1/application_types.go中的AppxSpec写上需要的字段\ntype AppxSpec struct &#123;\n // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster\n // Important: Run \"make\" to regenerate code after modifying this file\n\n // Foo is an example field of Appx. Edit appx_types.go to remove/update\n Image string `json:\"image,omitempty\"`\n Port  int    `json:\"port,omitempty\"`\n&#125;\n\n然后执行make manifests generate命令生成crd文件\n生成的crd文件在config/crd/bases/中\n实现控制器\n有crd只能在k8s中定义cr但是k8s并不知道如何处理这些cr，所以我们要实现控制器来处理这些逻辑\n\n我们需要实现的控制器逻辑在controllers/application_controller.go中的Reconcile函数中\n逻辑改完之后就需要上测试了，执行make install安装crd到集群，注意他会安装到~/.kube/config这个配置文件中的集群\n然后执行make run运行控制器，他会打印很多日志\n\n获取cd，拿到cr中定义的镜像和端口号\n\nappx := &amp;appxv1.Appx&#123;&#125;\nif err := r.Get(ctx, req.NamespacedName, appx); err != nil &#123;\n return ctrl.Result&#123;&#125;, err\n&#125;\n\n\n拿到信息之后需要创建对应的deployment对象和service对象，需要特别注意的是要管理创建的资源，不然删除的不会删除创建的子资源\n\nsvc := &amp;apiv1.Service&#123;&#125;\nif err := r.Get(ctx, req.NamespacedName, svc); err != nil &#123;\n  if client.IgnoreNotFound(err) != nil &#123; \n    return ctrl.Result&#123;&#125;, err// 如果有错误且不是没找到的话就直接返回错误\n  &#125;\n  // 没找到就创建资源\n  if svc.Name == \"\" &#123;\n    l.Info(\"创建service:\", \"名字\", appx.Name)\n    svc = &amp;apiv1.Service&#123;\n      ObjectMeta: metav1.ObjectMeta&#123;\n        Name:      req.Name,\n        Namespace: req.Namespace,\n      &#125;,\n        Spec: apiv1.ServiceSpec&#123;\n        Selector: map[string]string&#123;\"app\": req.Name&#125;,\n        Ports: []apiv1.ServicePort&#123;&#123;\n          Port:       int32(appx.Spec.Port),\n          TargetPort: intstr.FromInt(appx.Spec.Port),\n        &#125;,\n        &#125;,\n      &#125;,\n    &#125;\n    // 关联 appx和deployment\n    if err := controllerutil.SetOwnerReference(appx, svc, r.Scheme); err != nil &#123;\n       return ctrl.Result&#123;&#125;, err\n    &#125;\n    if err := r.Create(ctx, svc); err != nil &#123;\n       return ctrl.Result&#123;&#125;, err\n    &#125;\n    l.Info(\"创建service成功\")\n  &#125;\n&#125;\n\n\n如果已经有此资源,那么可能就需要更新资源了\n\n// svc\nsvc.Spec.Ports = []apiv1.ServicePort&#123;&#123;Port: int32(appx.Spec.Port)&#125;&#125;\nl.Info(\"更新service\", \"port\", appx.Spec.Image)\nif err := r.Update(ctx, svc); err != nil &#123;\n   return ctrl.Result&#123;&#125;, err\n&#125;\nl.Info(\"service更新完成\")\n\n到此一个简单的crd的控制逻辑就完成了\nstatus\n上面创建的cr当查看的时候并不会显示status\n\n\n在api/v1/appx_types.go中找到AppxStatus,添加上合适的字段\n\n// AppxStatus defines the observed state of Appx\ntype AppxStatus struct &#123;\n  // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster\n  // Important: Run \"make\" to regenerate code after modifying this file\n  // 必须要有json tag\n  Workload int32  `json:\"workload\"`\n  Svc      string `json:\"svc\"`\n&#125;\n\n\n在controllers/application_controller.go中更新status字段\n\nappx.Status.Workload = *deploy.Spec.Replicas\nappx.Status.Svc = fmt.Sprintf(\"%d\", svc.Spec.Ports[0].Port)\nr.Status().Update(ctx, appx)\n\n\n上面自会显示在get xx -o yaml当中,当我们想显示在 get xxx -o wide中时需要在api/v1/appx_types.go中添加注释，具体参考https://book.kubebuilder.io/reference/generating-crd.html\n\n// 注意type要对应上字段！！！\n//+kubebuilder:printcolumn:JSONPath=\".status.workload\",name=Workload,type=integer\n//+kubebuilder:printcolumn:JSONPath=\".status.svc\",name=Svc,type=string\n\n\n同样需要重新生成crd并且要安装\n\nevent事件\nevnet事件，有的时候告诉我们一些重要的信息\n\n\n在controllers/application_controller.go中增加字段\n\n// AppxReconciler reconciles a Appx object\ntype AppxReconciler struct &#123;\n  client.Client\n  Scheme   *runtime.Scheme\n  Recorder record.EventRecorder//增加事件结构体\n&#125;\n\n\n\n调用\n\nr.Recorder.Event(appx, apiv1.EventTypeNormal, \"找到cr\", appx.Name)\n\n\n在main.go中加上Recorder的初始化逻辑\n\nif err = (&amp;controllers.AppxReconciler&#123;\n  Client:   mgr.GetClient(),\n  Scheme:   mgr.GetScheme(),\n  Recorder: mgr.GetEventRecorderFor(\"Appx\"), //+\n&#125;).SetupWithManager(mgr); err != nil &#123;\n  setupLog.Error(err, \"unable to create controller\", \"controller\", \"Appx\")\n  os.Exit(1)\n&#125;\n\n$ kubectl get event\nLAST SEEN   TYPE     REASON   OBJECT     MESSAGE\n2m55s       Normal   找到cr     appx       \n4s          Normal   找到cr     appx/foo   foo  \n\n常用命令# 初始化\nkubebuilder init --repo github.com/naturelr/code-example/operator --domain naturelr.cc --skip-go-version-check\n\n# 创建 api\nkubebuilder create api --group appx --version v1 --kind Appx\n\n# 创建webhook\nkubebuilder create webhook --group nodes --version v1 --kind Appx --defaulting --programmatic-validation\n\n# 生成文件\nmake manifests generate\n\n# 安装crd等文件\nmake install\n\n# 本地调试运行\nmake run\n\n参考资料https://book.kubebuilder.io/introduction.htmlhttps://lailin.xyz/post/operator-03-kubebuilder-tutorial.html\n","tags":["k8s","go"]},{"title":"kubectl多集群管理","url":"/2020/09/14/kubectl%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/","content":"大部分情况我们不只一个k8s集群，这个时候我们需要快速的在各个集群之间进行切换。且省去每次都要申明namespace\n\n使用kubectxkubectx是个可以快速的切换集群且能设置namespace的官方地址\n安装brew install kubectx\n\n为了能够使用模糊推查找荐安装fzf\n使用\nkubectx 可以看到所有环境，通过模糊查找可快速选择集群\nkubens 可以看到当前环境所有的namespace，可以快速选择NS，选择NS之后执行的命令就是在当前NS中执行了，比如执行kubectl get pods 显示的就是当前NS所有的pod，不需要加上-n xxxx\n\n多集群的管理kubectx 所有解决了多个环境和命名空间的问题，但是没能解决快速添加集群利用kubectl的环境变量拿到所有的环境然后通过kubectl config view --raw合并成为一个config文件，脚本如下：\n#! /bin/bash\n# 合并$HOME/.kube/configs目录下的文件到$HOME/.kube/config\n# 配合kubectx工具进行环境切换\n\nCONFIGPATH=$HOME/.kube/configs\n\nFILEPATH=\n\nfor C in `ls $CONFIGPATH/*yaml`;do\n    echo \"找到配置文件:\"$C\n    FILEPATH=$FILEPATH$C:\ndone\n\nexport KUBECONFIG=$FILEPATH\n\nkubectl config view --raw > $HOME/.kube/config\n\nunset KUBECONFIG\n\n\n添加集群就只需要把集群的config文件保存到$HOME/.kube/configs下，名字为xxx.yaml，然后执行脚本，删除同理只需要将该集群的yaml文件从$HOME/.kube/configs中移除在执行脚本\n\n该方案有个问题就是如果集群中所有的名字一样就无法区分,随意随后开发了一个脚本使用链接kubeconfig到~&#x2F;config来切换，参考https://github.com/NatureLR/kubectl-cc\n\n\n","tags":["k8s"]},{"title":"kubectl插件管理工具krew","url":"/2021/04/18/kubectl%E6%8F%92%E4%BB%B6%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7krew/","content":"krew是一个kubectl的插件管理系统\n\n\n安装(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\" &amp;&amp;\n  tar zxvf krew.tar.gz &amp;&amp;\n  KREW=./krew-\"$&#123;OS&#125;_$&#123;ARCH&#125;\" &amp;&amp;\n  \"$KREW\" install krew\n)\n\n\n添加环境变量\n\nexport PATH=\"$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$PATH\"\n\n常用命令\nkubectl krew install xxx 安装插件\n\nkubectl krew uninstall xxx 卸载插件\n\nkubectl krew list xxx 查看插件\n\nkubectl krew update xxx 升级插件\n\n\n参考资料https://krew.sigs.k8s.io\n","tags":["k8s"]},{"title":"kubectl格式化输出","url":"/2020/07/21/kubectl%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA/","content":"有时候需要输出一些k8s的资源信息为一个表格比如统计资源你的数量\n\n将下面你的模板保存为template.txt\ncat &lt;&lt; EOF >> template.txt\n命名空间              名字           保留内存              保留cpu        最大内存  最大cpu\nmetadata.namespace   metadata.name spec.containers[*].resources.requests.memory  spec.containers[*].resources.requests.cpu   spec.containers[*].resources.limits.memory  spec.containers[*].resources.limits.cpu\nEOF\n\n然后执行\nkubectl get deployment  -o custom-columns-file=template.txt\n\n除了用模板文件还可以用\nkubectl get deployment  -o custom-columns=名字:.metadata.name,数量:.spec.replicas\n","tags":["k8s"]},{"title":"kubectl自带的debug命令","url":"/2022/09/16/kubectl%E8%87%AA%E5%B8%A6%E7%9A%84debug%E5%91%BD%E4%BB%A4/","content":"在使用k8s的时候需要调试的时候我们一般都是exec -it 命令登录上去执行一些调试命令，但是很多镜像为了体积和安全都不内置这些命令，导致我们需要手动安装调试麻烦\n\n\nkubectl在1.18之后新加了一个debug子命令将我们的调试容器放到需要调试的pod中方便调试\n支持情况k8s 1.18以后,需要开启特性\n使用调试pod\n将centos添加到pod进行调试\n\nkubectl debug cdebug-64cd86798b-sjxrl -it --image=centos -- sh\n\n\n将centos添加到pod进行调试的同时复制一个pod叫cdebug-debug且共享进程，–share-processes&#x3D;true只有在copy是才生效\n\nkubectl debug cdebug-64cd86798b-sjxrl -it --image=centos --share-processes --copy-to=cdebug-debug -- sh\n\n调试node\n需要注意的node会挂载在&#x2F;host下\n\nkubectl debug node/10.69.202.146 -it --image=centos -- sh\nchroot /host\n\n此功能也可以通过node_shell这个kubect插件来实现\n参考资料https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod\n","tags":["k8s"]},{"title":"kustomize","url":"/2021/07/21/kustomize/","content":"kustomize是k8s-sig开发的一个用来渲染一些k8s资源文件的工具\n\n\n\n主要场景就是多集群环境，一个服务在每个集群的配置不一样很容易造成混乱\n\n安装macos\nbrew install kustomize\n\n二进制手动安装\ncurl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"  | bash\n\nkubectl\n\nkubectl 已经集成了kustomize\n\nkubectl kustomize\nkubectl apply -k\n\n动手尝试1.创建kustomize描述文件创建一个名为base的文件夹并在文件夹里一个文件名为kustomization.yaml且写入如下内容\n# 设置ns\nnamespace: test\nresources:\n  - deployment.yaml\n# 生成config\nconfigMapGenerator:\n- name: example-configmap-1\n  envs:\n  - env.conf\n  literals:\n  - FOO=Bar\n# 生成secrets\nsecretGenerator:\n- name: example-secret-2\n  literals:\n  - username=admin\n  # 通过文件生成secret\n  files: \n  - passwd.conf\ngeneratorOptions: # 只对生成的资源有效\n  disableNameSuffixHash: true # 关闭生成的资源文件的hash值\n  labels: # 所有生产的资源都会有下面的标签\n    type: generated\n  annotations: # 所有生产的资源都会有下面的注解\n    note: generated\n# 镜像替换\nimages:\n  - name: nginx\n    newName: nginx\n    newTag: alpine\n\n2.创建依引用资源文件创建kustomization.yaml中引用的文件\n# 创建deployment.yaml 此文件为为模板文件\ncat &lt;&lt;EOF >base/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test\nspec:\n  selector:\n    matchLabels:\n      app: test\n  template:\n    metadata:\n      labels:\n        app: test\n    spec:\n      containers:\n      - name: test\n        image: nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n        ports:\n        - containerPort: 80\n        volumeMount:\n        - name: config\n          mountPath: /config\n      volumes:\n      - name: config\n        configMap:\n          name: example-configmap-1\nEOF\n\n# 创建配置文件\ncat &lt;&lt;EOF >base/env.conf\nkey=abcdef\ndebug=true\nEOF\n\n# 创建secrets文件\ncat &lt;&lt;EOF >base/passwd.conf\nuser=root\npasswd=123456\nEOF\n\n\n最终文件结构如下：base├── deployment.yaml├── env.conf├── kustomization.yaml└── passwd.conf\n3.编译执行kubectl kustomize base或者kustomize build base\n这时候我们发现生成的内容当中\b自动从.conf文件自动转换为k8s资源文件，且镜像被替换了生成的文件都有我们指定的标签\n基准覆盖上面我们创建的只是一个基本k8s资源文件，在实际中一个服务在各个环境会有细微的区别那么我们可以通过kustomize在基本上进行一些修改\n假如上面的服务我们要部署到测试环境中，在测试环境中ns需要加上一些dev等字段，且还有一些节点亲和等操作\n1.创建测试环境的kustomize文件\n在base同级目录中创建一个叫overlays的目录,且在里面在创建个目录叫dev\n\nmkdir -p overlays/dev\n\n\n写入kustomization.yaml\n\n# 引用基准资源\nresources:\n  - ../../base\n# 设置ns名字前缀\nnamePrefix: dev-\n# 设置ns名字后缀\nnameSuffix: \"-a\"\n# 设置公共标签\ncommonLabels:\n  env: dev\n# 设置公共注解\ncommonAnnotations:\n  owner: foo\nimages:\n  - name: nginx\n    newName: nginx\n    newTag: alpine\n# 合并补丁\npatchesStrategicMerge:\n  - nodeAffinity.yaml\n\n2.创建测试环境的引用文件cat &lt;&lt;EOF >overlays/dev/nodeAffinity.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test\n  spec:\n    template:\n      spec:\n        affinity:\n          nodeAffinity:\n            requiredDuringSchedulingIgnoredDuringExecution:\n              nodeSelectorTerms:\n              - matchExpressions:\n                - key: kubernetes.io/os\n                  operator: Exists\nEOF\n\n此时文件结构如下.├── base│   ├── deployment.yaml│   ├── env.conf│   ├── kustomization.yaml│   └── passwd.conf└── overlays    └── dev        ├── kustomization.yaml        ├── nodeAffinity.yaml\n3.编译测试环境kustomize build overlays/dev\n\n其他环境如法炮制，这样就可以优雅的管理服务在各个资源的描述，在结合argcd的情况下会更加的舒服！\nkubectl使用\nkubectl中只需要在后面加上-k即可对应命令如下\n\nkubectl apply -k\nkubectl apply --kustomize\n\nkubectl get -k\nkubectl diff -k\nkubectl describe -k\n\n参考资料\nhttps://kubernetes.io/zh/docs/tasks/manage-kubernetes-objects/kustomization\n\nhttps://kustomize.io\n\n\n","tags":["k8s"]},{"title":"kvm","url":"/2021/05/22/kvm%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"kvm基于linux内核的虚拟化\n\n\n\nkvm是基于硬件的完全虚拟化，集成在内核中，qemu主要外部设备的虚拟化两者各发挥所长\n\n检查硬件是否支持apt install cpu-checker\n\nkvm-ok\nINFO: /dev/kvm exists\nKVM acceleration can be used\n\n\n虚拟机中需要打开虚拟化嵌套支持\n\n\n安装sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager\n\n\nqemu-kvm -为KVM管理程序提供硬件仿真的软件。\nlibvirt-daemon-system -用于将libvirt守护程序作为系统服务运行的配置文件。\nlibvirt-clients -用于管理虚拟化平台的软件。\nbridge-utils -一组用于配置以太网桥的命令行工具。\nvirtinst -一组用于创建虚拟机的命令行工具。\nvirt-manager -易于使用的GUI界面和支持命令行工具，用于通过libvirt管理虚拟机。\n\n查看运行状态\nsudo systemctl status libvirtd\n\n图形化安装操作系统\n找到虚拟系统管理器\n\n\n\n点击新建，剩下的和vmware很像\n\n\n\n选择安装类型\n\n\n\n指定安装的iso镜像文件\n\n\n\n自动选择系统类型有问题，无法识别，这里改为通用默认类型\n\n\n\n启动成功进入了安装界面剩下的不做过多介绍\n\n\n其他界面说明\n系统启动引导界面\n\n\n\n这里可以看到磁盘的信息\n\n\n\n这里选择可图形服务器\n\n\n\n网卡配置界面\n\n\n命令行创建\n创建一个磁盘容量为15g，类型为qcow2，名字叫centos7.qcow2的虚拟磁盘\n\nqemu-img create -f qcow2 centos7.qcow2 15G\n\n\n创建虚拟机\n\nsudo virt-install \\\n --name=centos7 \\\n --disk path=/home/centos7.qcow2 \\\n --vcpus=1 --ram=1024 \\\n --cdrom=/home/CentOS-7-x86_64-Minimal-2003.iso \\\n --network bridge=virbr0 \\\n --graphics vnc,listen=0.0.0.0 \\\n --os-type=linux \\\n --os-variant=\"centos7.0\"\n\n\n此时可以通过vnc连接开始安装系统，也可以像图形化界面一样用虚拟机管理器安装\n\n\n选项说明\nvirt-install 中–os-variant可选值\n\n# ubuntu\nsudo apt -y install libosinfo-bin\n\n# cengtos\nyum -y install libosinfo\n\nvirsh常用命令\n\n\n命令\n说明\n\n\n\nvirsh start xxx\n启动名字为x的非活动虚拟机\n\n\nvirsh list  –all\n列出虚拟机\n\n\nvirsh create xxx.xml\n创建虚拟机，没有持久化\n\n\nvirsh suspend xxx\n暂停虚拟机\n\n\nvirsh resume xxxx\n启动暂停的虚拟机\n\n\nvirsh shutdown xxxx\n正常关闭虚拟机\n\n\nvirsh destroy xxxx\n强制关闭虚拟机\n\n\nvirsh undefine xxx\n删除虚拟机，只是从列表中删除，且不会删除活动的虚拟机\n\n\nvirsh dominfo xxx\n显示虚拟机的基本信息\n\n\nvirsh domname 2\n显示id号为2的虚拟机名\n\n\nvirsh domid xxx\n显示虚拟机id号\n\n\nvirsh domuuid xxx\n显示虚拟机的uuid\n\n\nvirsh domstate xxx\n显示虚拟机的当前状态\n\n\nvirsh dumpxml xxx\n显示虚拟机的当前配置文件（可能和定义虚拟机时的配置不同，因为当虚拟机启动时，需要给虚拟机 分配id号、uuid、vnc端口号等等）\n\n\nvirsh setmem x 512000\n给不活动虚拟机设置内存大小\n\n\nvirsh edit xxx\n编辑配置文件（一般是在刚定义完虚拟机之后）\n\n\n参考资料https://www.iplayio.cn/post/92661051https://www.cnblogs.com/saryli/p/11827903.html\n","tags":["kvm","虚拟化"]},{"title":"linux删除文件之后磁盘没释放","url":"/2020/08/04/linux-%E5%88%A0%E9%99%A4%E6%96%87%E4%BB%B6%E4%B9%8B%E5%90%8E%E6%B2%A1%E9%87%8A%E6%94%BE/","content":"记一下Linux删除文件之后磁盘没释放\n\n\nlinux 删除文件之后磁盘没释放\n原因则执行删除的时候是解除链接，如果文件是被打开的，进程会继续读取那个文件\n\n正确是置空文件，命令如下\ncat /dev/null>xxx.log\n\n可以用下面的命令查找一下类似的文件然后重启对应的程序即可\nlsof | grep deleted\n","tags":["linux"]},{"title":"linux中swap管理","url":"/2024/03/15/linux%E4%B8%ADswap%E7%AE%A1%E7%90%86/","content":"swap交换空叫或者叫虚拟内存，是linux中的一直机制，它允许使用磁盘来作为内存使用，用于内存不是很高的机器中\n\n\n由于是用磁盘来当做内存使用会导致磁盘的读写变多\n查看swap# swap中不为0则表示开启了\nfree -h\n\nswapon -s\n\n添加swap\n创建swap文件，例子是1G大小\n\nfallocate -l 1G /swapfile\n\n或\ndd if=/dev/zero of=/swapfile bs=1024 count=2097152\n\n\n设置权限\n\nchmod 600 /swapfile\n\n\n格式化文件\n\nmkswap &#x2F;swapfile\n\n添加\n\nswapon /swapfile\n\n\n持久化\n\necho \"/swapfile swap swap defaults 0 0\" >> /etc/fstab\n\n\n验证\n\nfree -h\n#                total        used        free      shared  buff/cache   available\n# Mem:           3.8Gi       3.4Gi       180Mi        95Mi       628Mi       476Mi\n# Swap:          1.0Gi       604Mi       419Mi\n\n关闭swap\n停止swap\n\nswapoff -v /swapfile\n\n\n删除或注释 /etc/fstab中类似/swapfile swap swap defaults 0 0\n\n删除文件\n\n\nrm /swapfile\n\n\n或者直接关闭所有\n\nswapoff -a\n\n调整交换频率Swappiness内核中有个参数Swappiness可以调整内存到虚拟内存的频率\n\n临时修改\n\nsudo sysctl -w vm.swappiness=10\n\n\n持久化\n\necho \"vm.swappiness=10\" >> /etc/sysctl.conf\n\n\n生效\n\nsysctl -p\n\n&#96;\n参考资料https://u.sb/debian-swap/\n","tags":["swap","memory"]},{"title":"linux内核升级","url":"/2022/05/02/linux%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/","content":"centos内核升级\n\n\n\n升级centos内核\n\n包管理安装添加epel仓库rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm\n\nyum --enablerepo=\"elrepo-kernel\" list --showduplicates | sort -r | grep kernel-ml.x86_64\n\n替换清华源# 备份\nsudo cp /etc/yum.repos.d/elrepo.repo /etc/yum.repos.d/elrepo.repo.bak\n\n# 然后编辑 /etc/yum.repos.d/elrepo.repo 文件，在 mirrorlist= 开头的行前面加 # 注释掉；\nsed  -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/elrepo.repo\n\n# 并将 elrepo.org/linux 替换为 mirrors.tuna.tsinghua.edu.cn/elrepo\nsed -i 's/elrepo.org\\/linux/mirrors.tuna.tsinghua.edu.cn\\/elrepo/g' /etc/yum.repos.d/elrepo.repo\n\n# 注释掉其他仓库\nsed  -i '/http:\\/\\/mirrors.coreix/d' /etc/yum.repos.d/elrepo.repo\nsed  -i '/http:\\/\\/mirror.rackspace.com/d' /etc/yum.repos.d/elrepo.repo\nsed  -i '/http:\\/\\/repos.lax-noc.com/d' /etc/yum.repos.d/elrepo.repo\n\n# 更新软件包缓存\nsudo yum makecache\n\n安装内核# 稳定版本\nyum --enablerepo=elrepo-kernel install  kernel-ml-devel kernel-ml -y\n\n# 安装长期支持版本\nyum --enablerepo=elrepo-kernel install kernel-lt-devel kernel-lt -y\n\n设置启动# 查看安装的内核\nawk -F\\' '$1==\"menuentry \" &#123;print $2&#125;' /etc/grub2.cfg\n\n# 设置启动顺序\ngrub2-set-default 0\n\n# 重启生效\nreboot\n\n\n源码安装下载源码mainline 最新稳定版stable 稳定版本longterm 长时间支持版本\n\n\n官方国内清华镜像源\nwget https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-5.17.5.tar.xz\n\ntar xvf linux-5.17.5.tar.xz\n\n安装编译工具centos7默认4.8.5目前最新的5.17内核需要5.1.0以上\n\n\n\n# 编译工具\nyum install -y ncurses-devel make gcc bc bison flex elfutils-libelf-devel openssl-devel\n\n# 升级gcc版本\nyum install  -y centos-release-scl\nyum install  -y devtoolset-7-gcc*\nscl enable devtoolset-7 bash\ngcc --version\n\n配置内核参数参数有两种配置方式：手动配置或者复制当前内核配置，最终在源码目录生成.config文件\n\n\n直接复制当前内核的参数cp -v &#x2F;boot&#x2F;config-$(uname -r) .config\n\n手动配置make menuconfig\n\n新的配置界面\nmake nconfig\n\n编译安装内核编译源码-j 参数根据cpu数量来设置以加快编译速度，通常是cpu数量的2倍\n\n\nmake -j 8\n\n安装make modules_install install\n\n设置开机启动# 查看启动顺序\nawk -F\\' '$1==\"menuentry \" &#123;print $2&#125;' /etc/grub2.cfg\n\n# 设置启动顺序(编号是上面命令看的的顺序)\ngrub2-set-default 0\n\n# 重启生效\nreboot\n\n编译rpm包\n# 安装rpm构建工具\nyum install -y rpm-build rpmlint yum-utils rpmdevtools\n\n# 构建rpm包\nmake rpm-pkg\n\n# 安装\nyum install -y xx.rpm\n\n# 重新生成grub.cfg\ngrub2-mkconfig -o /boot/grub2/grub.cfg\n\n# 设置启动顺序(编号是上面命令看的的顺序)\ngrub2-set-default 0\n\n# 重启生效\nreboot\n\n参考资料https://ahelpme.com/linux/centos7/how-to-install-new-gcc-and-development-tools-under-centos-7/https://nestealin.com/8bab8c2c/https://github.com/torvalds/linuxhttps://www.kernel.org/doc/html/latest\n","tags":["内核"]},{"title":"linux登录欢迎页","url":"/2020/08/03/linux%E7%99%BB%E5%BD%95%E6%AC%A2%E8%BF%8E%E9%A1%B5/","content":"ssh每次登录的时候显示一些信息\n\n编辑/etc/motd中的内容，即可在登录的时候打印出来\n\n例如将下面的复制进去\n......................阿弥陀佛......................\n                      _oo0oo_\n                     o8888888o\n                     88&quot; . &quot;88\n                     (| -_- |)\n                     0\\  &#x3D;  &#x2F;0\n                   ___&#x2F;‘---’\\___\n                  .&#39; \\|       |&#x2F; &#39;.\n                 &#x2F; \\\\|||  :  |||&#x2F;&#x2F; \\\n                &#x2F; _||||| -卍-|||||_ \\\n               |   | \\\\\\  -  &#x2F;&#x2F;&#x2F; |   |\n               | \\_|  &#39;&#39;\\---&#x2F;&#39;&#39;  |_&#x2F; |\n               \\  .-\\__  &#39;-&#39;  ___&#x2F;-. &#x2F;\n             ___&#39;. .&#39;  &#x2F;--.--\\  &#39;. .&#39;___\n         .&quot;&quot; ‘&lt;  ‘.___\\_&lt;|&gt;_&#x2F;___.’&gt;’ &quot;&quot;.\n       | | :  ‘- \\‘.;‘\\ _ &#x2F;’;.’&#x2F; - ’ : | |\n         \\  \\ ‘_.   \\_ __\\ &#x2F;__ _&#x2F;   .-’ &#x2F;  &#x2F;\n    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;‘-.____‘.___ \\_____&#x2F;___.-’___.-’&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n                      ‘&#x3D;---&#x3D;’\n\n....................佛祖保佑 ,永无BUG...................\n","tags":["linux"]},{"title":"linux魔法键","url":"/2020/09/29/linux%E9%AD%94%E6%B3%95%E9%94%AE/","content":"利用/proc/sys/kernel/sysrq处理Linux系统不能正常响应用户请求，比如不能重启这时可以使用强制重启echo b &gt;/proc/sys/kernel/sysrq\n\nSysRq也称为魔法键，可以使用键盘快捷键的，但还是使用命令明确一些\n检查当前状态  cat &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;sysrq\n各个数字对应的含义\n\n0 完全关闭\n1 开启sysrq所有功能\n&gt;1 允许的sysrq函数的位掩码 具体请看官方文档\n\n更改SysRq可以使用下面你的命令设置\necho &lt;number> >/proc/sys/kernel/sysrq\n\n或者使用sysctl\nsysctl -w kernel.sysrq=&lt;number>\n```  \n\n###### 使用SysRq\n\n```shell\necho &lt;command> > /proc/sysrq-trigger\n\n常用的command如下\n\nb 立即重启,但是不同步磁盘\ns 尝试同步磁盘\n其他的可以参考文档\n\n","tags":["linux"]},{"title":"logrotate","url":"/2020/12/14/logrotate/","content":"Linux系统随着时间日志越来越大，我们需要日志转储和处理以免导致磁盘爆满\n\n\n\nlogrotate是一个日志转储工具，centos默认安装并且根据策略每天执行一次\n\n安装\n一般cnetos都是默认安装如果没有安装执行：\n\nyum install logrotate\n\n常用参数\n-d 调试 logrotate -d &#x2F;etc&#x2F;logrotate.conf\n-f 强制运行 logrotate -f &#x2F;etc&#x2F;logrotate.conf\n-s 启动备用备用状态文件默认文件在/var/lib/logrotate/logrotate.status\n-v 详细模式\n指定logrotate的状态文件 logrotate -vf –s &#x2F;var&#x2F;log&#x2F;logrotate-status &#x2F;etc&#x2F;logrotate.conf 日志文件\n\n默认运行机制\ncrontab执行/etc/cron.daily下的logrotate脚本,由脚本调用logrotate执行配置目录和配置文件下的任务\n\n配置解释\n配置文件在/etc/logrotate.conf配置目录在/etc/logrotate.d/logrotate.d目录防止其他程序的配置文件比如syslog\n\n配置文件# see &quot;man logrotate&quot; for details                                      # 详细情况执行&#96;man logrotate\n# rotate log files weekly                                              # 日志文件每周转储一次（全局配置）\nweekly\n\n# keep 4 weeks worth of backlogs                                       # 保存4个转储周期\nrotate 4\n\n# create new (empty) log files after rotating old ones                 # 转储模式为create\ncreate\n\n# use date as a suffix of the rotated file                             # 转储的文件以日期最为后缀\ndateext\n\n# uncomment this if you want your log files compressed                 # 是否压缩\ncompress\n\n# RPM packages drop log rotation information into this directory       # 导入配置目录\ninclude &#x2F;etc&#x2F;logrotate.d\n\n# system-specific logs may be also be configured here.\n\n\n配置目录\n配置目录syslog为例：\n\n&#x2F;var&#x2F;log&#x2F;cron\n&#x2F;var&#x2F;log&#x2F;maillog\n&#x2F;var&#x2F;log&#x2F;messages\n&#x2F;var&#x2F;log&#x2F;secure\n&#x2F;var&#x2F;log&#x2F;spooler\n&#x2F;var&#x2F;log&#x2F;kern.log   # 目标日志文件\n&#123;\n    daily           # 执行周期还可以填写weekly,monthly，yearly\n    missingok       # 转储时忽略日志错误\n    sharedscripts   # 运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本\n    postrotate      # 脚本开始\n        &#x2F;usr&#x2F;bin&#x2F;systemctl kill -s HUP rsyslog.service &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 || true\n    endscript       # 脚本结束\n\n其他重要参数说明\ncompress                   通过gzip压缩日志\nnocompress                 不做gzip压缩处理\ncopytruncate               用于还在打开中的日志文件，把当前日志备份并截断；是先拷贝再清空的方式，拷贝和清空之间有一个时间差，可能会丢失部分日志数据。\nnocopytruncate             备份日志文件不过不截断\ncreate mode owner group    轮转时指定创建新文件的属性，如create 0777 nobody nobody\nnocreate                   不建立新的日志文件\ndelaycompress              和compress 一起使用时，转储的日志文件到下一次转储时才压缩\nnodelaycompress            覆盖 delaycompress 选项，转储同时压缩。\nmissingok                  如果日志丢失，不报错继续滚动下一个日志\nerrors address             专储时的错误信息发送到指定的Email 地址\nifempty                    即使日志文件为空文件也做轮转，这个是logrotate的缺省选项。\nnotifempty                 当日志文件为空时，不进行轮转\nmail address               把转储的日志文件发送到指定的E-mail 地址\nnomail                     转储时不发送日志文件\nolddir directory           转储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统\nnoolddir                   转储后的日志文件和当前日志文件放在同一个目录下\nsharedscripts              运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本\nprerotate                  在logrotate转储之前需要执行的指令，例如修改文件的属性等动作；必须独立成行\npostrotate                 在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行\ndaily                      指定转储周期为每天\nweekly                     指定转储周期为每周\nmonthly                    指定转储周期为每月\nrotate count               指定日志文件删除之前转储的次数，0 指没有备份，5 指保留5 个备份\ndateext                    使用当期日期作为命名格式\ndateformat .%s             配合dateext使用，紧跟在下一行出现，定义文件转储后的文件名，配合dateext使用，只支持 %Y %m %d %s 这四个参数\nsize(或minsize) log-size   日志文件超过多少之后就转储，可以是 100 100K  100M 100G这都是有效的\n\n参考资料https://wsgzao.github.io/post/logrotatehttps://www.cnblogs.com/kevingrace/p/6307298.htmlman logrotate\n","tags":["log"]},{"title":"longhorn部署安装","url":"/2024/06/18/longhorn%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85/","content":"Longhorn是Kubernetes的一个轻量级、可靠且易于使用的分布式块存储系统\n\n\n架构\n安装前置依赖\n部署iscsi\n\n# 使用k8s安装iscsi\nkubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.2/deploy/prerequisite/longhorn-iscsi-installation.yaml\n\n# 手动安装\nyum --setopt=tsflags=noscripts install iscsi-initiator-utils\necho \"InitiatorName=$(/sbin/iscsi-iname)\" > /etc/iscsi/initiatorname.iscsi\nsystemctl enable iscsid\nsystemctl start iscsid\nmodprobe iscsi_tcp\n\n\n节点需要nfs服务支持没有则安装\n\n# 使用k8s安装nfs\nkubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.2/deploy/prerequisite/longhorn-nfs-installation.yaml\n\n# 手动安装\ncat /boot/config-`uname -r`| grep CONFIG_NFS_V4_1\ncat /boot/config-`uname -r`| grep CONFIG_NFS_V4_2\nyum install nfs-utils\n\n部署longhornkubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.2/deploy/longhorn.yaml\n\n\nhelm安装Longhorn\n\nhelm repo add longhorn https://charts.longhorn.io\nhelm repo update\nhelm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace\n\n\n部署测试\n\nkubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/v1.6.2/examples/pod_with_pvc.yaml\n\n访问longhorn\n转发dashboard,访问localhost:8080\n\nkubectl -n longhorn-system port-forward svc/longhorn-frontend 8080:80\n\n\n参考资料https://longhorn.io/docs\n","tags":["存储"]},{"title":"lvm使用","url":"/2024/07/01/lvm%E4%BD%BF%E7%94%A8/","content":"lvm(Logical Volume Manager)是linux的逻辑卷，他在物理卷基础上抽象成一个逻辑的卷，这样就可以使用单个或者多个磁盘合并成一个逻辑磁盘\n\n\n术语\nPV：物理卷,可以任务就是普通的磁盘\nVG：卷组,蒋垛个PV合并成一个组\nLV：逻辑卷，从VG中分割出的一块空间创建磁盘，可以伸缩容\nPE：物理区域,PV中划分的一些块\n\n\n创建一个lvm\n磁盘分区，需要注意最后w保存到磁盘\n\nfdisk /dev/vdb\n# 欢迎使用 fdisk (util-linux 2.23.2)。\n# \n# 更改将停留在内存中，直到您决定将更改写入磁盘。\n# 使用写入命令前请三思。\n# \n# Device does not contain a recognized partition table\n# 使用磁盘标识符 0x4ffd3010 创建新的 DOS 磁盘标签。\n# \n# 命令(输入 m 获取帮助)：n\n# Partition type:\n#    p   primary (0 primary, 0 extended, 4 free)\n#    e   extended\n# Select (default p): p\n# 分区号 (1-4，默认 1)：\n# 起始 扇区 (2048-41943039，默认为 2048)：\n# 将使用默认值 2048\n# Last 扇区, +扇区 or +size&#123;K,M,G&#125; (2048-41943039，默认为 41943039)：+1G\n# 分区 1 已设置为 Linux 类型，大小设为 1 GiB\n# \n# 命令(输入 m 获取帮助)：t\n# 已选择分区 1\n# Hex 代码(输入 L 列出所有代码)：8e\n# 已将分区“Linux”的类型更改为“Linux LVM”\n# \n\n\n查看分区的磁盘\n\nfdisk -l\n\n# 磁盘 /dev/vda：21.5 GB, 21474836480 字节，41943040 个扇区\n# Units = 扇区 of 1 * 512 = 512 bytes\n# 扇区大小(逻辑/物理)：512 字节 / 512 字节\n# I/O 大小(最小/最佳)：512 字节 / 512 字节\n# 磁盘标签类型：dos\n# 磁盘标识符：0x000be0d0\n# \n#    设备 Boot      Start         End      Blocks   Id  System\n# /dev/vda1   *        2048    41943039    20970496   83  Linux\n# \n# 磁盘 /dev/vdb：21.5 GB, 21474836480 字节，41943040 个扇区\n# Units = 扇区 of 1 * 512 = 512 bytes\n# 扇区大小(逻辑/物理)：512 字节 / 512 字节\n# I/O 大小(最小/最佳)：512 字节 / 512 字节\n# 磁盘标签类型：dos\n# 磁盘标识符：0x4ffd3010\n# \n#    设备 Boot      Start         End      Blocks   Id  System\n# /dev/vdb1            2048     2099199     1048576   8e  Linux LVM\n# /dev/vdb2         2099200     4196351     1048576   8e  Linux LVM\n# /dev/vdb3         4196352     6293503     1048576   8e  Linux LVM\n\n\n创建物理卷\n\npvcreate /dev/vdb1 /dev/vdb2 /dev/vdb3\n#  Physical volume \"/dev/vdb1\" successfully created.\n#  Physical volume \"/dev/vdb2\" successfully created.\n#  Physical volume \"/dev/vdb3\" successfully created.\n\n\n查看物理卷\n\n pvdisplay\n#  \"/dev/vdb3\" is a new physical volume of \"1.00 GiB\"\n#  --- NEW Physical volume ---\n#  PV Name               /dev/vdb3\n#  VG Name\n#  PV Size               1.00 GiB\n#  Allocatable           NO\n#  PE Size               0\n#  Total PE              0\n#  Free PE               0\n#  Allocated PE          0\n#  PV UUID               9fri0N-wmH0-I2nr-Vd2y-vr7b-RWxO-3zg7Zt\n#\n#  \"/dev/vdb1\" is a new physical volume of \"1.00 GiB\"\n#  --- NEW Physical volume ---\n#  PV Name               /dev/vdb1\n#  VG Name\n#  PV Size               1.00 GiB\n#  Allocatable           NO\n#  PE Size               0\n#  Total PE              0\n#  Free PE               0\n#  Allocated PE          0\n#  PV UUID               3pKwuU-7U1d-Rhwz-7wMS-83nV-f2lq-FABsic\n#\n#  \"/dev/vdb2\" is a new physical volume of \"1.00 GiB\"\n#  --- NEW Physical volume ---\n#  PV Name               /dev/vdb2\n#  VG Name\n#  PV Size               1.00 GiB\n#  Allocatable           NO\n#  PE Size               0\n#  Total PE              0\n#  Free PE               0\n#  Allocated PE          0\n#  PV UUID               GkAeEq-wWcW-B1q8-ujUW-qsTm-PQAJ-SZXH1B\n\n\n创建逻辑卷组\n\nvgcreate volume-test /dev/vdb1 /dev/vdb2 /dev/vdb3\n#  Volume group \"volume-test\" successfully created\n\n\n查看创建的逻辑卷组\n\nvgdisplay\n#  --- Volume group ---\n#  VG Name               volume-test\n#  System ID\n#  Format                lvm2\n#  Metadata Areas        3\n#  Metadata Sequence No  1\n#  VG Access             read/write\n#  VG Status             resizable\n#  MAX LV                0\n#  Cur LV                0\n#  Open LV               0\n#  Max PV                0\n#  Cur PV                3\n#  Act PV                3\n#  VG Size               &lt;2.99 GiB\n#  PE Size               4.00 MiB\n#  Total PE              765\n#  Alloc PE / Size       0 / 0\n#  Free  PE / Size       765 / &lt;2.99 GiB\n#  VG UUID               N0Edem-cvO5-tn1e-qjTD-9Ivk-mLSw-yBEVHp\n\n\n创建逻辑卷\n\nlvcreate -L 200M -n lv1 volume-test\n#  Logical volume \"lv1\" created.\n\n\n查看逻辑卷\n\nlvdisplay\n#  --- Logical volume ---\n#  LV Path                /dev/volume-test/lv1\n#  LV Name                lv1\n#  VG Name                volume-test\n#  LV UUID                w7DEiD-0Q2r-4zDz-0art-fb6S-P2X4-wLyKOk\n#  LV Write Access        read/write\n#  LV Creation host, time 10-60-116-80, 2024-07-02 14:56:09 +0800\n#  LV Status              available\n#  # open                 0\n#  LV Size                200.00 MiB\n#  Current LE             50\n#  Segments               1\n#  Allocation             inherit\n#  Read ahead sectors     auto\n#  - currently set to     8192\n#  Block device           252:0\n\n\n格式化\n\nmkfs.ext4 /dev/volume-test/lv1\n# mke2fs 1.42.9 (28-Dec-2013)\n# 文件系统标签=\n# OS type: Linux\n# 块大小=1024 (log=0)\n# 分块大小=1024 (log=0)\n# Stride=0 blocks, Stripe width=0 blocks\n# 51200 inodes, 204800 blocks\n# 10240 blocks (5.00%) reserved for the super user\n# 第一个数据块=1\n# Maximum filesystem blocks=33816576\n# 25 block groups\n# 8192 blocks per group, 8192 fragments per group\n# 2048 inodes per group\n# Superblock backups stored on blocks:\n#         8193, 24577, 40961, 57345, 73729\n# \n# Allocating group tables: 完成\n# 正在写入inode表: 完成\n# Creating journal (4096 blocks): 完成\n# Writing superblocks and filesystem accounting information: 完成\n\n\n挂载到文件系统\n\nmount /dev/volume-test/lv1 /mnt/\n\n\n查看挂载效果\n\ndf -h\n# 文件系统                      容量  已用  可用 已用% 挂载点\n# devtmpfs                      411M     0  411M    0% /dev\n# tmpfs                         423M     0  423M    0% /dev/shm\n# tmpfs                         423M   12M  412M    3% /run\n# tmpfs                         423M     0  423M    0% /sys/fs/cgroup\n# /dev/vda1                      20G  1.8G   19G    9% /\n# tmpfs                          85M     0   85M    0% /run/user/0\n# /dev/mapper/volume--test-lv1  190M  1.6M  175M    1% /mnt\n\nlvm伸缩容扩容\numount挂载点\n\numount /mnt\n\n\n从200m扩容到300m\n\nlvresize -L 300M /dev/volume-test/lv1\n#  Size of logical volume volume-test/lv1 changed from 200.00 MiB (50 extents) to 300.00 MiB (75 extents).\n#  Logical volume volume-test/lv1 successfully resized.\n\n\n检查磁盘错误\n\ne2fsck -f /dev/volume-test/lv1\n# e2fsck 1.42.9 (28-Dec-2013)\n# 第一步: 检查inode,块,和大小\n# 第二步: 检查目录结构\n# 第3步: 检查目录连接性\n# Pass 4: Checking reference counts\n# 第5步: 检查簇概要信息\n# /dev/volume-test/lv1: 11/51200 files (0.0% non-contiguous), 12115/204800 blocks\n\n\n扩容文件系统\n\nresize2fs /dev/volume-test/lv1\n# resize2fs 1.42.9 (28-Dec-2013)\n# Resizing the filesystem on /dev/volume-test/lv1 to 307200 (1k) blocks.\n# The filesystem on /dev/volume-test/lv1 is now 307200 blocks long.\n\n\n查看扩容效果\n\nlvdisplay\n#  --- Logical volume ---\n#  LV Path                /dev/volume-test/lv1\n#  LV Name                lv1\n#  VG Name                volume-test\n#  LV UUID                w7DEiD-0Q2r-4zDz-0art-fb6S-P2X4-wLyKOk\n#  LV Write Access        read/write\n#  LV Creation host, time 10-60-116-80, 2024-07-02 14:56:09 +0800\n#  LV Status              available\n#  # open                 0\n#  LV Size                300.00 MiB\n#  Current LE             75\n#  Segments               1\n#  Allocation             inherit\n#  Read ahead sectors     auto\n#  - currently set to     8192\n#  Block device           252:0\n\n\n挂载，可以看到容量变大了\n\nmount /dev/volume-test/lv1 /mnt/\ndf -h\n# 文件系统                      容量  已用  可用 已用% 挂载点\n# devtmpfs                      411M     0  411M    0% /dev\n# tmpfs                         423M     0  423M    0% /dev/shm\n# tmpfs                         423M   12M  412M    3% /run\n# tmpfs                         423M     0  423M    0% /sys/fs/cgroup\n# /dev/vda1                      20G  1.8G   19G    9% /\n# tmpfs                          85M     0   85M    0% /run/user/0\n# /dev/mapper/volume--test-lv1  287M  2.1M  266M    1% /mnt\n\n缩容\n卸载挂载点\n\numount /dev/volume-test/lv1\n\n\n和扩容一样检查磁盘错误\n\ne2fsck -f /dev/volume-test/lv1\n# e2fsck 1.42.9 (28-Dec-2013)\n# 第一步: 检查inode,块,和大小\n# 第二步: 检查目录结构\n# 第3步: 检查目录连接性\n# Pass 4: Checking reference counts\n# 第5步: 检查簇概要信息\n# /dev/volume-test/lv1: 11/77824 files (0.0% non-contiguous), 15987/307200 blocks\n\n\n减少文件系统容量\n\nresize2fs /dev/volume-test/lv1 200M\n# resize2fs 1.42.9 (28-Dec-2013)\n# Resizing the filesystem on /dev/volume-test/lv1 to 204800 (1k) blocks.\n# The filesystem on /dev/volume-test/lv1 is now 204800 blocks long.\n\n\n\n减小逻辑卷，需要一次确认\n\nlvresize -L 200M /dev/volume-test/lv1\n#   WARNING: Reducing active logical volume to 200.00 MiB.\n#   THIS MAY DESTROY YOUR DATA (filesystem etc.)\n# Do you really want to reduce volume-test/lv1? [y/n]: y\n#   Size of logical volume volume-test/lv1 changed from 300.00 MiB (75 extents) to 200.00 MiB (50 extents).\n#   Logical volume volume-test/lv1 successfully resized.\n\n\n确认大小\n\nlvdisplay\n#  --- Logical volume ---\n#  LV Path                /dev/volume-test/lv1\n#  LV Name                lv1\n#  VG Name                volume-test\n#  LV UUID                w7DEiD-0Q2r-4zDz-0art-fb6S-P2X4-wLyKOk\n#  LV Write Access        read/write\n#  LV Creation host, time 10-60-116-80, 2024-07-02 14:56:09 +0800\n#  LV Status              available\n#  # open                 0\n#  LV Size                200.00 MiB\n#  Current LE             50\n#  Segments               1\n#  Allocation             inherit\n#  Read ahead sectors     auto\n#  - currently set to     8192\n#  Block device           252:0\n\n\n挂载文件系统\n\nmount /dev/volume-test/lv1 /mnt/\ndf -h\n# 文件系统                      容量  已用  可用 已用% 挂载点\n# devtmpfs                      411M     0  411M    0% /dev\n# tmpfs                         423M     0  423M    0% /dev/shm\n# tmpfs                         423M   12M  412M    3% /run\n# tmpfs                         423M     0  423M    0% /sys/fs/cgroup\n# /dev/vda1                      20G  1.8G   19G    9% /\n# tmpfs                          85M     0   85M    0% /run/user/0\n# /dev/mapper/volume--test-lv1  190M  1.6M  175M    1% /mnt\n\n扩容逻辑卷组\n查看当前逻辑卷组状态\n\ngdisplay volume-test\n#  --- Volume group ---\n#  VG Name               volume-test\n#  System ID\n#  Format                lvm2\n#  Metadata Areas        3\n#  Metadata Sequence No  4\n#  VG Access             read/write\n#  VG Status             resizable\n#  MAX LV                0\n#  Cur LV                1\n#  Open LV               1\n#  Max PV                0\n#  Cur PV                3\n#  Act PV                3\n#  VG Size               &lt;2.99 GiB\n#  PE Size               4.00 MiB\n#  Total PE              765\n#  Alloc PE / Size       50 / 200.00 MiB\n#  Free  PE / Size       715 / 2.79 GiB\n#  VG UUID               N0Edem-cvO5-tn1e-qjTD-9Ivk-mLSw-yBEVHp\n\nfdisk /dev/vdc\n# 欢迎使用 fdisk (util-linux 2.23.2)。\n# \n# 更改将停留在内存中，直到您决定将更改写入磁盘。\n# 使用写入命令前请三思。\n# \n# Device does not contain a recognized partition table\n# 使用磁盘标识符 0xfae484d7 创建新的 DOS 磁盘标签。\n# \n# 命令(输入 m 获取帮助)：n\n# Partition type:\n#    p   primary (0 primary, 0 extended, 4 free)\n#    e   extended\n# Select (default p): p\n# 分区号 (1-4，默认 1)：\n# 起始 扇区 (2048-41943039，默认为 2048)：\n# 将使用默认值 2048\n# Last 扇区, +扇区 or +size&#123;K,M,G&#125; (2048-41943039，默认为 41943039)：+2G\n# 分区 1 已设置为 Linux 类型，大小设为 2 GiB\n# \n# 命令(输入 m 获取帮助)：t\n# 已选择分区 1\n# Hex 代码(输入 L 列出所有代码)：\n# Hex 代码(输入 L 列出所有代码)：8e\n# 已将分区“Linux”的类型更改为“Linux LVM”\n# \n# 命令(输入 m 获取帮助)：w\n# The partition table has been altered!\n\n#Calling ioctl() to re-read partition table.\n#正在同步磁盘。\n\n\n查看创建的新分区\n\nlsblk -l\n# NAME             MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n# vdb              253:16   0   20G  0 disk\n# vdb2             253:18   0    1G  0 part\n# vdb3             253:19   0    1G  0 part\n# vdb1             253:17   0    1G  0 part\n# volume--test-lv1 252:0    0  200M  0 lvm  /mnt\n# vdc              253:32   0   20G  0 disk\n# vdc1             253:33   0    2G  0 part\n# vda              253:0    0   20G  0 disk\n# vda1             253:1    0   20G  0 part /\n\n\n创建物理卷\n\npvcreate /dev/vdc1\n#  Physical volume \"/dev/vdc1\" successfully created.\n\n\n扩容逻辑卷组\n\nvgextend volume-test /dev/vdc1\n#  Volume group \"volume-test\" successfully extended\n\nvgdisplay\n#  --- Volume group ---\n#  VG Name               volume-test\n#  System ID\n#  Format                lvm2\n#  Metadata Areas        4\n#  Metadata Sequence No  5\n#  VG Access             read/write\n#  VG Status             resizable\n#  MAX LV                0\n#  Cur LV                1\n#  Open LV               1\n#  Max PV                0\n#  Cur PV                4\n#  Act PV                4\n#  VG Size               4.98 GiB\n#  PE Size               4.00 MiB\n#  Total PE              1276\n#  Alloc PE / Size       50 / 200.00 MiB\n#  Free  PE / Size       1226 / &lt;4.79 GiB\n#  VG UUID               N0Edem-cvO5-tn1e-qjTD-9Ivk-mLSw-yBEVHp\n\n快照\n查看lv信息\n\nlvdisplay\n#  --- Logical volume ---\n#  LV Path                /dev/volume-test/lv1\n#  LV Name                lv1\n#  VG Name                volume-test\n#  LV UUID                w7DEiD-0Q2r-4zDz-0art-fb6S-P2X4-wLyKOk\n#  LV Write Access        read/write\n#  LV Creation host, time 10-60-116-80, 2024-07-02 14:56:09 +0800\n#  LV Status              available\n#  # open                 1\n#  LV Size                200.00 MiB\n#  Current LE             50\n#  Segments               1\n#  Allocation             inherit\n#  Read ahead sectors     auto\n#  - currently set to     8192\n#  Block device           252:2\n\n\n写入测试数据\n\ndate > /mnt/data\ncat /mnt/data\n# 2024年 07月 02日 星期二 16:29:17 CST\n\n\n创建快照\n\nlvcreate --size 200M --snapshot --name snap /dev/volume-test/lv1\n\n\n删除数据\n\nrm -rf /mnt/data\n\n\n卸载挂载点\n\numount /mnt\n\n\n取消逻辑卷激活，确保没写入\n\nlvchange -an /dev/volume-test/lv1\n\n\n恢复快照(合并)\n\nlvconvert --merge /dev/volume-test/snap\n#  Merging of snapshot volume-test/snap will occur on next activation of volume-test/lv1.\n\n\n激活逻辑卷,合并将在这个步骤执行\n\nlvchange -ay /dev/volume-test/lv1\n\nlvdisplay /dev/volume-test/lv1\n#  --- Logical volume ---\n#  LV Path                /dev/volume-test/lv1\n#  LV Name                lv1\n#  VG Name                volume-test\n#  LV UUID                w7DEiD-0Q2r-4zDz-0art-fb6S-P2X4-wLyKOk\n#  LV Write Access        read/write\n#  LV Creation host, time 10-60-116-80, 2024-07-02 14:56:09 +0800\n#  LV Status              available\n#  # open                 0\n#  LV Size                200.00 MiB\n#  Current LE             50\n#  Segments               1\n#  Allocation             inherit\n#  Read ahead sectors     auto\n#  - currently set to     8192\n#  Block device           252:2\n\n\n挂载磁盘并验证，之前删除的数据恢复了\n\nmount /dev/volume-test/lv1 /mnt/\ncat /mnt/data\n2024年 07月 02日 星期二 16:29:17 CST\n\n使用缓存加速\n创建一个使用ssd的pv\n\nlvcreate --size 100M --name cachevol volume-test /dev/vdc1\n\n\n将加速盘附加到逻辑卷\n\nlvconvert --type cache --cachepool cachevol volume-test/lv1\n\n\n验证\n\nlvs --all --options +devices volume-test\n\n参考资料https://linux.cn/article-3218-1.htmlhttps://docs.redhat.com/zh_hans/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/enabling-dm-cache-caching-for-a-logical-volume_enabling-caching-to-improve-logical-volume-performance\n","tags":["存储"]},{"title":"metallb","url":"/2023/08/18/metallb/","content":"metallb是一个开源的负载均衡器，主要解决裸金属情况下k8s使用lb类型的service的情况\n\n\n部署kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.10/config/manifests/metallb-native.yaml\n\nL2模式\nl2只能在同一个二层中使用，且只有容灾没有负载均衡功能，所有访问lb的流量都转发选举的主节点上在通过该节点转发到对应的pod\n\n\n配置\n获取minikube的网段\n\ndocker network inspect minikube  |grep -i Subnet\n#                    \"Subnet\": \"192.168.49.0/24\",\n\n\n配置lb地址范围\n\napiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: first-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  #- 192.168.10.0/24\n  - 192.168.49.100-192.168.49.200 # docker的网络中拿去一分部给lb\n  #- fc00:f853:0ccd:e799::/124\n\n\n配置该地址范围的类型\n\napiVersion: metallb.io/v1beta1\nkind: L2Advertisement\nmetadata:\n  name: example\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - first-pool\n\n测试\n部署一个服务器\n\nkubectl apply -f https://raw.githubusercontent.com/NatureLR/net-echo/master/k8s.yaml\nkubectl patch svc net-echo -p '&#123;\"spec\":&#123;\"type\": \"LoadBalancer\"&#125;&#125;'\n\nk get svc net-echo -o wide\nNAME       TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE   SELECTOR\nnet-echo   LoadBalancer   10.103.124.212   192.168.49.100   80:31495/TCP   54s   app=net-echo\n\n\n我得测试环境为minikube,创建一个在同一个二层的pod\n\n# 指定网络为minikube\ndocker run --net minikube -it  alpine sh\n# 安装curl\napk add curl\n\ncurl 192.168.49.100\n#ClientAddr: 10.244.205.192:48450\n#ClientReqPath: /\n#ClientReqMeth: GET\n#ServerHostName: net-echo-7cccf56f57-8xtcw\n#ServerAddr: 10.244.205.194\n\n\n查看arp表，192.168.49.100这个vip是02:42:c0:a8:31:03响应的\n\narp -n\n# ? (192.168.49.3) at 02:42:c0:a8:31:03 [ether]  on eth0\n# ? (192.168.49.100) at 02:42:c0:a8:31:03 [ether]  on eth0\n# ? (192.168.49.1) at 02:42:56:00:61:a6 [ether]  on eth0\n\n\n请求最终发送到了minikube-m02这个节点然后再到pod上\n\nminikube ssh --node=\"minikube-m02\"\nip addr |grep 02:42:c0:a8:31:03\n\n#docker@minikube-m02:~$ ip addr |grep 02:42:c0:a8:31:03\n#    link/ether 02:42:c0:a8:31:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n\n\n在起一个容器,会发现其实都是同一个节点在响应这也是二层的\n\ndocker run --net minikube -it  alpine sh\napk add curl\n\ncurl 192.168.49.100\n# / #  curl 192.168.49.100\n# ClientAddr: 10.244.205.192:61123\n# ClientReqPath: /\n# ClientReqMeth: GET\n# ServerHostName: net-echo-7cccf56f57-8xtcw\n# ServerAddr: 10.244.205.194\n\narp -n\n# / # arp -n\n# ? (192.168.49.1) at 02:42:56:00:61:a6 [ether]  on eth0\n# ? (192.168.49.100) at 02:42:c0:a8:31:03 [ether]  on eth0\n# ? (192.168.49.3) at 02:42:c0:a8:31:03 [ether]  on eth0\n\n\n可以看到同一个二层的集群外的机器访问lb都是通过mac流向同一个node，从而通过kube-proxy流向pod\n\nBGP模式\nbgp模式通过向路由发布bgp路由从而实现vip功能，此功能需要路由器支持bgp协议故无法实验\n\n参考资料https://metallb.universe.tf\n","tags":["k8s","负载均衡"]},{"title":"minikube安装使用","url":"/2020/09/14/minikube%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/","content":"在做k8s开发的时候受限于本地的性能以及复杂度不能搭建一个完整的k8s集群，这个时候需要minikube来搭建k8s开发环境\n\n\n下载安装\n阿里云版本地址,官方版本地址,推荐阿里云版本\n\n下载阿里云版本二进制文件Macoscurl -Lo minikube https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.13.0/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/\n\nLinuxcurl -Lo minikube https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.14.2/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/\n\n验证安装执行minikube version验证安装\n启动Minikubeminikube start --driver=docker --image-mirror-country cn\n\n这样就启动一个使用docker作为驱动的minikube，稍等一会就会启动成功，并且将kubectl设置为minikube再次启动是只需要执行minikube start即可\n多节点\n添加\n\nminikube node add\n\n\n查看\n\nminikube node list\n\n\n删除\n\nminikube delete &lt;名字>\n\n常用命令\nminikube start 启动集群\n\nminikube stop 停止集群\n\nminikube delete 删除集群\n\nminikube dashboard 打开k8s报表\n\nminikube status 查看minikube状态\n\nminikube ssh 登录到minikube节点上\n\n\n","tags":["k8s","minikube"]},{"title":"node-local-dns","url":"/2025/04/08/node-local-dns/","content":"NodeLocal DNSCache通过在使用ds部署一个dns缓存在每个节点上，从而解决pod的dns请求转发到dns所在的节点\n\n\n同时解决由于conntrack 竞争导致的UDP DNS条目填满conntrack表\ndns缓存在请求dns的时候使用tcp而非udp\n\n架构图如下\n\n\n安装wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml\n\n\niptables\n\nkubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath=&#123;.spec.clusterIP&#125;`\ndomain=cluster.local\nlocaldns=169.254.20.10\nsed -i \"s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g\" nodelocaldns.yaml\n\n\nipvs\n\nkubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath=&#123;.spec.clusterIP&#125;`\ndomain=cluster.local\nlocaldns=169.254.20.10\nsed -i \"s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/$kubedns/g\" nodelocaldns.yaml\n\n\n测试pod\n\nkubectl run busybox --image=busybox --restart=Never --namespace=default --command -- sleep infinity\n\n使用修改kubelet配置\n添加下面的参数  \n–cluster-dns: 创建的pod的中dns地址加上nodelocaldns的地址和本来kubedns地址即可\n–cluster-domain: 集群的域名，一般是cluster.local\n\n\n\nvim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf \n# 增加 --cluster-dns\n# --cluster-dns=169.254.20.10 --cluster-dns=&lt;kube-dns ip> --cluster-domain=&lt;search domain>\n\n手动修改pod配置\n在pod中添加如下配置\n\ndnsConfig:\n  nameservers:\n  - 169.254.20.10\n  - 192.168.0.2 # kube-dns地址，根据集群网络配置会有所不同\n  options:\n  - name: ndots\n    value: \"3\"\n  - name: attempts\n    value: \"2\"\n  - name: timeout\n    value: \"1\"\n  searches: # cluster.local 根据集群domain来替换\n  - default.svc.cluster.local\n  - svc.cluster.local\n  - cluster.local\ndnsPolicy: None\n\nwebhook自动注入已经有开源的webhook来实现了参考此项目\n其中cert-mananger如果集群中有了则不需要安装,且此项目的集群domain只支持cluster.local\nkubectl apply -f deploy/cert-manager/cert-manager.yaml\nkubectl apply -f deploy/cert-manager/serving-cert.yaml\nkubectl apply -f deploy/cert-manager/issuer.yaml\nkubectl apply -f deploy/\n\n\n给默认ns打上标签，并创建测试pod\n\nkubectl label namespace default node-local-dns-injection=enabled\nkubectl run busybox --image=busybox --restart=Never --namespace=default --command -- sleep infinity\n\n\n查看测试pod的dnsconfig字段,发现多了一些字段\n\nkubectl get po busybox  -o jsonpath=\"&#123;.spec.dnsConfig&#125;\" |json_pp\n# &#123;\n#    \"nameservers\" : [\n#       \"169.254.20.10\"\n#    ],\n#    \"options\" : [\n#       &#123;\n#          \"name\" : \"ndots\",\n#          \"value\" : \"2\"\n#       &#125;\n#    ],\n#    \"searches\" : [\n#       \"default.svc.cluster.local\",\n#       \"svc.cluster.local\",\n#       \"cluster.local\"\n#    ]\n# &#125;\n\n性能测试apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: dnsperf\n  labels:\n    app: dnsperf\nspec:\n  selector:\n    matchLabels:\n      app: dnsperf\n  template:\n    metadata:\n      labels:\n        app: dnsperf\n    spec:\n      containers:\n      - name: dnsperf\n        image: dnsperf:latest\n        command: [\"/bin/sh\", \"-c\", \"sleep infinity\"]\n        #resources:\n        #  limits:\n        #    memory: \"128Mi\"\n        #    cpu: \"500m\"\n        #  requests:\n        #    memory: \"64Mi\"\n        #    cpu: \"250m\"\n\n\npod内执行下面的命令设置测试域名\n\ncat &lt;&lt;EOF >records.txt\nkubernetes A\nkubernetes.default.svc.cluster.local A\nkube-dns.kube-system.svc.cluster.local A\nEOF\n\n\npod内执行以下测试命令\n\ndnsperf -l 60 -s 169.254.20.10 -Q 100000 -d records.txt\n\n\n参数说明\n-l 测试时间\n-s 制定dns服务器\n-Q 最高qps\n-d 查询的列表\n\n\n\n参考资料https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/nodelocaldnshttps://help.aliyun.com/zh/ack/ack-managed-and-ack-dedicated/user-guide/configure-nodelocal-dnscachehttps://www.lixueduan.com/posts/kubernetes/23-node-local-dns\n","tags":["k8s","网络","dns"]},{"title":"nsenter命令使用","url":"/2021/04/26/nsenter%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/","content":"nsenter在不同的命名空间中执行命令\n\n\n\nnamespace是linux中用于个隔离资源的特性，大名鼎鼎的docker就是基于此，而nsenter就是可以在不用docker exec的情况下进入别的namespace常用的使用场景是很多容器都很精简，一些命令没有对于调试网络来说很麻烦，这个时候可以只进入改容器的网络命名空间，调试更加方便\n\n安装一般linux发行版自带,位于util-linux包中\n选项说明-t, –target pid：指定被进入命名空间的目标进程的pid-m, –mount[&#x3D;file]：进入mount命令空间。如果指定了file，则进入file的命令空间-u, –uts[&#x3D;file]：进入uts命令空间。如果指定了file，则进入file的命令空间-i, –ipc[&#x3D;file]：进入ipc命令空间。如果指定了file，则进入file的命令空间-n, –net[&#x3D;file]：进入net命令空间。如果指定了file，则进入file的命令空间-p, –pid[&#x3D;file]：进入pid命令空间。如果指定了file，则进入file的命令空间-U, –user[&#x3D;file]：进入user命令空间。如果指定了file，则进入file的命令空间-G, –setgid gid：设置运行程序的gid-S, –setuid uid：设置运行程序的uid-r, –root[&#x3D;directory]：设置根目录-w, –wd[&#x3D;directory]：设置工作目录\n例子# 获取容器的pid\ndocker inspect alpine -f '&#123;&#123;.State.Pid&#125;&#125;'\n\n# 进入pid对应的namespace的ns命名空间，这时可以执行节点的ip addr命令查看对应pid的网络情况\nsudo nsenter --target $PID --net\n\n# 等同于 docker exec\nnsenter --target $PID --mount --uts --ipc --net --pid \n\n参考资料https://man7.org/linux/man-pages/man1/nsenter.1.htmlhttps://staight.github.io/2019/09/23/nsenter%E5%91%BD%E4%BB%A4%E7%AE%80%E4%BB%8B/\n"},{"title":"nodeJS基本使用","url":"/2020/09/22/nodejs%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"nodejS和相关组件常见的命令记录\n\n\n\n中文官方：https://nodejs.org/zh-cn\n\n安装Node.js\nCentOS\nsudo yum install epel-release #安装epel源\nsudo yum install nodejs 安装nodeJs\n\n\nMacOS\nbrew install node\n官网下载安装包\n\n\n\n检查是否安装成功node --version\n\n安装NVM管理Nodejs版本有些node代码有版本要求，nvm可以在各个版本时间切换\n执行下面的命令安装：\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.1/install.sh | bash\n\n安装确认：\nnvm version\n\n配置淘宝源查看源\nnpm get registry\n\n查看修改为淘宝\nnpm config set registry http://registry.npm.taobao.org/\n\n使用nrm管理源安装nrm\nnpm install -g nrm\n\n查看源\nnrm ls\n\n  npm -------- https://registry.npmjs.org/\n  yarn ------- https://registry.yarnpkg.com/\n  cnpm ------- http://r.cnpmjs.org/\n* taobao ----- https://registry.npm.taobao.org/\n  nj --------- https://registry.nodejitsu.com/\n  npmMirror -- https://skimdb.npmjs.com/registry/\n  edunpm ----- http://registry.enpmjs.org/\n\n切换源\n# 切换到淘宝\nnrm use taobao\n\n删除源\nnrm del taobao\n\n增加源\nnrm add &lt;仓库名字> &lt;仓库地址>\n\n安装NCU检查模块更新npm install -g npm-check-updates\n\n常用命令\nnpm 命令\nnpm install xxx 安装到当前目录\nnpm install -g xxx 安装全局模块\nnpm uninstall xxx 卸载模块\nnpm uninstall -g  xxx 卸载全局模块\nnpm list –depth&#x3D;0 查看所有高级的模块\nnpm list –depth&#x3D;0 -global 查看所有全局安装的模块\n\n\nnvm\nnvm install xxx 安装指定版本的node\nnvm ls 查看现在node版本情况\nnvm use xxx 使用某个版本的node\nnvm use system 使用系统安装的node\nnvm uninstall xxx 卸载某个模块\n\n\nncu\nncu 插件模块是否有更新\nncu -g 检查全局模块是否有更新\nncu -u 更新到package.json\n\n\n\n","tags":["node","web"]},{"title":"openvswitch","url":"/2021/08/24/openvswitch/","content":"ovs是一个开源的虚拟交换机，具有强大的功能\n\n\n\novs通过flow能实现很多策略和功能\n\n安装apt\nsudo apt install openvswitch-switch \n\nyum\n官方未提供yum源需要编译安装\n\n# 安装编译依赖\nyum -y install \nepel-release \\\nrpm-build \\\nrpmlint \\\nyum-utils \\\nrpmdevtools \\\ngcc  \\\ngcc-c++ \\\nautoconf  \\\nautomake  \\\nlibtool \\\nsystemd-units  \\\nopenssl \\\nopenssl-devel \\\npython3-devel \\\ndesktop-file-utils \\\ngroff  \\\ngraphviz \\\ncheckpolicy \\\nselinux-policy-devel \\\npython3-sphinx \\ # 需要epel源\nprocps-ng \\\nlibcap-ng \\\nlibcap-ng-devel \\\nlibpcap-devel  \\\nnumactl-devel \\\ndpdk-devel \\\nlibbpf-devel \\\nnumactl-devel \\\nunbound  \\\nunbound-devel\n\n# 创建并切换到ovs用户\nuseradd ovs &amp;&amp; su - ovs \n\n# 创建编译的文件夹\nrpmdev-setuptree\nwget https://www.openvswitch.org/releases/openvswitch-2.16.0.tar.gz\ntar -C ~/rpmbuild/SOURCES/ -xzf  openvswitch-2.16.0.tar.gz\n\n# 编译为rpm\nrpmbuild -bb --nocheck ~/rpmbuild/SOURCES/openvswitch-2.16.0/rhel/openvswitch-fedora.spec\n\n# 安装\nyum -y install ~/rpmbuild/SOURCES/openvswitch-2.16.0-1.el7.x86_64.rpm\n\n# 启动服务\nsystemctl start openvswitch\nsystemctl enable openvswitch\n\nbridge查看\novs-vsctl list-br\n\n增加\n# 增加一个网桥叫vbr0\novs-vsctl add-br vbr0\n\n删除\novs-vsctl del-br ovs-switch\n\nport查看所有ports\novs-vsctl list-ports BRIDGE\n\n查看端口id\novs-vsctl list interface veth | grep \"ofport\"\n\n增加\novs-vsctl  add-port BRIDGE PORT\n\n删除\novs-vsctl del-port BRIDGE PORT\n\nflow\nflow翻译为流表，其表示一些规则，能够控制数据包的转发\n\n显示 vbr0的 flow\novs-ofctl dump-flows vbr0\n\ncookie=0x0, duration=17.496s, table=0, n_packets=0, n_bytes=0, priority=0 actions=NORMAL\n\n清除vbr0所有flows\novs-ofctl del-flows vbr0\n\n显示vbr0的groups表\novs-ofctl dump-groups vbr0\n\n增加流表\novs-ofctl add-flow vbr0 \"table=0, priority=0 actions=NORMAL\"\n\nflow语法一般语法为: 基本 匹配规则 actions 组成\n如：&quot;table=0, priority=0 actions=NORMAL&quot;\n基本:\n\nduration_sec – 生效时间\ntable_id – 所属表项，id越小匹配靠前\npriority – 优先级,数越大优先级越高\nn_packets – 处理数据包数量\nidle_timeout – 空闲超时时间（秒），超时则自动删除该表规则，0 表示该流规则永不过期。\n\nidle_timeout 不包含在 ovs-ofctl dump-flows br_name 的输出。\n匹配字段:\nin_port – vSwitch 的 INPUT Port 号dl_src (Data Link layer) – 源 MAC 地址dl_dst – 目的 MAC 地址nw_src (Network layer) – 源 IP 地址nw_dst – 目的 IP 地址tp_src – TCP&#x2F;UDP 源端口号tp_dst – TCP&#x2F;UDP 目的端口号dl_type – 以太网协议类型，又称数据包（Packet）类型ARP Packet – dl_type&#x3D;0x0806IP Packet – dl_type&#x3D;0x0800RARP Packet – dl_type&#x3D;0x8035nw_proto – 网络层协议类型，与 dl_type 一起使用ICMP Packet – dl_type&#x3D;0x0800,nw_proto&#x3D;1TCP Packet – dl_type&#x3D;0x0800,nw_proto&#x3D;6UDP Packet – dl_type&#x3D;0x0800,nw_proto&#x3D;17\nactions:\n\nNORMAL 和普通交换机一样正常转发\nOUTPUT 转发到某个端口\nGROUP 指定某个grup在处理\nDROP 丢弃\n\n例子：\n# 增加一条flows匹配端口id是1的端口，将他的数据转发到端口是2的接口上\novs-ofctl add-flow vbr0 \"table=1,priority=1,in_port=1,actions=output:2\"\n\ngroup查看\novs-ofctl dump-groups vbr0\n\n全部删除\novs-ofctl del-groups vbr0\n\n增加group表\novs-ofctl add-group vbr0 &lt;group>\n\ngroup语法组表需要在流表上跳转，一个group有很多bucket，很具类型选择执行\ngroup有很多类型(type)\n\nselect 随机执行一个Bucket，一般用于负载均衡\n\nall 所有的Bucket 都执行\n\n\ngroup表有很多可以参考后面的地址\n# 将目标地址是192.168.1.66的流量跳转到group:1去\novs-ofctl add-flow vbr0 \"table=0,priority=888,in_port=5,dl_type=0x0800,nw_dst:192.168.1.66/32,actions=group:1\"\n\n# 修改ip地址为172.16.1.1或172.16.1.1者然后从vbr0发出\novs-ofctl add-group vbr0 group_id=1,type=select,bucket=actions=mod_nw_dst:172.16.1.1,output:vbr0,bucket=actions=mod_nw_dst:172.16.1..2,output:vbr0\n\novs-docker\ndocker默认未集成ovs驱动，我们可以通过创建个无网络的容器通过ovs-docker这个工具配置网络\n\n# 启动一个无网络的容器\ndocker run --net=none --privileged=true -it ubuntu:14.04 bash\n# 在容器id为04c864c8ec59 中创建一个叫eth0的网卡并连接在vbr0,\novs-docker add-port vbr0 eth0 04c864c8ec59 --ipaddress=\"192.168.1.2/24\" --gateway=192.168.1.1\n\n\n参考资料https://zhuanlan.zhihu.com/p/37408341https://www.cnblogs.com/jmilkfan-fanguiju/p/11825081.htmlhttps://docs.openvswitch.org/en/latest\n","tags":["网络"]},{"title":"osquery","url":"/2021/05/22/osquery/","content":"osquery是一个由FaceBook开源用于对系统进行查询、监控以及分析的一款软件，其最意思的地方是使用sql来查询系统的一些信息\n\n\n安装macosbrew install --cask osquery\n\nubuntuexport OSQUERY_KEY=1484120AC4E9F8A1A577AEEE97A80C63C9D8B80B\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys $OSQUERY_KEY\nsudo add-apt-repository 'deb [arch=amd64] https://pkg.osquery.io/deb deb main'\nsudo apt-get update\nsudo apt-get install osquery\n\ncentoscurl -L https://pkg.osquery.io/rpm/GPG | sudo tee /etc/pki/rpm-gpg/RPM-GPG-KEY-osquery\nsudo yum-config-manager --add-repo https://pkg.osquery.io/rpm/osquery-s3-rpm.repo\nsudo yum-config-manager --enable osquery-s3-rpm-repo\nsudo yum install osquer\n\n使用\nosquery存在两种运行模式，分别是osqueryi(交互式模式类似sqllite)、osqueryd(后台进程模式)。\n\nosqueryi# 进入交互模式\nosqueryi\n\n\n查看所有的表\n\n.table\n\n\n查看dns这个图表的所有内容\n\n.all dns_resolvers\nselect * from dns_resolvers\n\n\n查看dns这个图表的所有内容\n\ndns_resolvers\n\n\n查看表结构\n\n.schema dns_resolvers\n\n\n设置显示模式\n\n.mod csv\n\n\n查看帮助\n\n.help\n\n常用sql# 负载\nselect period,average from load_average;\n\n# 内存\nselect memory_total,memory_free,swap_cached,active from memory_info;\n\n# 磁盘\nselect path,type,blocks,blocks_free from mounts where blocks!=0;\n\n# 查询监听0.0.0.0的进程的名字，端口和pid\nSELECT DISTINCT processes.name, listening_ports.port, processes.pid\n  FROM listening_ports JOIN processes USING (pid)\n  WHERE listening_ports.address = '0.0.0.0';\n\n\n参考资料https://osquery.io/\nSpoock’s Blog | osquery初识\n","tags":["工具"]},{"title":"pod管理","url":"/2020/01/17/pod%E7%AE%A1%E7%90%86/","content":"k8s pod常见操作\n\n\n一键删除pod状态为Terminating的PODkubectl get pods |grep Terminating |awk -F \" \" '&#123;print$1&#125;'|xargs -n 1 kubectl delete pods --force --grace-period 0\n\n横向扩容横向扩容有两种方式，使用命令或yaml文件\n手动扩容kubectl scale &lt;资源类型> &lt;资源名字> --replicas &lt;副本数量> 将pod的副本书保持到指定数量\n\n例子：kubectl scale deployment webhook –replicas 2 将test的副本数扩容到2  \n自动横向扩容（HPA）\n命令行kubectl autoscale &lt;资源类型> &lt;资源名字> --min=&lt;最小副本> --max=&lt;最大副本> --cpu-percent=&lt;CPU阈值> -n &lt;namespace>\n\nkubectl get hpa\n\nkubectl describe hpa &lt;hpa名字>\n\nkubectl deleted hpa &lt;hpa名字>  删除hpa\n\n配置文件形式apiVersion: autoscaling/v2beta1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: productpage-v1 # hpa名字\n  namespace: default\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1beta1\n    kind: Deployment\n    name: productpage-v1\n  minReplicas: 1\n  maxReplicas: 8\n  metrics:\n  - type: Resource\n    resource:\n      name: memory\n      targetAverageUtilization: 50\n  - type: Resource\n    resource:\n      name: cpu\n      targetAverageUtilization: 50\n","tags":["k8s"]},{"title":"pprometheus-operator使用","url":"/2025/01/22/prometheus-operator%E4%BD%BF%E7%94%A8/","content":"prometheus operator可以是cr的形式在k8s部署Prometheus\n\n\n架构\nprometheus operator自定义资源\nPrometheus 定义Prometheus\nPrometheusAgent，定义Prometheus不过只负责抓取数据，告警等功能不可用\nAlertmanager 定义了alertmanager\nThanosRuler thanos规则\nServiceMonitor 定义需要监控的svc\nPodMonitor 定义监控pod\nProbe 拨测配置\nScrapeConfig Prometheus抓取metrics的配置主要用于外部的资源\nPrometheusRule 定义Prometheus的告警规则\nAlertmanagerConfig 定义alertmanager的配置\n\n部署\n直接部署kube-prometnheus stack\n\ngit clone https://github.com/prometheus-operator/kube-prometheus.git\ncd kube-prometheus\nkubectl apply --server-side -f manifests/setup\nkubectl apply -f manifests/\n\n\n使用helm部署,helm部署的和上面相比缺少一些组件\n\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nhelm install prometheus  prometheus-community/kube-prometheus-stack\n\n\n默认的一些监控规则说明 https://runbooks.prometheus-operator.dev/runbooks\n\nServiceMonitorapiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app: demo\n  name: demo\nspec:\n  endpoints:\n  - port: \"http\" # metrics端口\n    path: /metrics # metrics的路径\n    metricRelabelings:\n    - action: replace\n      sourceLabels: [__name__]\n      regex: .*\n      targetLabel: model_id # 自定义标签key\n      replacement: xxxxxxxx # 自定义标签value\n  jobLabel: jobLabel\n  namespaceSelector: # namespace选择\n    matchNames:\n    - default\n  selector:  # svc选择\n    matchLabels:\n      app: demo\n\n参考资料https://prometheus.io/docs/introduction/overview/\ngrafana好看的报表：16098，13105\n","tags":["k8s","监控"]},{"title":"pve中使用arpl安装群晖.md","url":"/2023/11/05/pve%E4%B8%AD%E4%BD%BF%E7%94%A8arpl%E5%AE%89%E8%A3%85%E7%BE%A4%E6%99%96/","content":"群辉很强大，但是配置拉胯，且价格感人，使用pve虚拟化安装群辉\n\n\n使用开源的arpl固件安装\n下载arpl固件\narpl有几个版本原版是巴西的fbelavenuto项目地址https://github.com/fbelavenuto/arpl/releases但是fbelavenuto因为个人原因停更了\n\n第二个是国内的wjz304根据fbelavenuto汉化并加速了不过因为原作者停更也停更了,项目地址https://github.com/wjz304/arpl-zh_CN\n\n第三个还是wjz304的第二个则是因为原作者停更了不想破坏发布流程重新弄得一个项目https://github.com/wjz304/rr，此仓库因一些原因被被作者删除,qq群更新\n\n\npve则选择带img的\n创建pve虚拟机\n上传到pve\n\n\n\n\n创建虚机，起个名字\n\n\n\n操作系统稍后安装这里选择不要操作系统\n\n\n\n这里注意这个机型要选择q35\n\n\n\n磁盘同样不要，稍后添加\n\ncpu根据宿主机情况选择\n\n\n\n\n内存编译的时候最好大于4g\n\n\n\n网络默认即可\n\n\n\n确认页\n\n\n\n将最开始导入的img文件导入到创建的虚拟机中\n\nqm importdisk 106 /var/lib/vz/template/iso/arpl_rr_4GB.img local-lvm\n\n\n\n将手动导入的磁盘修改为sata类型的磁盘\n\n\n\n双击 即可修改\n\n\n\n再添加一个sata的磁盘作为系统盘\n\n\n\n修改引导顺序为刚刚导入的第一个磁盘\n\n\n\n启动虚拟机则看到此界面则启动成功，浏览器打开提示的地址\n\n\n构建固件\n打开之后则进入构建界面\n\n\n\n我这个版本是有中文版本的选择修改语言\n\n\n \n\n改完语言后选择选择型号\n\n \n\n我这里选择ds923+\n\n\n\n然后选择版本\n\n\n\n这里选择7.2版本了\n\n\n\n然后开始编译引导，稍等片刻\n\n\n\n编译完成后有个启动\n\n\n\n等待一会后则进入提示的地址,就进入了群辉安装引导界面了,按提示选择安装大概十分钟左\n\n\n\n\n稍等则进入dsm系统\n\n\n参考资料https://www.cnblogs.com/mokou/p/17042705.html\n","tags":["虚拟化","pve"]},{"title":"pve环境制作cloud-init模板","url":"/2024/03/28/pve%E5%88%B6%E4%BD%9Ccloud-init%E6%A8%A1%E6%9D%BF/","content":"cloudinit模版可以让我们在通过模板创建系统的时候设置好ip，用户名密码等无需开机进入后在设置\n\n\n下载cloudinit镜像\ncloudimages下载地址\n\n\n\n\n名字\n地址\n\n\n\ncentos7\nhttp://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-2211.qcow2\n\n\ndebian12\nhttps://cloud.debian.org/images/cloud/bookworm/20230802-1460/debian-12-genericcloud-amd64-20230802-1460.qcow2\n\n\nubuntu22.04\nhttp://cloud-images.ubuntu.com/releases/22.04/release/ubuntu-22.04-server-cloudimg-amd64.vmdk\n\n\n导入pve中img=\"\" # 虚拟机镜像\nid=\"\" # 虚拟机id\nname=\"\" # 虚拟机名字\ndisk=\"\" # 虚拟机存放磁盘的存储池\n\n# 创建机器\nqm create $id --name $name --net0 virtio,bridge=vmbr0\n# 导入启动盘\nqm importdisk $id $img $disk\n# 添加磁盘\nqm set $id --scsihw virtio-scsi-pci --scsi0 $disk:vm-$id-disk-0\n#调整磁盘大小\nqm disk resize $id scsi0 20G\n# 添加cloud-init\nqm set $id --ide2 $disk:cloudinit\n# 设置启动盘\nqm set $id --boot c --bootdisk scsi0\nqm set $id --serial0 socket --vga serial0\nqm set $id --agent enabled=1,fstrim_cloned_disks=1 #optional but recommended\n\n配置模版\n此时pve界面中已经可以看到这个虚拟机，启动他，然后设置这个虚拟机，后面就不用每次都要设置一些东西了\n\n\n换源：更新apt源为国内源，这里有坑有些cloudinit官方镜像（debian）使用了cloudinit代管了apt源，这就导致修改源的时候会被cloudinit给改回去正确的做法是修改cloudinit配置\n\nvim /etc/cloud/cloud.cfg\n\npackage_mirrors:\n  - arches: [default]\n   # 修改这里\n    failsafe:\n      primary: https://deb.debian.org/debian \n      security: https://deb.debian.org/debian-security\n\n\n升级并安装qga\n\nsudo apt update\nsudo apt full-upgrade\nsudo apt install qemu-guest-agent\n\nsudo systemctl start qemu-guest-agent\nsudo systemctl enable qemu-guest-agent\n\n\n修改时区\n\nsudo timedatectl set-timezone Asia/Shanghai\n\n转换为模版\n设置为模版,也可以在ui上直接设置为模板\n\nqm template $id\n","tags":["虚拟化","pve"]},{"title":"pve虚拟化安装以及基本使用","url":"/2024/04/01/pve%E8%99%9A%E6%8B%9F%E5%8C%96%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","content":"pve是一个虚拟化平台，基于debian\n\n\npve下载地址https://www.proxmox.com/en/downloads\n\n通过ventoy或者其他工具安装系统，安装过程有图形化ui很简单\n\n安装pve-tools\npve tools内置了一些常用的设置比如改国内源，去除企业版提示\n\ngit clone https://github.com/ivanhao/pvetools.git\n\n虚机假死id=\"\"\nps -ef|grep \"/usr/bin/kvm -id $id\"|grep -v grep\n\nkill -9 $id\n\n退出集群systemctl stop pve-cluster.service\nsystemctl stop corosync.service\npmxcfs  -l\n\nrm /etc/pve/corosync.conf\nrm -rf /etc/corosync/*\nkillall pmxcfs\nsystemctl start pve-cluster.service\n\nrm -rf  /etc/pve/nodes/&lt;节点名字>\n","tags":["虚拟化","pve"]},{"title":"restic备份linux目录","url":"/2024/07/09/restic%E5%A4%87%E4%BB%BDlinux%E7%9B%AE%E5%BD%95/","content":"restic是一个用go写的快速备份程序，它不仅仅支持备份到本地还支持远程，以及支持版本管理\n\n\n安装\nmacos\n\nbrew install restic\n\n\ndebian\n\napt install restic\n\n\n访问restic release下载对应的二进制https://github.com/restic/restic/releases\n\nwget https://github.com/restic/restic/releases/download/v0.16.5/restic_0.16.5_linux_amd64.bz2\nbzip2  -d restic_0.16.5_linux_amd64.bz2\nchmod +x restic_0.16.5_linux_amd64\nrestic version\n\n初始化\n初始化一个备份需要密码\n\nrestic init --repo /tmp/backup\n# enter password for new repository:\n# enter password again:\n# created restic repository f41ad7ac1a at /tmp/backup\n# \n# Please note that knowledge of your password is required to access\n# the repository. Losing your password means that your data is\n# irrecoverably lost.\n\n\n可以通过设置环境变量来设置备份地址和密码\n\nexport RESTIC_REPOSITORY=/tmp/backup\nexport RESTIC_PASSWORD=123456\nrestic init\n\n# restic init\n# created restic repository d79e5ccb6a at /tmp/backup\n# \n# Please note that knowledge of your password is required to access\n# the repository. Losing your password means that your data is\n# irrecoverably lost.\n\n\n还可以使用文件和shell来输入设置密码\n参数:–password-command，环境变量:$RESTIC_PASSWORD_COMMAND\n参数:–password-file，环境变量:$RESTIC_PASSWORD_FILE\n\n\n\n远程备份\n使用sftp备份\n\nrestic -r sftp:user@host:/data/backup init\n\n\n还有其他支持的类型的备份参考官方文档\n\n备份restic -r /tmp/backup/ --verbose backup ~/work\n\n\n–exclude可以选择忽略备份的文件\n\n可以通过snapshots查看备份\n\n\nrestic -r /tmp/backup snapshots\n# repository d79e5ccb opened (version 2, compression level auto)\n# ID        Time                 Host        Tags        Paths\n# ------------------------------------------------------------------------\n# eb8f1539  2024-07-11 11:20:07  docker                  /home/debian/work\n# ------------------------------------------------------------------------\n\n备份管理查看快照restic -r /tmp/backup snapshots\n# repository d79e5ccb opened (version 2, compression level auto)\n# ID        Time                 Host        Tags        Paths\n# ------------------------------------------------------------------------\n# eb8f1539  2024-07-11 11:20:07  docker                  /home/debian/work\n# 35d7dbd3  2024-07-11 11:48:20  docker                  /home/debian/work\n# ------------------------------------------------------------------------\n# 2 snapshots\n\n挂载快照mkdir ./mnt\nrestic -r /tmp/backup/ mount ./mnt\n\n\n通过tree可以看到挂载后会按id以及hosts等方式来存放快照\n\ntree ./mnt\n# ./mnt\n# ├── hosts\n# │   └── docker\n# │       ├── 2024-07-11T11:20:07+08:00\n# │       │   └── home\n# │       │       └── debian\n# │       │           └── work\n# │       │               └── 123\n# │       └── latest -> 2024-07-11T11:20:07+08:00\n# ├── ids\n# │   └── eb8f1539\n# │       └── home\n# │           └── debian\n# │               └── work\n# │                   └── 123\n# ├── snapshots\n# │   ├── 2024-07-11T11:20:07+08:00\n# │   │   └── home\n# │   │       └── debian\n# │   │           └── work\n# │   │               └── 123\n# │   └── latest -> 2024-07-11T11:20:07+08:00\n# └── tags\n# \n# 20 directories, 3 files\n\n比较2个快照restic -r /tmp/backup diff eb8f1539 35d7dbd3\n# repository d79e5ccb opened (version 2, compression level auto)\n# comparing snapshot eb8f1539 to 35d7dbd3:\n# \n# [0:00] 100.00%  2 / 2 index files loaded\n# +    /home/debian/work/456\n# \n# Files:           1 new,     0 removed,     0 changed\n# Dirs:            0 new,     0 removed\n# Others:          0 new,     0 removed\n# Data Blobs:      0 new,     0 removed\n# Tree Blobs:      4 new,     4 removed\n#   Added:   1.722 KiB\n#   Removed: 1.440 KiB\n\n删除快照restic -r /tmp/backup snapshots\n# repository d79e5ccb opened (version 2, compression level auto)\n# ID        Time                 Host        Tags        Paths\n# ------------------------------------------------------------------------\n# eb8f1539  2024-07-11 11:20:07  docker                  /home/debian/work\n# 35d7dbd3  2024-07-11 11:48:20  docker                  /home/debian/work\n# ------------------------------------------------------------------------\n# 2 snapshots\n\nrestic -r /tmp/backup forget 35d7dbd3\n# repository d79e5ccb opened (version 2, compression level auto)\n# [0:00] 100.00%  1 / 1 files deleted\n\nrestic -r /tmp/backup snapshots\n# repository d79e5ccb opened (version 2, compression level auto)\n# ID        Time                 Host        Tags        Paths\n# ------------------------------------------------------------------------\n# eb8f1539  2024-07-11 11:20:07  docker                  /home/debian/work\n# ------------------------------------------------------------------------\n# 1 snapshots\n\n还原\n还原到本地的restore目录\n\nrestic -r /tmp/backup/ restore eb8f1539 --target ./restore\n# repository d79e5ccb opened (version 2, compression level auto)\n# \n# restoring &lt;Snapshot eb8f1539 of [/home/debian/work] at 2024-07-11 11:20:07.329664725 +0800 CST by debian@docker> to ./restore\n# Summary: Restored 4 files/dirs (4 B) in 0:00\n\n\n查看备份的文件可以看到还有路径\n\ntree ./restore/\n# ./restore/\n# └── home\n#     └── debian\n#         └── work\n#             └── 123\n# \n# 4 directories, 1 file\n\n参考资料https://restic.readthedocs.io/en/latest/010_introduction.html\n","tags":["备份"]},{"title":"rpm包构建","url":"/2020/11/16/rpm%E5%8C%85%E6%9E%84%E5%BB%BA/","content":"RPM（Redhat Package Manager）是Rhel，Centos，Fedora等系统的软件包管理格式\n\n\n安装\n在centos等使用rpm的系统中安装\n\nyum install -y rpm-build rpmlint yum-utils rpmdevtools\n\n生成目录结构初始化目录结构：\nrpmdev-setuptree\n\n在home目录下生成目录结构如下\nrpmbuild\n├── BUILD\n├── BUILDROOT\n├── RPMS\n├── SOURCES\n├── SPECS\n└── SRPMS\n\n\n\n\n目录位置\n宏代码\n说明\n用途\n\n\n\nBUILD\n%_builddir\n编译目录\n%build阶段在此目录执行编译\n\n\nBUILDROOT\n%_buildrootdir\n安装虚拟目录\n%install阶段在此目录执行安装脚本\n\n\nRPMS\n%_rpmdir\nrpm目录\n生成的rpm包所在目录\n\n\nSOURCES\n%_sourcedir\n源码目录\n源码包目录,%prep阶段从此目录找需要解压的包\n\n\nSRPMS\n%_srcrpmdir\n源码rpm目录\n生成的rpm源码包所在目录\n\n\nSPECS\n%_specdir\nSpec目录\nspec文件存放的目录\n\n\n编写spec文件%global debug_package %&#123;nil&#125;\n\nName:           &#123;&#123;.project&#125;&#125;\nVersion:        %&#123;_version&#125;\nRelease:        1%&#123;?dist&#125;\nSummary:        &#123;&#123;.ShortDescribe&#125;&#125;\n\nGroup:          Application&#x2F;WebServer\nLicense:        Apache 2.0\nURL:            http:&#x2F;&#x2F;www.baidu.com\nSource0:        %&#123;name&#125;.tar.gz\n\n# 构建依赖\nBuildRequires:  git\nBuildRequires:  make\n\n# 详细描述\n%description\n\n&#123;&#123;.LongDescribe&#125;&#125;\n\n# 构建之前执行的脚本，一般为解压缩将在source目录的压缩包解压到build目录\n%prep\n\n# %setup 不加任何选项，仅将软件包打开。\n# %setup -a 切换目录前，解压指定 Source 文件，例如 &quot;-a 0&quot; 表示解压 &quot;Source0&quot;\n# %setup -n newdir 将软件包解压在newdir目录。\n# %setup -c 解压缩之前先产生目录。\n# %setup -b num 将第 num 个 source 文件解压缩。\n# %setup -D 解压前不删除目录\n# %setup -T 不使用default的解压缩操作。\n# %setup -q 不显示解包过程\n# %setup -T -b 0 将第 0 个源代码文件解压缩。\n# %setup -c -n newdir 指定目录名称 newdir，并在此目录产生 rpm 套件。\n# %setup -q 不打印解压日志\n\n%setup -q -c -n src -a 0\n\n# 编译脚本\n%build\n\ncd &#123;&#123;.project&#125;&#125; &amp;&amp; make\n\n# 检查\n%check\n\n&#123;&#123;.project&#125;&#125;&#x2F;bin&#x2F;&#123;&#123;.project&#125;&#125; version\n\n# 安装脚本,将build目录产生的可执行文件复制到buildroot虚拟目录中\n%install\n\ninstall -D  -p  -m 0755 $&#123;RPM_BUILD_DIR&#125;&#x2F;src&#x2F;&#123;&#123;.project&#125;&#125;&#x2F;bin&#x2F;&#123;&#123;.project&#125;&#125; $&#123;RPM_BUILD_ROOT&#125;%&#123;_bindir&#125;&#x2F;&#123;&#123;.project&#125;&#125;\ninstall -D -m 0644 $&#123;RPM_BUILD_DIR&#125;&#x2F;src&#x2F;&#123;&#123;.project&#125;&#125;&#x2F;&#123;&#123;.project&#125;&#125;.service $&#123;RPM_BUILD_ROOT&#125;%&#123;_unitdir&#125;&#x2F;&#123;&#123;.project&#125;&#125;.service\n\n# 说明%&#123;buildroot&#125;中那些文件和目录需要打包到rpm中\n%files\n\n%&#123;_bindir&#125;&#x2F;&#123;&#123;.project&#125;&#125;\n%&#123;_unitdir&#125;&#x2F;&#123;&#123;.project&#125;&#125;.service\n\n# 变更记录\n%changelog\n\n将上面的文件保存到rpmbuild/SPECS目录\n构建将上面的spec文件保存为test.spec到~/rpmbuild/SPECS/中执行\nrpmbuild -ba ~&#x2F;rpmbuild&#x2F;SPECS&#x2F;test.spec\n\n脚本如果没有问题的话在~/rpmbuild/RPMS目录下生成rpm文件~/rpmbuild/SRPMS为rpm源码包\n常用选项\n-ba 表示构建二进制包和源码包\n-bb 只构建二进制包\n–clean 构建完成后清理\n–define&#x3D;”k v” 定义spec中的变量\n–help 查看帮助\n\n参考https://www.cnblogs.com/michael-xiang/p/10480809.htmlhttps://www.cnblogs.com/jing99/p/9672295.html\n","tags":["rpm"]},{"title":"rsync使用","url":"/2023/06/07/rsync%E4%BD%BF%E7%94%A8/","content":"rsync是一个用于文件同步和传输的实用工具。它可以在本地或远程系统之间进行文件传输，并提供许多功能，例如增量复制、备份、远程同步等。\n\n\n安装\ncentos\n\nsudo yum install rsync\n\n\ndebian\n\nsudo apt-get install rsync\n\n\nmacos自带但是版本比较老，可以用homebrew更新\n\n基本使用\n基本使用\n\nrsync -r $src $dest\n\n\n同步元信息比如创建时间\n\nrsync -a $src $dest\n\n\n显示进度-v\n\nrsync -av $src $dest\n\n\n压缩传输-z\n\nrsync -avz $src $dest\n\n\nsrc末尾带/在目标上不创建目录,既带/意思是将目录下的文件传输到目标，不带则便是将文件夹传输到目标\n\nSSH远程同步\n一般只需要在目标前加上用户名和ip和冒号即可\n\nrsync -rv -e ssh $src root@0.0.0.0:$dest\n\n\n上传,-e ssh可以省略\n\nrsync -rv $src root@0.0.0.0:$dest\n\n\n下载\n\nrsync -rv root@0.0.0.0:$src $dest\n\n\n指定ssh端口\n\nrsync -rv -e \"ssh -p2222\" $src root@0.0.0.0:$dest\n\nrsync协议同步服务端部署创建配置文件cat &lt;&lt; EOF > rsyncd.conf\nuid = root\ngid = root\nport = 873\nfake super = yes\nuse chroot = yes\nmax connections = 200\ntimeout = 600\nignore errors\nread only = no\nlist = yes\nauth users = rsync\nsecrets file = /root/rsync/rsyncd.passwd\nlog file = /root/rsync/rsyncd.log\npid file = /root/rsync/rsyncd.pid\nlock  file = /root/rsync/rsyncd.lock\n#####################################\n[rsync]\ncomment = rsync\npath = /root/rsync/data/\nEOF\n\n\n参数说明\n\n参阅 https://docs.rockylinux.org/books/learning_rsync/04_rsync_configure/\n\n\n\n项\n说明\n\n\n\naddress &#x3D; 192.168.100.4\nrsync默认监听的IP地址\n\n\nport &#x3D; 873\nrsync默认监听的端口\n\n\npid file &#x3D; &#x2F;var&#x2F;run&#x2F;rsyncd.pid\n进程pid的文件位置\n\n\nlog file &#x3D; &#x2F;var&#x2F;log&#x2F;rsyncd.log\n日志的文件位置\n\n\n[share]\n共享名称\n\n\ncomment &#x3D; rsync\n备注或者描述信息\n\n\npath &#x3D; &#x2F;rsync&#x2F;\n所在的系统路径位置\n\n\nread only &#x3D; yes\nyes表示只读，no表示可读可写\n\n\nlist &#x3D; yes\nyes表示可以看到共享名字\n\n\ndont compress &#x3D; *.gz *.gz2 *.zip\n哪些文件类型不对它进行压缩\n\n\nauth users &#x3D; rsync\n启用虚拟用户，定义个虚拟用户叫什么。 需要自行创建\n\n\nsecrets file &#x3D; &#x2F;etc&#x2F;rsyncd_users.db\n用来指定虚拟用户的密码文件位置，必须以.db结尾。 文件的内容格式是”用户名:密码”，一行一个\n\n\nfake super  &#x3D; yes\nyes表示不需要daemon以root运行，就可以存储文件的完整属性。\n\n\nuid &#x3D;\n\n\n\ngid &#x3D;\n两个参数用来指定当以root身份运行rsync守护进程时，指定传输文件所使用的用户和组，默认都是nobody 默认是nobody\n\n\nuse chroot  &#x3D;  yes\n传输前是否需要进行根目录的锁定，yes是，no否。 rsync为了增加安全性，默认值为yes。\n\n\nmax  connections  &#x3D;  4\n允许最大的连接数，默认值为0，表示不做限制\n\n\nlock file &#x3D; &#x2F;var&#x2F;run&#x2F;rsyncd.lock\n指定的锁文件，和“max  connections ”参数关联\n\n\nexclude  &#x3D;  lost+found&#x2F;\n排除不需要传输的目录\n\n\ntransfer logging  &#x3D;  yes\n是否启用类似ftp的日志格式来记录rsync的上传和下载\n\n\ntimeout &#x3D;  900\n指定超时时间。 指定超时的时间，如果在指定时间内没有数据被传输，则rsync将直接退出。 单位为秒，默认值为0表示永不超时\n\n\nignore nonreadable &#x3D; yes\n是否忽略用户没有访问权限的文件\n\n\nmotd file &#x3D; &#x2F;etc&#x2F;rsyncd&#x2F;rsyncd.motd\n用于指定消息文件的路径。 默认情况下，是没有 motd 文件的。 这个消息就是当用户登录以后显示的欢迎信息。\n\n\nhosts allow &#x3D; 10.1.1.1&#x2F;24\n用于指定哪些IP或者网段的客户端允许访问。 可填写ip、网段、主机名、域下面的主机，多个用空格隔开。 默认允许所有人访问\n\n\nhosts deny &#x3D;  10.1.1.20\n用户指定哪些ip或者网段的客户端不允许访问。 如果hosts allow和hosts deny有相同的匹配结果，则该客户端最终不能访问。 如果客户端的地址即不在hosts allow中，也不在hosts deny中，则该客户端允许访问。 默认情况下，没有该参数\n\n\nauth  users &#x3D; li\n启用虚拟用户，多个用户用英语状态的逗号进行隔开\n\n\nsyslog facility  &#x3D; daemon\n定义系统日志的级别， 有这些值可填：auth、authpriv、cron、daemon、ftp、kern、lpr、mail、news、 security、syslog、user、uucp、 local0、local1、local2、local3、local4、local5、local6和local7。 默认值是daemon\n\n\n创建密码文件\n这个要和配置文件对应一致\n\n# 账户名:密码\necho \"rsync:123456\" > rsyncd.passwd\nchmod 600 rsyncd.passwd\n\n管理脚本\n管理脚本有很多种可以自己创建一个systemd管理，这里我就简单点使用脚本管理\n\n\n启动脚本\n\ncat &lt;&lt;EOF > start.sh\n#! /bin/bash\nrsync --daemon --config=./rsyncd.conf\nEOF\n\n\n\n停止脚本\n\ncat &lt;&lt; EOF > stop.sh\n#! /bin/bash\nkill -15 $(cat rsyncd.pid)\nEOF\n\n目录总览.\n├── data\n├── rsyncd.conf\n├── rsyncd.lock\n├── rsyncd.log\n├── rsyncd.passwd\n├── start.sh\n└── stop.sh\n\nrsync客户端访问\n在前面加了个rsync://或者::指定协议\nmodule是rsync守护进程指定的\n默认交互式输入名\n\n# 这个用户名是rsync配置文件里的虚拟用户\nrsync -rvP $src/ $user@$ip::/$module/$dest \nrsync -rvP $src/ rsync://$user@$ip/$module/$dest\n# 例子\n# rsync -rvp src/ rsync@192.168.1.1::rsync\n# rsync -rvp src/ rsync://rsync@192.168.1.1/rsync\n\n\n显示所有模块\n\nrsync -rvP $src/ rsync://$user@$ip/\nrsync -rvP $src/ $user@$ip::\n# 例子\n# rsync -rvp src/ rsync://rsync@10.69.202.146/\n# rsync -rvp src/ rsync@10.69.202.146::\n\n\n使用变量的方式传入密码\n\nRSYNC_PASSWORD=$passwd rsync -rvP $src/ $user@$ip::$module\n# 例子\n# RSYNC_PASSWORD=123456 rsync -rvp src/ rsync@192.168.1.1::rsync   \n\n\n使用文件传入密码\n\n创建密码文件,权限需要600,这个文件的格式和服务端的不一致\n\n\necho \"123456\" > rsync.passwd\nchown 600 rsync.passwd\n\n\n格式\n\nrsync -rvp $src/ $user@$addr::$module --password-file=$paaswdfile\n# 例子\n# rsync -rvp src/ rsync@192.168.1.1::rsync --password-file=rsync.passwd\n\n断点续传\n–partial传输中断不删除\n–progress显示进度\n-P 是--progress和--partial这两个参数的结合\n\nrsync -avP $src $dest\n\n镜像同步\n–delete镜像同步，目标目录和源目录一致,目标目录多余的会被删除\n\nrsync -av --delete $src $dest\n\n\n–existing 只传输目标有的的\n\n–ignore-existing 只传输目标没有的\n\n\n设置带宽\n–bwlimit 设置带宽,单位是KB&#x2F;s\n\nrsync -rv --bwlimit=1000  $src $dest\n\n文件过滤\n–include指定同步的文件\n\n# 排除日志文件\nrsync -rv --include=\"*.dat\" $src $dest\n\n\n–exclude排除同步的文件\n\n# 排除日志文件\nrsync -rv --exclude=\"*.log\" $src $dest\n\n增量备份\n–link-dest&#x3D;$DIR 和基准目录不一样的文件创建链接,注意这个目录需要时绝对路径\n\nrsync -a -v --link-dest=$base $src $dest\n\n参考资料https://www.ruanyifeng.com/blog/2020/08/rsync.html\n","tags":["rsync"]},{"title":"ssh隧道","url":"/2020/11/12/ssh%E9%9A%A7%E9%81%93/","content":"通过ssh端口转发，穿透内网或绕过防火墙，以及tcp流量加密保护\n\nSSH隧道端口转发默认是开启如果没开启则需要AllowTcpForwarding=yes\n本地端口转发\n\n在192.168.1.1上执行下面的命令通过192.168.1.2的22号端口将本地的3306端口代理到192.168.1.1的3306端口\n\nssh -g -f -N -L 3306:192.168.1.2:3306 root@172.16.1.2 -p 22\n\n远程端口转发\n\n在192.168.1.1上执行下面的命令则表示将192.168.1.2的3306端口转发到1.1.1.1的3306端口上，这样1.1.1.1就能访问192.168.1.2上3306端口的服务了\n\nssh -f -N -R 3306:192.168.1.2:3306 root@1.1.1.1 -p 22\n\n本地转发和远程转发ssh -f -N -&lt;L|R&gt; &lt;映射IP&gt;:&lt;映射端口&gt;:&lt;转发IP&gt;:&lt;转发端口&gt; &lt;服务账号&gt;@&lt;服务IP&gt; -p &lt;服务端口&gt;\n\n不管是本地转发和转成转都是转发端口\n远程和本地是相对于执行ssh隧道命令的位置来说的\n本地转发将映射端口映射到ssh客户端，也就是上面命令中的映射IP和映射端口在本地，映射IP可以省略默认为localhost且映射IP只能写本地拥有的IP，-g选项可以将绑定变为0.0.0.0,转发IP和转发端口则是服务端需要转发的地址和端口\n远程转发则将映射端口映射到ssh服务端，也就是将映射IP和映射端口放在了ssh服务端,转发ip和转发端口是通过ssh客户端执行的\n\n选项解释\n“-L选项”：表示使用本地端口转发创建ssh隧道\n“-R选项”：表示使用远程端口转发创建ssh隧道\n“-N选项”：表示创建隧道以后不连接到sshServer端，通常与”-f”选项连用\n“-f选项”：表示在后台运行ssh隧道，通常与”-N”选项连用\n“-g选项”：表示ssh隧道对应的转发端口将监听在主机的所有IP中，不使用”-g选项”时，转发端口默认只监听在主机的本地回环地址中，”-g”表示开启网关模式，远程端口转发中，无法开启网关功能\n\n","tags":["ssh","内网穿透"]},{"title":"syslog","url":"/2020/12/08/syslog/","content":"syslog是linux系统中常见得日志系统配合systemd-Journal使用\n\nrsyslogd是syslog的改进版本，可以将日志通过syslog协议发送到日志服务器\n查看log\n/var/log/下各个文件,根据配置文件设置使用grep查找 某些路径可以通过配置文件修改\n\n\nboot.log 系统启动日志\nmessage 包含整个系统的信息，mail, cron, daemon, kern, auth等相关的日志信息\ndmesg 开机启动内核缓冲日志，可以使用dmesg命令直接查看\nmaillog mail.log 邮件服务日志\nyum.log yum安装的日志\ndnf.log centos8中使用dnf来代替yum\ncron crontab定时任务的日志\nbtmp 尝试登录失败的信息，也可以使用last -f /var/log/btmp\nwtmp 登录信息，使用last -f /var/log/wtmp查看\nlastlog 最近用户登录信息，不是文本文件使用命令lastlog直接查看\nspooler linux 新闻群组方面的日志，内容一般是空的\nsssd 系统守护进程安全日志\ntuned 系统调优工具tuned的日志\nanaconda.log 存储安装相关的信息\njournal systemd-journal日志，使用journalctl查看\n\n配置文件\nsyslog的配置目录在/etc/rsyslog.conf和/etc/rsyslog.d/之中，/etc/rsyslog.conf是默认配置的文件下面以centos8为例解释\n\n#### MODULES ####                                                                                      # 模块设置\n\nmodule(load&#x3D;&quot;imuxsock&quot; # provides support for local system logging (e.g. via logger command)           # 提供对本地命令支持如：logger\n       SysSock.Use&#x3D;&quot;off&quot;) # Turn off message reception via local log socket;                           # 关闭本地套接字接受\n# local messages are retrieved through imjournal now.                                                  # 从systemd-journal获取消息\nmodule(load&#x3D;&quot;imjournal&quot; # provides access to the systemd journal                                       \n       StateFile&#x3D;&quot;imjournal.state&quot;) # File to store the position in the journal                        #\n#module(load&#x3D;&quot;imklog&quot;) # reads kernel messages (the same are read from journald)                       # 读取内核消息，有一些来自journald\n#module(load&#x3D;&quot;immark&quot;) # provides --MARK-- message capability                                          # MARK消息\n\n# Provides UDP syslog reception                                                                        # 接受udp syslog消息\n# for parameters see http:&#x2F;&#x2F;www.rsyslog.com&#x2F;doc&#x2F;imudp.html\n#module(load&#x3D;&quot;imudp&quot;) # needs to be done just once                                                     # 只需要做一次\n#input(type&#x3D;&quot;imudp&quot; port&#x3D;&quot;514&quot;)\n\n# Provides TCP syslog reception                                                                        # 接受tpc syslog消息\n# for parameters see http:&#x2F;&#x2F;www.rsyslog.com&#x2F;doc&#x2F;imtcp.html\n#module(load&#x3D;&quot;imtcp&quot;) # needs to be done just once\n#input(type&#x3D;&quot;imtcp&quot; port&#x3D;&quot;514&quot;)\n\n#### GLOBAL DIRECTIVES ####                                                                            # 全局目录设置\n\n# Where to place auxiliary files                                                                       # 在那放辅助文件\nglobal(workDirectory&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;rsyslog&quot;)\n\n# Use default timestamp format                                                                         # 使用默认的时间戳格式\nmodule(load&#x3D;&quot;builtin:omfile&quot; Template&#x3D;&quot;RSYSLOG_TraditionalFileFormat&quot;)\n\n# Include all config files in &#x2F;etc&#x2F;rsyslog.d&#x2F;                                                          # 导入目录下的所有文件\ninclude(file&#x3D;&quot;&#x2F;etc&#x2F;rsyslog.d&#x2F;*.conf&quot; mode&#x3D;&quot;optional&quot;)\n\n#### RULES ####                                                                                        # 规则文件\n\n# Log all kernel messages to the console.                                                              # 收集内核日志到控制台\n# Logging much else clutters up the screen.                                                            # 日志太多会把屏幕弄乱\n#kern.*                                                 &#x2F;dev&#x2F;console\n\n# Log anything (except mail) of level info or higher.                                                  # 记录任何除了邮件的日志\n# Don&#39;t log private authentication messages!                                                           # 不要记录认真消息\n*.info;mail.none;authpriv.none;cron.none                &#x2F;var&#x2F;log&#x2F;messages\n\n# The authpriv file has restricted access.                                                             # 认证相关的消息存放的路径\nauthpriv.*                                              &#x2F;var&#x2F;log&#x2F;secure\n\n# Log all the mail messages in one place.                                                              # 所有的右键消息存放位置，- 表示异步因为数据库比较多\nmail.*                                                  -&#x2F;var&#x2F;log&#x2F;maillog\n\n\n# Log cron stuff                                                                                       # 定时任务的日志\ncron.*                                                  &#x2F;var&#x2F;log&#x2F;cron\n\n# Everybody gets emergency messages                                                                    # 记录所有的大于等于emerg级别信息, 以wall方式发送给每个登录到系统的人\n*.emerg                                                 :omusrmsg:*\n\n# Save news errors of level crit and higher in a special file.                                         # 记录uucp,news.crit等存放在&#x2F;var&#x2F;log&#x2F;spooler\nuucp,news.crit                                          &#x2F;var&#x2F;log&#x2F;spooler\n\n# Save boot messages also to boot.log                                                                  # 启动相关的消息\nlocal7.*                                                &#x2F;var&#x2F;log&#x2F;boot.log\n\n# ### sample forwarding rule ###                                                                       # 转发规则\n#action(type&#x3D;&quot;omfwd&quot;  \n# An on-disk queue is created for this action. If the remote host is                                   # 为此操作创建一个磁盘队列。 如果远程主机是down掉，消息被假脱机到磁盘，并在重新启动时发送。\n# down, messages are spooled to disk and sent when it is up again.\n#queue.filename&#x3D;&quot;fwdRule1&quot;       # unique name prefix for spool files                                  # 假脱机文件的唯一名称前缀\n#queue.maxdiskspace&#x3D;&quot;1g&quot;         # 1gb space limit (use as much as possible)                           # 最多1gb的空间(尽可能多的使用)\n#queue.saveonshutdown&#x3D;&quot;on&quot;       # save messages to disk on shutdown                                   # 关机是保存消息到磁盘\n#queue.type&#x3D;&quot;LinkedList&quot;         # run asynchronously                                                  # 使用链接列表模式\n#action.resumeRetryCount&#x3D;&quot;-1&quot;    # infinite retries if host is down                                    # 主机关机则无限重试\n# Remote Logging (we use TCP for reliable delivery)                                                    # 远程日志，（使用可靠的tcp）\n# remote_host is: name&#x2F;ip, e.g. 192.168.0.1, port optional e.g. 10514                                  # 远程机器是名字&#x2F;ip\n#Target&#x3D;&quot;remote_host&quot; Port&#x3D;&quot;XXX&quot; Protocol&#x3D;&quot;tcp&quot;)\n\n常见操作# 查看状态\nsystemctl status rsyslog\n# 重启\nsystemctl restart rsyslog\n# 停止\nsystemctl stop rsyslog\n\n参考资料https://www.debugger.wiki/article/html/1563278670670182https://www.cnblogs.com/bonelee/p/9477544.html\n","tags":["日志"]},{"title":"tcpdump","url":"/2021/08/10/tcpdump/","content":"tcpdump是linux下的一个网络抓包工具\n\n\n\ntcpdump非常强大且复杂命令，是我们平常排查网络相关问题的得力助手\n\n安装一般linux发行版自带基本无需安装\n常用操作抓取有192.168.1.1的包,来源或目的\ntcpdump host 192.168.1.1\n\n在所有网卡中抓取有192.168.1.1的包,来源或目的\ntcpdump -i any host 192.168.1.1 \n\n抓取主机是192.168.1.1 或 192.168.1.2\ntcpdump -i any host 192.168.1.1 or 192.168.1.2\n\n抓取除了192.168.1.1的包\ntcpdump -i any host ! 192.168.1.1\n\n抓取所有的流量\ntcpdump -nS\n\n指定端口\ntcpdump -i any port 22\n\n抓取192.168.1.1到192.168.1.2的80端口\ntcpdump  -i any  src host  192.168.1.1 and dst host 192.168.1.2 and dst port 80\n\ntcpdump -i eth0 icmp\ntcpdump -i eth0 ip\ntcpdump -i eth0 tcp\ntcpdump -i eth0 udp\ntcpdump -i eth0 arp\n\n\n显示参数\n-n 不解析主机名\n-nn 不解析主机名以及协议\n-v 打印详情\n-e 显示二层信息\n-s 抓取全部数据0为全部，默认96\n-c 抓包次数\n\nhttp协议所有的get请求\ntcpdump -i eth0 -s 0 -A 'tcp[((tcp[12:1] &amp; 0xf0) >> 2):4] = 0x47455420'\n\nPOST 请求\ntcpdump -i any -s 0 -A 'tcp[((tcp[12:1] &amp; 0xf0) >> 2):4] = 0x504F5354'\n\n抓取80端口的http协议get请求的流量,只需要指定下tcp dst port 80指定下端口号,post同理\ntcpdump -i any -s 0 -A 'tcp dst port 80 and tcp[((tcp[12:1] &amp; 0xf0) >> 2):4] = 0x47455420'\n\n抓取192.168.1.1的80端口中get和post请求的http流量的请求和响应\ntcpdump -i any -s 0 -A 'tcp dst port 80 and tcp[((tcp[12:1] &amp; 0xf0) >> 2):4] = 0x47455420 or tcp[((tcp[12:1] &amp; 0xf0) >> 2):4] = 0x504F5354 or tcp[((tcp[12:1] &amp; 0xf0) >> 2):4] = 0x48545450 or tcp[((tcp[12:1] &amp; 0xf0) >> 2):4] = 0x3C21444F and host 192.168.1.1'\n\n监控所有的get和post的主机和地址\ntcpdump -i any  -s 0 -v -n -l | egrep -i \"POST /|GET /|Host:\"\n\n导出文件-w 表示把数据报文输出到文件-r 表示读取数据报文\n抓取所有的包保存到tcpdump.pcap\ntcpdump -i any -s 0 -X -w tcpdump.pcap\n\n读取pcap文件\ntcpdump -A -r tcpdump.pcap\n\n保存文件的时候切分文件\ntcpdump -i any host -G 60 -C 150M -w test_%Y%m%d%H%M.pcap\n\n参考资料https://www.middlewareinventory.com/blog/tcpdump-capture-http-get-post-requests-apache-weblogic-websphere/https://www.cnblogs.com/bakari/p/10748721.html\n","tags":["网络"]},{"title":"victoria-metrics部署","url":"/2025/01/22/victoria-metrics%E9%83%A8%E7%BD%B2/","content":"victoria-metrics是一个兼容Prometheus的监控，和Prometheus性能优于Prometheus同时支持集群模式横向扩容\n\n\n单点模式单点模式部署helm repo add vm https://victoriametrics.github.io/helm-charts/\nhelm repo update\n\n\n默认就是单节点模式\n\nhelm install vm vm/victoria-metrics-k8s-stack -n victoria-metrics --create-namespace \n\n单节点模式访问报表grafana\n获取grafana的密码，用户名默认为admin\n\nkubectl get secrets vm-grafana -o jsonpath=\"&#123;.data.admin-password&#125;\" | base64 -d\n\n\n转发grafana\n\nkubectl -n victoria-metrics port-forward svc/vm-grafana 3000:80\n\nvmui\n访问vmui\n\nkubectl -n victoria-metrics port-forward svc/vmsingle-vm-victoria-metrics-k8s-stack 8429:8429\n\n集群模式\n集群模式架构\n\n\n\nvminsert负责将数据写入到vmstore中\n\nvmsotre负责存储数据\n\nvmselect负责数据查询\n\nvmalert负责告警规则执行\n\nvmagent负责抓取数据并通过vminsert写数据\n\n全部链路\n\n\nflowchart LR\n  exporter(各种exporter)--&gt;vmagent(vmagent)\n  vmagent(vmagent)--&gt;vminsert(vminsert)\n  Prometheus(prometheus)-- 远程写 --&gt;vminsert(vminsert)\n  vminsert(vminsert)--&gt;vmstorage(vmstorage)\n  vmstorage[(vmstorage)]--&gt;vmselect(vmselect)\n  vmselect(vmselect)--&gt;grafana(grafana)\n  vmselect(vmselect)--&gt;vmalert(vmalert)\n  vmalert(vmalert)--&gt;alertmanager(alertmanager)\n  alertmanager(alertmanager)--&gt;alert(各种告警方式)\n\n集群模式部署\n添加vm仓库\n\nhelm repo add vm https://victoriametrics.github.io/helm-charts/\nhelm repo update\n\n\n通过命令行设置为集群模式\n\nhelm install vm vm/victoria-metrics-k8s-stack -n victoria-metrics --create-namespace --set vmcluster.enabled=true --set vmsingle.enabled=false\n\n\n导出默认的values文件通过values文件来设置\n\nhelm show values vm/victoria-metrics-k8s-stack > values.yaml\n\n\n修改values.yaml中的配置\n\nvmsingle:\n  enabled: false\nvmcluster:\n  enabled: true\n\nhelm install vm  vm/victoria-metrics-k8s-stack -n victoria-metrics -f values.yaml\n\n集群模式访问报表集群模式访访问grafana和单节点一样参考grafana\n查询ui\n访问查询ui,查询是vmselect负责的所以转发vmselect，访问路径是：http://127.0.0.1:8481/select/0/vmui\n\nkubectl -n victoria-metrics port-forward svc/vmselect-vm-victoria-metrics-k8s-stack 8481:8481\n\n收集metrics的uikubectl -n victoria-metrics port-forward svc/vmagent-vm-victoria-metrics-k8s-stack 8429:8429\n\n\nhttp://127.0.0.1:8429/targets\n服务发现http://127.0.0.1:8429/service-discovery\n\n参考资料https://docs.victoriametrics.com\n","tags":["k8s","监控"]},{"title":"ventoy","url":"/2022/03/16/ventoy/","content":"ventoy是一个国人开发的使用usb安装系统的工具\n\n\n\n我们一般安装操作系统使用以下三种方式\n\n\n使用软碟通将官方操作系统刻录到u盘里，这种原生无污染且没有第三方插件和广告等，但是比较麻烦换个操作系统就需要重新烧写且win7以下需要手动打补丁\n\n而用别人制作pe则是先进入pe系统然后在用pe的工具安装系统，这种这种需要看pe是否干净\n\n之前比较流行的ghosts等一件安装的系统则比较简单快捷，但是容易内置第三方应用\n\n\n而ventoy则可以在将自己写入到u盘中，将操作系统放入执行的文件夹在启动的时可以选择指定的操作系统，不需要烧写。配合winpe则可以完成日常常用操作，更厉害的时候他支持linux\n安装在https://www.ventoy.net/cn/download.html界面下载GitHub的国内比较慢，推荐镜像网站\n写到U盘安装的时候会格式化磁盘\n\n\n\n安装系统\n将iso文件放到u盘里面\n设置启动方式为u盘启动，就可以看到下面的界面使用使用上下键选择要启动的系统回车键安装，则可以进入系统的安装界面\n\n\n参考资料https://www.ventoy.net/cn/index.html\n","tags":["重装系统"]},{"title":"为ssh配置双因素认证","url":"/2022/12/30/%E4%B8%BAssh%E9%85%8D%E7%BD%AE%E5%8F%8C%E5%9B%A0%E7%B4%A0%E8%AE%A4%E8%AF%81/","content":"双因素认证使用totp算法来生成动态的验证码来验证\n\n\n安装google-authenticator\n安装epel源，国内也可以使用阿里的\n\nyum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\nyum install -y google-authenticator\n\n配置google-authenticatorecho \"auth required pam_google_authenticator.so\" >>/etc/pam.d/sshd\n\necho \"ChallengeResponseAuthentication yes\" >> /etc/ssh/sshd_config\n\nsystemctl restart sshd\n\n使用google-authenticator\n会问你几个问题，一路y即可，之后会有个二维码拿手机支持totp的软件扫码即可\n\n同时会在当前用户的家目录下生产一个.google_authenticator文件\n\n\ngoogle-authenticator\n\n配置秘钥登录也使用双因数认证\n默认情况下只有使用密码才会验证双因素\n\necho \"AuthenticationMethods publickey,password publickey,keyboard-interactive\" >> /etc/ssh/sshd_config\n\n\n在&#x2F;etc&#x2F;pam.d&#x2F;sshd中将 auth  substack  password-auth注释掉\n\n重启sshd服务\n\n\nsystemctl restart sshd\n\n参考资料https://blog.csdn.net/m0_37886429/article/details/103609673\n","tags":["安全"]},{"title":"使用buildx编译多平台镜像","url":"/2023/06/16/%E4%BD%BF%E7%94%A8buildx%E7%BC%96%E8%AF%91%E5%A4%9A%E5%B9%B3%E5%8F%B0%E9%95%9C%E5%83%8F/","content":"目前大部分使用docker的场景中不单单只是amd64平台了有时我们需要再arm和adm64上都能运行\n\n\n新版本的docker默认自带\n创建buildx\n查看当前buildx实例\n\ndocker buildx ls\n# NAME/NODE DRIVER/ENDPOINT STATUS  BUILDKIT PLATFORMS\n# default * docker\n#   default default         running 23.0.5   linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\n\n默认会有个实例叫default，default实例下有一个default的node，一个实例下可以有多个node,星号是默认使用的实例,node有很多种类型\n\n\n创建buildx\n\ndocker buildx create --name main --node local --driver docker-container --platform linux/amd64,linux/arm64,linux/arm/v8 --use\n# main\n\n\n查看下\n\ndocker buildx ls\nNAME/NODE DRIVER/ENDPOINT             STATUS   BUILDKIT PLATFORMS\nmain *    docker-container\n  local   unix:///var/run/docker.sock inactive          linux/amd64*, linux/arm64*, linux/arm/v8*\ndefault   docker\n  default default                     running  23.0.5   linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386\n\n\n\n\n参数\n说明\n\n\n\n–name\n实例名字\n\n\n–drive\n使用的驱动:docker,docker-contran,k8s,remote\n\n\n–driver-op\n设置各个驱动的参数，比如docker-contran的镜像，k8s驱动的副本数等\n\n\n–platform\n编译的平台\n\n\n–user\n默认使用这个实例，等同于docker buildx use\n\n\n\n使用这个实例\n\ndocker buildx use main\n\n\n当我们执行编译的时候会先下载buildx镜像并运行起来，然后使用这个容器运行的buildx来编译镜像\n\n编译\n–platform执行要编译的平台，其他的参数和普通的build差不多\n\n# 直接上传到仓库\ndocker buildx build --platform linux/amd64,linux/arm64,linux/arm -t naturelingran/m3u8-downloader -o type=registry .\n\n\n输出本地\n\ndocker buildx build --platform linux/amd64,linux/arm64,linux/arm -t naturelingran/m3u8-downloader -o type=local,dest=./output .\n\n\ntar包\n\ndocker buildx build --platform linux/amd64,linux/arm64,linux/arm -t naturelingran/m3u8-downloader --output type=tar,dest=./output.tar .\n\n\n直接导入到本地docker中，只支持单平台架构\n\ndocker buildx build --platform linux/arm64 -t naturelingran/m3u8-downloader --load . \n\n参考资料https://docs.docker.com/engine/reference/commandline/buildx_create\n","tags":["docker"]},{"title":"使用docker-registry部署docker加速仓库","url":"/2024/06/25/%E4%BD%BF%E7%94%A8docker-registry%E9%83%A8%E7%BD%B2docker%E5%8A%A0%E9%80%9F%E4%BB%93%E5%BA%93/","content":"最近个国内各大docker镜像仓库都无法访问，我们可以在海外自己部署一个镜像仓库\n\n\n部署加速仓库\n部署文件，docekr compose up -d\n\nversion: '3.8'\nservices:\n  docker-registry:\n    image: registry:latest\n    container_name: docker-registry\n    restart: always\n    volumes:\n      - ./data/docker-registry/config:/etc/docker/registry/\n      - ./data/docker-registry/lib:/var/lib/registry\n    ports:\n      - 80:5000\n\n\n将下面的配置文件保存到./data/docker-registry/config/config.yml\n\n---\nversion: 0.1\nlog:\n  fields:\n    service: registry\nstorage:\n  cache:\n    blobdescriptor: inmemory\n  filesystem:\n    rootdirectory: /var/lib/registry\n  #tag:\n  #  concurrencylimit: 8\n  #delete: #配置删除权限,默认安装的Registry不支持删除\n  #  enabled: true\nhttp:\n  addr: :5000\n  headers:\n    X-Content-Type-Options: [nosniff]\n#auth:\n#  htpasswd:\n#    realm: basic-realm\n#    path: /etc/docker/registry/htpasswd\nhealth:\n  storagedriver:\n    enabled: true\n    interval: 10s\n    threshold: 3\nproxy:\n    remoteurl: https://registry-1.docker.io\n\ndocker配置&#123;\n  \"registry-mirrors\": [\"http://&lt;ip:port>\"],\n  \"insecure-registries\": [\"http://&lt;ip:port>\"]\n&#125;\n\n\n重启docker\n\nsystemctl restart docker\n\n认证\n生成密码\n\ndocker run --rm --entrypoint htpasswd httpd:2 -Bbn &lt;账号密码> &lt;密码> > htpasswd\n\n\n将生成的文件放在上面配置的路径\n\n参考资料https://distribution.github.io/distribution/https://docs.docker.com/docker-hub/mirror/#configure-the-cache\n","tags":["docker","镜像仓库"]},{"title":"使用k3d在docker中部署k3s","url":"/2024/07/02/%E4%BD%BF%E7%94%A8k3d%E5%9C%A8docker%E4%B8%AD%E9%83%A8%E7%BD%B2k3s/","content":"k3d是一个在docker里面运行k3s的项目,主要用于本地开发k8s环境，这minikube很相似\n\n\n部署\n需要事先安装好docker和kubectl\n\ncurl -s https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;k3d-io&#x2F;k3d&#x2F;main&#x2F;install.sh | bash\n\n基本操作\n创建集群\n\nk3d cluster create &lt;集群名字>\n\n\n三节点master\n\nk3d cluster create &lt;集群名字> --servers 3\n\n\n删除集群\n\nk3d cluster delete &lt;集群名字>\n\n\n添加节点\n\nk3d node create &lt;节点名> -c &lt;集群名>\n\n\n将docker的镜像导入到k3d中\n\nk3d image import &lt;镜像名> -c &lt;集群名字>\n\nGPU\n默认k3d的镜像是没法使用gpu的需要手动构建gpu镜像,官方提供了一个dockerfile文件，cuda版本需要和节点的相同\n\nARG K3S_TAG=\"v1.28.8-k3s1\"\nARG CUDA_TAG=\"12.4.1-base-ubuntu22.04\"\n\nFROM rancher/k3s:$K3S_TAG as k3s\nFROM nvcr.io/nvidia/cuda:$CUDA_TAG\n\n# Install the NVIDIA container toolkit\nRUN apt-get update &amp;&amp; apt-get install -y curl \\\n    &amp;&amp; curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n    &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n      sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n      tee /etc/apt/sources.list.d/nvidia-container-toolkit.list \\\n    &amp;&amp; apt-get update &amp;&amp; apt-get install -y nvidia-container-toolkit \\\n    &amp;&amp; nvidia-ctk runtime configure --runtime=containerd\n\nCOPY --from=k3s / / --exclude=/bin\nCOPY --from=k3s /bin /bin\n\n# Deploy the nvidia driver plugin on startup\nCOPY device-plugin-daemonset.yaml /var/lib/rancher/k3s/server/manifests/nvidia-device-plugin-daemonset.yaml\n\nVOLUME /var/lib/kubelet\nVOLUME /var/lib/rancher/k3s\nVOLUME /var/lib/cni\nVOLUME /var/log\n\nENV PATH=\"$PATH:/bin/aux\"\n\nENTRYPOINT [\"/bin/k3s\"]\nCMD [\"agent\"]\n\n\n还需要nvidia-device-plugin\n\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: nvidia\nhandler: nvidia\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nvidia-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: nvidia-device-plugin-ds\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        name: nvidia-device-plugin-ds\n    spec:\n      runtimeClassName: nvidia # Explicitly request the runtime\n      tolerations:\n      - key: nvidia.com/gpu\n        operator: Exists\n        effect: NoSchedule\n      # Mark this pod as a critical add-on; when enabled, the critical add-on\n      # scheduler reserves resources for critical add-on pods so that they can\n      # be rescheduled after a failure.\n      # See https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/\n      priorityClassName: \"system-node-critical\"\n      containers:\n      - image: nvcr.io/nvidia/k8s-device-plugin:v0.15.0-rc.2\n        name: nvidia-device-plugin-ctr\n        env:\n          - name: FAIL_ON_INIT_ERROR\n            value: \"false\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop: [\"ALL\"]\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n\n\n使用编译脚本编译,需要实际填写自己的镜像仓库地址\n\n#!/bin/bash\n\nset -euxo pipefail\n\nK3S_TAG=$&#123;K3S_TAG:=\"v1.28.8-k3s1\"&#125; # replace + with -, if needed\nCUDA_TAG=$&#123;CUDA_TAG:=\"12.4.1-base-ubuntu22.04\"&#125;\nIMAGE_REGISTRY=$&#123;IMAGE_REGISTRY:=\"&lt;镜像仓库>\"&#125;\nIMAGE_REPOSITORY=$&#123;IMAGE_REPOSITORY:=\"rancher/k3s\"&#125;\nIMAGE_TAG=\"$K3S_TAG-cuda-$CUDA_TAG\"\nIMAGE=$&#123;IMAGE:=\"$IMAGE_REGISTRY/$IMAGE_REPOSITORY:$IMAGE_TAG\"&#125;\n\necho \"IMAGE=$IMAGE\"\n\ndocker build \\\n  --build-arg K3S_TAG=$K3S_TAG \\\n  --build-arg CUDA_TAG=$CUDA_TAG \\\n  -t $IMAGE .\ndocker push $IMAGE\necho \"Done!\"\n\n\n使用gpu镜像,其中环境变量是设置nvidia驱动功能这里设置了全部开启，不然测试pod会跑不起来，这些环境你变量在nvidia官网文档\n\nk3d cluster create gputest --image=&lt;GPU镜像> --gpus=1 -e \"NVIDIA_DRIVER_CAPABILITIES=compute,utility,compat32,graphics,video,display@server:0\"\n\n\n部署这个测试pod\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  runtimeClassName: nvidia # Explicitly request the runtime\n  restartPolicy: OnFailure\n  containers:\n    - name: cuda-vector-add\n      image: \"k8s.gcr.io/cuda-vector-add:v0.1\"\n      resources:\n        limits:\n          nvidia.com/gpu: 1\n\n\npod日志结果,部署成功\n\nkubectl logs -f cuda-vector-add\n# [Vector addition of 50000 elements]\n# Copy input data from the host memory to the CUDA device\n# CUDA kernel launch with 196 blocks of 256 threads\n# Copy output data from the CUDA device to the host memory\n# Test PASSED\n# Done\n\n参考资料https://k3d.io/v5.6.3/usage/configfile/\n","tags":["docker","k8s","gpu","k3s"]},{"title":"使用kubespray部署k8s集群","url":"/2023/05/25/%E4%BD%BF%E7%94%A8kubespray%E9%83%A8%E7%BD%B2k8s%E9%9B%86%E7%BE%A4/","content":"kubespray是k8s兴趣小组开发的一个使用ansible的部署脚本,和kubeadm需要再每个节点上操作是去中心化的,这样会很麻烦\n\n\n而kubespray则sh你在kubeadm上是用ansible将部署自动化,kubeadm不关注除了k8s组件之外的东西，然而现实是我们在部署的时候需要安装一些组件以及一些设置比如关闭swap等\n部署集群默认是在线模式,需要访问外网\n\n\n\nPython版本等问题很麻烦这里使用docker镜像\n\ndocker pull quay.io/kubespray/kubespray:v2.22.1\n\n\n启动,这里将本地的inventory文件夹和key映射到容器中供kubespray使用\n\ndocker run --rm -it -v $(pwd)/inventory:/inventory -v \"$&#123;HOME&#125;\"/.ssh/id_rsa:/root/.ssh/id_rsa quay.io/kubespray/kubespray:v2.22.1 bash\n\n\n拷贝模本文件到自己的inventory下\n\ncp -rfp inventory/sample /inventory/mycluster\n\n\n通过脚本自动生成hosts文件,也可以自己写hosts文件,且配置集群\n\ndeclare -a IPS=(10.7.19.47 10.7.170.8 10.7.36.194)\nCONFIG_FILE=/inventory/mycluster/hosts.yaml python3 contrib/inventory_builder/inventory.py $&#123;IPS[@]&#125;\n\n\n部署集群\n\nansible-playbook -i /inventory/mycluster/hosts.yaml --private-key /root/.ssh/id_rsa cluster.yml\n\n\n稍等片可以就部署一个集群\n\n节点伸缩添加节点\n在ansible清单中添加新节点相关信息，然后执行scale.yml指定新的节点名字\n\nansible-playbook -i /inventory/mycluster/hosts.yaml --private-key /root/.ssh/id_rsa scale.yml --limit $new_node\n\n删除节点\n执行remove-node.yml这个ploybook,并且添加-e node变量来指定节点\n\nansible-playbook -i /inventory/mycluster/hosts.yaml --private-key /root/.ssh/id_rsa remove-node.yml -e node=$node \n\n\n节点不在线删除\n\nansible-playbook -i /inventory/mycluster/hosts.yaml --private-key /root/.ssh/id_rsa remove-node.yml -e node=$node -e reset_nodes=false -e allow_ungraceful_removal=true\n\n\n最后在清单中删除已经清理的节点\n\n清理安装ansible-playbook -i /inventory/mycluster/hosts.yaml --private-key /root/.ssh/id_rsa reset.yml --limit $node\n\n参考资料http://blog.naturelr.cc\n","tags":["k8s"]},{"title":"使用minio-operator部署minio","url":"/2022/12/12/%E4%BD%BF%E7%94%A8minio-operator%E9%83%A8%E7%BD%B2minio/","content":"minio是一个对象存储服务,兼容s3协议\n\n\n使用minio-operator部署minio时可以图形化方便的部署\n安装\nkubectl安装minio插件\n\nkubectl krew update\n\nkubectl krew install minio\n\n\n安装minio-operator到集群\n\nkubectl minio init \n\n部署minio\n打开minio-operator的ui,该命令会打印出token,填写到\n\nkubectl minio proxy -n minio-operator\n\n\n浏览器打开localhost:9090即可打开ui，输入上面的token\n\n\n\n创建minio namespace,具体ns名字可以自己选择\n\nkubectl create namespace minio\n\n\n点击部署界面\n\n\n\n填写基本配置\n\n\n\n配置是否使用https,默认使用https，\n\n\n\n配置是否对外暴露控制台等服务，其实这个就是配置minio的svc的类型，默认是lb的\n\n\n\n剩下的还有一些监控和审计日志的配置默认即可\n\n点击创建\n\n\n\n\n稍等片刻即部署成功\n\n\n参考资料https://min.io/docs/minio/kubernetes/upstream/\n","tags":["k8s","存储"]},{"title":"使用multus-cni为pod创建多个网卡","url":"/2023/06/16/%E4%BD%BF%E7%94%A8multus-cni%E4%B8%BApod%E5%88%9B%E5%BB%BA%E5%A4%9A%E4%B8%AA%E7%BD%91%E5%8D%A1/","content":"k8s的cni一般只创建一个网卡，有些时候我需要多个网卡，multus-cni\n\n\n安装kubectl apply -f https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/master/deployments/multus-daemonset-thick.yml\n\n\n可能会出现oom,官方默认给的内存太小根据需要可以大点\n\n配置\n编写cni配置文件，根据实际情况编写\n\ncat &lt;&lt;EOF | kubectl create -f -\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: macvlan-conf\nspec:\n  config: '&#123;\n      \"cniVersion\": \"0.3.0\",\n      \"type\": \"macvlan\",\n      \"master\": \"eth0\",\n      \"mode\": \"bridge\",\n      \"ipam\": &#123;\n        \"type\": \"host-local\",\n        \"subnet\": \"192.168.58.0/24\",\n        \"rangeStart\": \"192.168.58.100\",\n        \"rangeEnd\": \"192.168.58.200\",\n        \"routes\": [\n          &#123; \"dst\": \"0.0.0.0/0\" &#125;\n        ],\n        \"gateway\": \"192.168.58.1\"\n      &#125;\n    &#125;'\nEOF\n\n\n在pod的注解上添加上面创建的cm的名字\n\nannotations:\n    k8s.v1.cni.cncf.io/networks: macvlan-conf\n\n\n进入pod会发现多一个网卡\n\n如果多个则用逗号隔开,类似\n\n\nannotations:\n    k8s.v1.cni.cncf.io/networks: macvlan-conf,macvlan-conf\n\n# k exec -it cdebug-79585bd577-ptltw -- ip addr\n# 1: lo: &lt;LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n#     inet 127.0.0.1/8 scope host lo\n#        valid_lft forever preferred_lft forever\n# 2: tunl0@NONE: &lt;NOARP> mtu 1480 qdisc noop state DOWN group default qlen 1000\n#     link/ipip 0.0.0.0 brd 0.0.0.0\n# 3: ip6tnl0@NONE: &lt;NOARP> mtu 1452 qdisc noop state DOWN group default qlen 1000\n#     link/tunnel6 :: brd :: permaddr bec6:214:eb31::\n# 5: eth0@if27: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65515 qdisc noqueue state UP group default \n#     link/ether 9a:8c:bd:86:e4:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n#     inet 10.244.120.104/32 scope global eth0\n#        valid_lft forever preferred_lft forever\n# 6: net1@if9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65535 qdisc noqueue state UP group default  第二个网卡\n#     link/ether 8a:9d:50:21:27:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n#     inet 192.168.58.103/24 brd 192.168.58.255 scope global net1\n#        valid_lft forever preferred_lft forever\n\n参考资料https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/thick-plugin.md\n","tags":["cni","网络"]},{"title":"使用netbird异地组网","url":"/2024/01/12/%E4%BD%BF%E7%94%A8netbird%E5%BC%82%E5%9C%B0%E7%BB%84%E7%BD%91/","content":"netbird是一个基于wireguard的异地组网程序,他可以做到所有的设备通过一个虚拟网络通讯\n\n\n\n自建服务端安装\n防火墙需要放行tcp的80,443;UDP的3478, 49152-65535\n\n执行以下脚本安装\n\n\nmkdir netbird &amp;&amp; cd netbird\nexport NETBIRD_DOMAIN=&lt;域名>; curl -fsSL https://github.com/netbirdio/netbird/releases/latest/download/getting-started-with-zitadel.sh | bash\n\n设置外网IP\n由于云服务商的eip是基于nat的导致coturn不知道外网ip所以我们需要告诉他,如果不设置会导致移动端连接失败该问题在v0.25.4中修复相关PR\n\necho \"external-ip=&lt;eip>\" >>turnserver.conf\n\ndocker compose up -d --force-recreate coturn\n\n添加客户端\n在ui中点击add peer有各个系统的对应添加客户端的详细说明\n\n配置路由\n比如家里的网络是192.168.1.0&#x2F;24 部署在家里的netbird客户端名字叫home\n\n点击network router-&gt;add route–&gt;Network Range中填写192.168.1.0&#x2F;24,route peer选择home(部署在家中的)–&gt;add roure 完成添加\n\n添加完成之后则可以访问家中的设备\n\n\n\n配置DNS\n当家里有自己的dns时则可以用家里的dns解析\n\ndns–&gt; add nameserver–&gt;选择 add custom nameserver–&gt; name随便你写,Nameservers选择你的dns地址，Match domains则选择那些域名解析到这个dns，Distribution groups选择那些gruop生效\n\n\n\n参考资料https://docs.netbird.io/https://docs.netbird.io/how-to/getting-started#running-net-bird-with-a-setup-key\n","tags":["网络"]},{"title":"使用skopeo同步docker镜像","url":"/2022/12/19/%E4%BD%BF%E7%94%A8skopeo%E5%90%8C%E6%AD%A5docker%E9%95%9C%E5%83%8F/","content":"在大部分场景下我们内部都会有一个镜像仓库来保证k8s活着cicd在拉镜像下的体验,以往我们需要使用docker pull\n\n\n下载下镜像然后使用docker push上传到内部仓库这个过程很繁琐,skopeo就是为了解决这个问题而诞生\n安装\ncentos7的rpm很老，建议使用容器运行\n\n\nmacos\n\nbrew install skopeo\n\n查看镜像情况\n查看镜像详情\n\nskopeo inspect docker://docker.io/alpine:latest --override-os linux\n\nskopeo list-tags docker://docker.io/alpine --override-os linux\n\n登录skopeo login -u &lt;用户名> &lt;仓库地址>\n\n复制镜像\n从本地复制到仓库\n\nskopeo copy docker-daemon:alpine:latest docker://uhub.service.ucloud.cn/naturelr/test-zxz/alpine:latest\n\n\n从一个仓库复制到另一个仓库\n\n\n–override-os linux 是因为本地是m1的mac而改镜像没有改os的所以要加上这个参数,同时还有–override-arch只不过这个是arch–override-arch amd64 同样是因为我本地m1是arm的但是我们需要amd64的如果仓库不是https的使用–dest-tls-verify&#x3D;false  \n\nskopeo copy docker://docker.io/busybox:latest docker://uhub.service.ucloud.cn/naturelr/test-zxz/busybox:latest --override-os linux --override-arch amd64\n\n\n创建保存的目录,直接mkdir貌似有问题\n\ninstall -d images\n\n\n普通复制\n\nskopeo copy docker://docker.io/busybox:latest dir:images\n\n\n保存oci格式\n\nskopeo copy docker://docker.io/busybox:latest oci:images\n\n同步镜像\n同步是指将一个镜像所有的tag全部复制到另一个地方\n\n\n从一个仓库同步到本地目录\n\nskopeo sync --src docker --dest dir uhub.service.ucloud.cn/naturelr/busybox:latest images\n\n\n从一个仓库同步到另一个仓库\n\nskopeo sync --src docker --dest docker docker.io/redis uhub.service.ucloud.cn/naturelr/test-zxz/redis\n\n\n文件同步\n\nregistry.example.com:\n    images:\n        busybox: []\n        redis:\n            - \"1.0\"\n            - \"2.0\"\n            - \"sha256:0000000000000000000000000000000011111111111111111111111111111111\"\n    images-by-tag-regex:\n        nginx: ^1\\.13\\.[12]-alpine-perl$\n    credentials:\n        username: john\n        password: this is a secret\n    tls-verify: true\n    cert-dir: /home/john/certs\nquay.io:\n    tls-verify: false\n    images:\n        coreos/etcd:\n            - latest\n\nskopeo sync --src yaml --dest docker sync.yml my-registry.local.lan/repo/\n\n删除镜像skopeo delete docker://harbor.fumai.com/library/alpine:latest\n\n参考资料https://mp.weixin.qq.com/s/WVE6Iz6AuXH0Hu_ayBfzRw\n","tags":["镜像"]},{"title":"使用velero备份k8s集群","url":"/2022/12/12/%E4%BD%BF%E7%94%A8velero%E5%A4%87%E4%BB%BDk8s%E9%9B%86%E7%BE%A4/","content":"velero是k8s的备份还原工具，他不同于etcd的数据库级备份是一个应用级的备份\n\n\n原理\n客户端安装\nmacos\n\nbrew install velero\n\n\n在https://github.com/vmware-tanzu/velero/releases中下载对应系统的二进制文件\n\n服务端安装verero使用可以使用s3协议作为存储后端\n\n创建s3认证文件\n\n[default]\naws_access_key_id=&lt;公钥>\naws_secret_access_key=&lt;私钥>\n\n\n\n安装服务端\n\n\nplugins的版本根据verero可以选择对应的版本,这里使用了minio作为后端\n\nvelero install \\\n    --provider aws \\\n    --plugins velero/velero-plugin-for-aws:v1.6.0 \\\n    --bucket velero \\\n    --secret-file ./cert \\\n    --use-volume-snapshots=false \\\n    --use-node-agent \\\n    --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://minio.minio.svc.cluster.local:80\n\n备份\n备份指定命名空间,可以多个ns，*为所有的命名空间,默认备份所有命名空间\n\n\n\n\n参数\n说明\n\n\n\n-w(–wait)\n可以实时查看备份进度\n\n\n–ttl\n备份回收的时间\n\n\n-l(–selector)\n使用标签来选择备份资源\n\n\n–include-namespaces\n包含的ns\n\n\n–exclude-namespaces\n不包含的ns\n\n\n–storage-location\n备份的位置\n\n\nvelero backup create &lt;备份的名字> --include-namespaces &lt;指定命名空间>\n\n\n查看备份\n\nvelero backup get\n\n\n查看备份详情\n\nvelero backup describe &lt;备份的名字>\n\n\n查看备份日志\n\nvelero backup logs &lt;备份的名字>\n\n\n删除备份\n\nvelero backup delete &lt;备份的名字>\n\n还原velero restore create --from-backup &lt;备份的名字&gt;\n\n\n查看还原\n\nvelero restore get\n\n\n查看还原详细信息\n\nvelero restore describe &lt;还原的名字>\n\n\n查看还原的日志\n\nvelero restore logs &lt;还原的名字>\n\n定时备份定时备份和手动备份差不多只不过添加了一个类似cron的参数\n\n每天备份指定命名空间\n\nvelero schedule create &lt;备份的名字&gt; --schedule&#x3D;&quot;@daily&quot; --include-namespaces &lt;指定命名空间&gt;\n\n\n使用cron语法来定时备份\n\nvelero schedule create &lt;备份的名字> --schedule=\"0 1 * * *\" --include-namespaces &lt;指定命名空间>\n\n备份位置velero可也设置备份多个位置\nvelero backup-location get\n\n卸载velero uninstall\n\npvc备份hostpath无法备份\n\n\n\npvc备份需要2个条件一个是安装的时候需要有--use-node-agent参数,将会部署一个ds\n\npod上需要有下面的注释\n\n具体是先使用Restic或Kopia\n\n\nbackup.velero.io/backup-volumes: '&lt;卷名字1,卷名字2...>'\n\n集群迁移\n使用备份还原迁移集群时2个使用同一个后端存储，最好安装的命令和参数一致\n集群的pvc等也要一致，如a集群备份使用的是nfs,那么b集群也要有nfs这个存储类否则还原会失败\n\n参考资料https://velero.io/docs/\n","tags":["k8s","备份"]},{"title":"初试kubevirt","url":"/2022/07/17/%E5%88%9D%E8%AF%95kubevirt/","content":"kubevirt是一个可以在k8s上管理虚拟机的应用\n\n\n\n可以通过cr的方式创建虚拟机，是k8s具备提供虚拟化服务\n\n安装\n安装资源发布在官方仓库,这里提供快捷安装方法\n\nkubevirt\n更新也是一样的\n\n# 定义版本\nexport RELEASE&#x3D;v0.51.0\n\n# 部署operator\nkubectl apply -f https:&#x2F;&#x2F;github.com&#x2F;kubevirt&#x2F;kubevirt&#x2F;releases&#x2F;download&#x2F;$RELEASE&#x2F;kubevirt-operator.yaml\n\n# 部署kubevirt的cr\nkubectl apply -f https:&#x2F;&#x2F;github.com&#x2F;kubevirt&#x2F;kubevirt&#x2F;releases&#x2F;download&#x2F;$RELEASE&#x2F;kubevirt-cr.yaml\n\n# 查看状态\nkubectl -n kubevirt wait kv kubevirt --for condition&#x3D;Available\n\n下面的yaml增加了热迁移特性且调整副本数为1\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: kubevirt.io/v1\nkind: KubeVirt\nmetadata:\n  name: kubevirt\n  namespace: kubevirt\nspec:\n  certificateRotateStrategy: &#123;&#125;\n  configuration:\n    developerConfiguration:\n      featureGates: \n      - LiveMigration # 热迁移特性\n  customizeComponents: &#123;&#125;\n  imagePullPolicy: IfNotPresent\n  infra:\n    replicas: 1 # api副本数调整为1，使用默认的2\n  workloadUpdateStrategy: &#123;&#125;\nEOF\n\n\n如果是在虚拟机中需要打开嵌套虚拟化,如果没法打开就使用软件仿真\n\nkubectl create configmap kubevirt-config -n kubevirt --from-literal debug.useEmulation&#x3D;true\n\nkubectl插件kubectl krew install virt\n\nvirtctl命令行工具export VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases | grep tag_name | grep -v -- '-rc' | sort -r | head -1 | awk -F': ' '&#123;print $2&#125;' | sed 's/,//' | xargs)\nexport ARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed 's/x86_64/amd64/') || windows-amd64.exe\necho $&#123;ARCH&#125;\n\ncurl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/$&#123;VERSION&#125;/virtctl-$&#123;VERSION&#125;-$&#123;ARCH&#125;\nchmod +x virtctl\nsudo install virtctl /usr/local/bin\n\n卸载卸载有顺序先删除自动以资源,再删除oper，强制删除ns会导致ns处于Terminating状态\n\n\nexport RELEASE=v0.54.0\nkubectl delete -n kubevirt kubevirt kubevirt --wait=true # --wait=true should anyway be default\nkubectl delete apiservices v1alpha3.subresources.kubevirt.io # this needs to be deleted to avoid stuck terminating namespaces\nkubectl delete mutatingwebhookconfigurations virt-api-mutator # not blocking but would be left over\nkubectl delete validatingwebhookconfigurations virt-api-validator # not blocking but would be left over\nkubectl delete -f https://github.com/kubevirt/kubevirt/releases/download/$&#123;RELEASE&#125;/kubevirt-operator.yaml --wait=false\n\n管理虚拟机\n创建虚拟机\n\nkubectl apply -f https://kubevirt.io/labs/manifests/vm.yaml\n\n\n查看虚拟机状态\n\nk get vms                                                  \n# NAME     AGE   STATUS    READY\n# testvm   7s    Stopped   False\n\nk get vmi\n#NAME     AGE     PHASE     IP            NODENAME         READY\n#testvm   6h16m   Running   10.244.1.44   192.168.32.133   True\n\n\n启动虚拟机\n\nvirtctl start testvm\n\n\n停止虚拟机\n\nvirtctl stop testvm\n\n\n登录虚拟机\n\nvirtctl console testvm\n\n\n删除虚拟机\n\nkubectl delete vm testvm\n\ncdi\n导入镜像创建虚拟机，使用pvc提供虚拟机磁盘\n\nCDI 支持 qemu 支持的raw和qcow2 ISO 可以使用gz或xz格式压缩图像\n安装cdiexport VERSION=$(curl -s https://github.com/kubevirt/containerized-data-importer/releases/latest | grep -o \"v[0-9]\\.[0-9]*\\.[0-9]*\")\nkubectl apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-operator.yaml\nkubectl apply -f https://github.com/kubevirt/containerized-data-importer/releases/download/$VERSION/cdi-cr.yaml\n\n创建磁盘持久卷(dv)dv datavolumes缩写，实际上是经过cdi处理之后的放在pvc的.img文件\n\npvc自动获取，从连接下载自动解压到指定的pvc中\n手动上传，使用virtctl工具上传到pvc中\n\npvc自动拉取cat &lt;&lt;EOF > pvc_fedora.yml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: \"fedora\"\n  labels:\n    app: containerized-data-importer\n  annotations:\n    cdi.kubevirt.io/storage.import.endpoint: \"https://download.fedoraproject.org/pub/fedora/linux/releases/33/Cloud/x86_64/images/Fedora-Cloud-Base-33-1.2.x86_64.raw.xz\" # 这个国内很慢建议且有时候会404,建议手动下载下来放到本地web服务上\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nEOF\n\nkubectl apply -f pvc_fedora.yaml\n\n这个时候会有创建导入到pvc的pod\n手动上传镜像192.168.32.132:31937替换为实际的nodeport地址\n\n\n访问cdi-uploadproxy这里使用nodeport\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: cdi-uploadproxy-nodeport\n  namespace: cdi\nspec:\n  ports:\n  - port: 443\n    protocol: TCP\n    targetPort: 8443\n  selector:\n    cdi.kubevirt.io: cdi-uploadproxy\n  sessionAffinity: None\n  type: NodePort\nEOF\n\n上传由于证书的问题会导致上传失败，有2种方法来解决证书问题\n使用hosts\n获取信任域名\n\necho | openssl s_client -showcerts -connect  192.168.32.132:31937 2>/dev/null \\\n     | openssl x509 -inform pem -noout -text \\\n     | sed -n -e '/Subject.*CN/p' -e '/Subject Alternative/&#123;N;p&#125;'\n\n输出\nSubject: CN=cdi-uploadproxy\n    X509v3 Subject Alternative Name:\n        DNS:cdi-uploadproxy, DNS:cdi-uploadproxy.cdi, DNS:cdi-uploadproxy.cdi.svc\n\n其中Subject就是讲作为认证的域名\n\n组合成hosts条目\n\necho \"192.168.32.132  cdi-uploadproxy\" >> /etc/hosts\n\n\n上传\n\nvirtctl image-upload dv dv-test\n    --size=5Gi \\\n    --image-path=./Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\\n    --uploadproxy-url=https://cdi-uploadproxy:31937 \\\n    --insecure # 忽略证书错误\n\n信任证书\n导出证书\n\nkubectl get secret -n cdi cdi-uploadproxy-server-cert \\\n  -o jsonpath=\"&#123;.data['tls\\.crt']&#125;\" \\\n  | base64 -d > cdi-uploadproxy-server-cert.crt\n\n\n安装证书\n\n# 安装证书\nsudo cp cdi-uploadproxy-server-cert.crt /etc/pki/ca-trust/source/anchors\n\n# 刷新证书\nsudo update-ca-trust\n\n\n上传\n\nvirtctl image-upload dv dv-test\n    --size=5Gi \\\n    --image-path=./Fedora-Cloud-Base-33-1.2.x86_64.raw.xz \\\n    --uploadproxy-url=https://cdi-uploadproxy:31937 \n    #--insecure # 不需要此参数\n\n设置默认上传地址kubectl patch cdi cdi \\\n  --type merge \\\n  --patch '&#123;\"spec\":&#123;\"config\":&#123;\"uploadProxyURLOverride\":\"https://cdi-uploadproxy:31937\"&#125;&#125;&#125;'\n\n上传的时候就不需要带--uploadproxy-url参数了\n使用pvc的作为vm系统盘cat &lt;&lt;EOF > vm1.yaml\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  generation: 1\n  labels:\n    kubevirt.io/os: linux\n  name: vm1\nspec:\n  running: true\n  template:\n      labels:\n        kubevirt.io/domain: vm1\n    spec:\n      domain:\n        cpu:\n          cores: 1\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: disk0\n          - cdrom:\n              bus: sata\n              readonly: true\n            name: cloudinitdisk\n        machine:\n          type: q35\n        resources:\n          requests:\n            memory: 512M\n      volumes:\n      - name: disk0\n        persistentVolumeClaim:\n          claimName: fedora\n      - cloudInitNoCloud:\n          userData: |\n            #cloud-config\n            hostname: vm1\n            ssh_pwauth: True\n            disable_root: false\n            ssh_authorized_keys:\n            - ssh-rsa &lt;公钥> \n        name: cloudinitdisk\nEOF\n\nkubectl apply -f vm1.yaml\n\n\n直接创建vmi\n\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: kubevirt.io/v1alpha3\nkind: VirtualMachineInstance\nmetadata:\n  name: dv-test\nspec:\n  domain:\n    devices:\n      disks:\n      - disk:\n          bus: virtio\n        name: dvdisk\n    machine:\n      type: \"\"\n    resources:\n      requests:\n        memory: 64M\n  terminationGracePeriodSeconds: 0\n  volumes:\n  - name: dvdisk\n    dataVolume:\n      name: dv-test\nstatus: &#123;&#125;\nEOF\n\n\n使用vm\n\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  labels:\n    kubevirt.io/os: linux\n  name: dv-test\nspec:\n  running: true\n  template:\n    metadata:\n      labels:\n        kubevirt.io/domain: dv-test\n    spec:\n      domain:\n        cpu:\n          cores: 1\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: disk0\n        machine:\n          type: q35\n        resources:\n          requests:\n            memory: 512M\n      volumes:\n      - name: disk0\n        dataVolume:\n          name: dv-test\n\n参考资料https://kubevirt.io/user-guide/\n","tags":["k8s","虚拟化"]},{"title":"利用lxcfs实现容器资源视图","url":"/2025/04/27/%E5%88%A9%E7%94%A8lxcfs%E5%AE%9E%E7%8E%B0%E5%AE%B9%E5%99%A8%E8%B5%84%E6%BA%90%E8%A7%86%E5%9B%BE/","content":"一般在k8s或者docker中我们设置了cpu和内存的使用限制，但是在进入容器的时候执行top和free等命令会发现显示的数值为宿主的，这是因为cgorup只是限制了cpu和内存等资源和使用\n并没有将&#x2F;proc目录下的一些信息同步\n\n\nlxcfs是一个使用FUSE实现的一个文件系统，可以让容器的资源显示被限制的资源\n\n安装wget https://copr-be.cloud.fedoraproject.org/results/ganto/lxc3/epel-7-x86_64/01041891-lxcfs/lxcfs-3.1.2-0.2.el7.x86_64.rpm;\nrpm -ivh lxcfs-3.1.2-0.2.el7.x86_64.rpm --force --nodeps\n\nsudo mkdir -p /var/lib/lxcfs\nsudo lxcfs /var/lib/lxcfs\n\n\n使用systemd来运行\n\ncat > /usr/lib/systemd/system/lxcfs.service &lt;&lt;EOF\n[Unit]\nDescription=lxcfs\n\n[Service]\nExecStart=/usr/bin/lxcfs -f /var/lib/lxcfs\nRestart=on-failure\n#ExecReload=/bin/kill -s SIGHUP $MAINPID\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\ndockersystemctl daemon-reload &amp;&amp; systemctl enable lxcfs &amp;&amp; systemctl start lxcfs &amp;&amp; systemctl status lxcfs \n\ndocker run -it --rm -m 256m  --cpus 1  \\\n      -v /var/lib/lxcfs/proc/cpuinfo:/proc/cpuinfo:rw \\\n      -v /var/lib/lxcfs/proc/diskstats:/proc/diskstats:rw \\\n      -v /var/lib/lxcfs/proc/meminfo:/proc/meminfo:rw \\\n      -v /var/lib/lxcfs/proc/stat:/proc/stat:rw \\\n      -v /var/lib/lxcfs/proc/swaps:/proc/swaps:rw \\\n      -v /var/lib/lxcfs/proc/uptime:/proc/uptime:rw \\\n      ubuntu:latest /bin/bash\n\n# root@488762b74702:/# free -h\n#                total        used        free      shared  buff/cache   available\n# Mem:           256Mi       1.4Mi       254Mi          0B          0B       254Mi\n# Swap:             0B          0B          0B\n# root@488762b74702:/# cat /proc/cpuinfo| grep \"processor\"| wc -l\n# 2\n\nk8s\ndaemonset，也可以在每个节点上使用systemd来启动lxcfs\n\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: lxcfs\n  labels:\n    app: lxcfs\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: lxcfs\n  template:\n    metadata:\n      labels:\n        app: lxcfs\n    spec:\n      hostPID: true\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      containers:\n      - name: lxcfs\n        image: registry.cn-hangzhou.aliyuncs.com/denverdino/lxcfs:3.1.2\n        imagePullPolicy: Always\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: cgroup\n          mountPath: /sys/fs/cgroup\n        - name: lxcfs\n          mountPath: /var/lib/lxcfs\n          mountPropagation: Bidirectional\n        - name: usr-local\n          mountPath: /usr/local\n        - name: usr-lib64\n          mountPath: /usr/lib64\n      volumes:\n      - name: cgroup\n        hostPath:\n          path: /sys/fs/cgroup\n      - name: usr-local\n        hostPath:\n          path: /usr/local\n      - name: usr-lib64\n        hostPath:\n          path: /usr/lib64\n      - name: lxcfs\n        hostPath:\n          path: /var/lib/lxcfs\n          type: DirectoryOrCreate\n\n\n部署cert-manager，webhook需要用他申请证书\n\nhelm install \\\n    cert-manager jetstack/cert-manager \\\n    --namespace cert-manager \\\n    --create-namespace \\\n\n\nwebhook，lxcfs-admission-webhook这个项目测试太老了新的没发部署，自己写了个：https://github.com/NatureLR/lxcfs-admission-webhook\n\ngit clone https://github.com/NatureLR/lxcfs-admission-webhook.git\ncd lxcfs-admission-webhook\nmake deploy \n\n参考资料https://github.com/lxc/lxcfshttps://k8s.huweihuang.com/project/resource/lxcfshttps://github.com/denverdino/lxcfs-admission-webhook\n","tags":["docker","k8s"]},{"title":"在k8s中calico部署和使用","url":"/2023/05/10/%E5%9C%A8k8s%E4%B8%ADcalico%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/","content":"calico是k8s中常见的网络插件,非常灵活,支持ipip,vxlan隧道bgp路由以及ebpf,虚机k8s均可使用\n\n\n架构\n\n\n\n组件\n作用\n\n\n\nfelix\n状态报告，路由规划，接口管理，acl\n\n\nbird\n负责路由宣告，以及路由反射\n\n\nconfd\n监控bgp变更，配置和更新bird的配置\n\n\n储存插件\n存储配置,有etcd和k8s两种选择\n\n\nCNI插件\n为pod配置网络\n\n\ntypha\n为confd和felix和后端存储之间提供缓存等增加性能服务\n\n\ncalicoctl\n命令行工具\n\n\ndikastes\n配合istio\n\n\n术语ippool\nip池子,默认calico将会从池子中分配给podip\n\nipamblocks\n从ippool里分割出来的ip段，为了减少路由数量，calico宣告路由时是以块为单位在pod所在的节点进行宣告的\n\n部署calico cnicurl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calico.yaml -O\n\nkubectl apply -f calico.yaml\n\n安装calicoctl\ncalicoctl使用calic的命令行客户端攻击可以用来查看一些信息，有三种安装方法选一种即可\n\n用容器的方式运行calicoctlcurl https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/calicoctl.yaml -o calicoctl.yaml\n\nkubectl apply -f calicoctl.yaml\n\necho alias calicoctl=\"kubectl exec -i -n kube-system calicoctl -- /calicoctl\"\n\n使用方法:calicoctl version\n二进制文件curl -L https://github.com/projectcalico/calico/releases/latest/download/calicoctl-linux-amd64 -o calicoctl\n\nchmod +x calicoctl\nmv calicoctl /usr/local/bin/\n\n使用方法:calicoctl version\nkubectl插件curl -L https://github.com/projectcalico/calico/releases/latest/download/calicoctl-linux-amd64 -o kubectl-calico\nchmod +x kubectl-calico\nmv kubectl-calico /usr/local/bin/\n\n使用方法: kubectl calico version\ncalicoctl常用命令\n查看ipam分配情况\n\ncalicoctl ipam show\n# +----------+---------------+-----------+------------+--------------+\n# | GROUPING |     CIDR      | IPS TOTAL | IPS IN USE |   IPS FREE   |\n# +----------+---------------+-----------+------------+--------------+\n# | IP Pool  | 10.244.0.0/16 |     65536 | 10 (0%)    | 65526 (100%) |\n# +----------+---------------+-----------+------------+--------------+\n\n\n查看blocks分配情况\n\ncalicoctl ipam show --show-blocks\n#+----------+-------------------+-----------+------------+--------------+\n#| GROUPING |       CIDR        | IPS TOTAL | IPS IN USE |   IPS FREE   |\n#+----------+-------------------+-----------+------------+--------------+\n#| IP Pool  | 10.244.0.0/16     |     65536 | 10 (0%)    | 65526 (100%) |\n#| Block    | 10.244.120.64/26  |        64 | 5 (8%)     | 59 (92%)     |\n#| Block    | 10.244.205.192/26 |        64 | 5 (8%)     | 59 (92%)     |\n#+----------+-------------------+-----------+------------+--------------+\n\n\n查看ippool\n\ncalicoctl get ippool -o wide\n# NAME                  CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR   \n# default-ipv4-ippool   10.244.0.0/16   true   Always     Never       false      false              all() \n\nIPIP模式calico的网络模式默认是IPIP模式\n\n通过calicoctl查看ippool的IPIPMODE字段，如下\n\ncalicoctl get ippool -o wide\n# NAME                  CIDR               NAT     IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR\n# default-ipv4-ippool   10.244.0.0/16      true    Always      Never       false      false              all()\n\n\n启动ipipmode分为两种，一种是Always，还有一种CrossSubnet\nalways，无论是否跨子网都通过ipip来通讯\nCrossSubnet，只有在跨子网时使用ipip模式\nNever,关闭从不使用ipip模式\n\n\n\nipip路径分析\n部署一个nginx\n\nk get po -l app=nginx -o wide          \n# NAME                     READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES\n# nginx-7fc57c59f7-4nxhh   1/1     Running   0          6m30s   10.244.120.68    minikube       &lt;none>           &lt;none>\n# nginx-7fc57c59f7-hf2g6   1/1     Running   0          6m43s   10.244.205.195   minikube-m02   &lt;none>           &lt;none>\n# nginx-7fc57c59f7-rcdtw   1/1     Running   0          6m30s   10.244.205.196   minikube-m02   &lt;none>           &lt;none>\n\n\n进入一个nginx的pod去ping另一个容器\n\nkubectl exec -it nginx-7fc57c59f7-4nxhh -- ping 10.244.205.195\n\nipip-pod到pod所在的node\n查看容器的网卡和路由信息\n\nkubectl exec -it nginx-7fc57c59f7-4nxhh -- sh -c \"ip addr;ip r\"\n# 1: lo: &lt;LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000\n#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n#     inet 127.0.0.1/8 scope host lo\n#        valid_lft forever preferred_lft forever\n# 2: tunl0@NONE: &lt;NOARP> mtu 1480 qdisc noop state DOWN qlen 1000\n#     link/ipip 0.0.0.0 brd 0.0.0.0\n# 3: ip6tnl0@NONE: &lt;NOARP> mtu 1452 qdisc noop state DOWN qlen 1000\n#     link/tunnel6 00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00 brd 00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00\n# 5: eth0@if10: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 65515 qdisc noqueue state UP \n#     link/ether fe:bb:51:af:03:a8 brd ff:ff:ff:ff:ff:ff\n#     inet 10.244.120.68/32 scope global eth0\n#        valid_lft forever preferred_lft forever\n\n# default via 169.254.1.1 dev eth0 \n# 169.254.1.1 dev eth0 scope link \n\n\n从上面的信息比较难以理解的是路由的网关是169.254.1.1，169.254.0.0&#x2F;16为保留地址一般用于dhcp获取，而calico则将容器的默认路由设置为此,当容器发现目标地址不是本ip段时，会将流量发送给网关，这时需要知道网关的mac地址这里calico将网关设置为169.254.1.1而没有任何一个网卡是169.254.1.1,其实是因为开了arp_proxy代答,具体使用了pod的veth的外面的网卡,这样流量就通过二层到达主机官方的FAQ\n\ntcpdump -i cali1143a22bb0c host 10.244.120.68 -ennnvv\n# ...\n# 09:21:15.764922 fe:bb:51:af:03:a8 > ee:ee:ee:ee:ee:ee, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Request who-has 169.254.1.1 tell 10.244.120.68, length 28\n# 09:21:15.764944 ee:ee:ee:ee:ee:ee > fe:bb:51:af:03:a8, ethertype ARP (0x0806), length 42: Ethernet (len 6), IPv4 (len 4), Reply 169.254.1.1 is-at ee:ee:ee:ee:ee:ee, length 28\n# ...\n\ncat /proc/sys/net/ipv4/conf/cali1143a22bb0c/proxy_arp\n# 1\n\n\nveth抓包\n\n tcpdump -i cali1143a22bb0c  host 10.244.120.68 -ennnvv\n# tcpdump: listening on cali1143a22bb0c, link-type EN10MB (Ethernet), capture size 262144 bytes\n# 09:39:53.417261 fe:bb:51:af:03:a8 > ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 64, id 37455, offset 0, flags [DF], proto ICMP (1), length 84)\n#     10.244.120.68 > 10.244.205.195: ICMP echo request, id 15617, seq 12, length 64\n# 09:39:53.417585 ee:ee:ee:ee:ee:ee > fe:bb:51:af:03:a8, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 62, id 24660, offset 0, flags [none], proto ICMP (1), length 84)\n#     10.244.205.195 > 10.244.120.68: ICMP echo reply, id 15617, seq 12, length 64\n# 09:39:54.417872 fe:bb:51:af:03:a8 > ee:ee:ee:ee:ee:ee, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 64, id 38148, offset 0, flags [DF], proto ICMP (1), length 84)\n#     10.244.120.68 > 10.244.205.195: ICMP echo request, id 15617, seq 13, length 64\n# 09:39:54.418089 ee:ee:ee:ee:ee:ee > fe:bb:51:af:03:a8, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 62, id 24786, offset 0, flags [none], proto ICMP (1), length 84)\n#     10.244.205.195 > 10.244.120.68: ICMP echo reply, id 15617, seq 13, length 64\n# ...\n\nipip-pod所在的node到目标所在的node的pod\n查看minikube上的路由\n\nip r\n# default via 192.168.49.1 dev eth0 \n# blackhole 10.244.120.64/26 proto bird \n# 10.244.120.65 dev califc4f8273134 scope link \n# 10.244.120.66 dev cali54e305c20b5 scope link \n# 10.244.120.67 dev cali00c313c8253 scope link \n# 10.244.120.68 dev cali1143a22bb0c scope link  # 这个就是我们进行ping的pod的路由\n# 10.244.205.192/26 via 192.168.49.3 dev tunl0 proto bird onlink  # 这个是minikube-m02这个节点上的路由，如果要访问minikube-m02上的pod则经过本路由\n# 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown \n# 192.168.49.0/24 dev eth0 proto kernel scope link src 192.168.49.2 \n\n\n隧道抓包\n\ntcpdump -i tunl0 host 10.244.120.68 -ennnvv\n# tcpdump: listening on tunl0, link-type RAW (Raw IP), capture size 262144 bytes\n# 09:40:55.459832 ip: (tos 0x0, ttl 63, id 114, offset 0, flags [DF], proto ICMP (1), length 84) 10.244.120.68 > 10.244.205.195: ICMP echo request, id 15617, seq 74, length 64\n# 09:40:55.460009 ip: (tos 0x0, ttl 63, id 58352, offset 0, flags [none], proto ICMP (1), length 84) 10.244.205.195 > 10.244.120.68: ICMP echo reply, id 15617, seq 74, length 64\n# 09:40:56.460530 ip: (tos 0x0, ttl 63, id 495, offset 0, flags [DF], proto ICMP (1), length 84) 10.244.120.68 > 10.244.205.195: ICMP echo request, id 15617, seq 75, length 64\n# 09:40:56.460780 ip: (tos 0x0, ttl 63, id 59235, offset 0, flags [none], proto ICMP (1), length 84) 10.244.205.195 > 10.244.120.68: ICMP echo reply, id 15617, seq 75, length 64\n# 09:40:57.461367 ip: (tos 0x0, ttl 63, id 979, offset 0, flags [DF], proto ICMP (1), length 84) 10.244.120.68 > 10.244.205.195: ICMP echo request, id 15617, seq 76, length 64\n# 09:40:57.461510 ip: (tos 0x0, ttl 63, id 59858, offset 0, flags [none], proto ICMP (1), length 84) 10.244.205.195 > 10.244.120.68: ICMP echo reply, id 15617, seq 76, length 64\n\n\n到此pod的流量通过ipip封装发送到目标pod所在的node上,目标node将ipip包解封包，然后查找路由表发送到目标pod的veth网卡中\n\n登录另一个node\n\n\nminikube ssh --node=\"minikube-m02\"\n# Last login: Tue May 16 10:37:54 2023 from 192.168.49.1\n# docker@minikube-m02:~$ \nsudo su\n\n\n通过ip找到对应的网卡\n\nip r |grep 10.244.205.195\n# 10.244.205.195 dev cali614e1c7b24e scope link \n\n\n抓包对应的网卡\n\ntcpdump -i cali614e1c7b24e \n# tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n# listening on cali614e1c7b24e, link-type EN10MB (Ethernet), capture size 262144 bytes\n# 10:50:48.678597 IP 10.244.120.68 > 10.244.205.195: ICMP echo request, id 21761, seq 88, length 64\n# 10:50:48.678615 IP 10.244.205.195 > 10.244.120.68: ICMP echo reply, id 21761, seq 88, length 64\n# 10:50:49.678987 IP 10.244.120.68 > 10.244.205.195: ICMP echo request, id 21761, seq 89, length 64\n# 10:50:49.679010 IP 10.244.205.195 > 10.244.120.68: ICMP echo reply, id 21761, seq 89, length 64\n# 10:50:50.680533 IP 10.244.120.68 > 10.244.205.195: ICMP echo request, id 21761, seq 90, length 64\n# 10:50:50.680595 IP 10.244.205.195 > 10.244.120.68: ICMP echo reply, id 21761, seq 90, length 64\n\n\n抓包ipip隧道网卡\n\ntcpdump -i tunl0 \n# tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n# listening on tunl0, link-type RAW (Raw IP), capture size 262144 bytes\n# 10:51:06.694392 IP 10.244.120.68 > 10.244.205.195: ICMP echo request, id 21761, seq 106, length 64\n# 10:51:06.694504 IP 10.244.205.195 > 10.244.120.68: ICMP echo reply, id 21761, seq 106, length 64\n# 10:51:07.695538 IP 10.244.120.68 > 10.244.205.195: ICMP echo request, id 21761, seq 107, length 64\n# ...\n\n\n通过以上抓包可以确定流量的路径\n\n2个pod在同一个node上\n当目标pod和源pod在同一个node上执行通过node上的路由到对应的veth网卡,不经过隧道\n\n这次ping一个在相同node上的pod的ip\n\n\nkubectl get po whoami-7c88bd4c6f-7tc5b -o wide                   \n# NAME                      READY   STATUS    RESTARTS   AGE     IP              NODE       NOMINATED NODE   READINESS GATES\n# whoami-7c88bd4c6f-7tc5b   1/1     Running   0          3h18m   10.244.120.67   minikube   &lt;none>           &lt;none>\n\nkubectl exec -it nginx-7fc57c59f7-4nxhh -- ping 10.244.120.67\n# PING 10.244.120.67 (10.244.120.67): 56 data bytes\n# PING 10.244.120.67 (10.244.120.67): 56 data bytes\n# 64 bytes from 10.244.120.67: seq=0 ttl=63 time=1.777 ms\n# 64 bytes from 10.244.120.67: seq=1 ttl=63 time=0.542 ms\n# 64 bytes from 10.244.120.67: seq=2 ttl=63 time=0.132 ms\n# ...\n\n\n直接抓包隧道发现没有流量\n\nminikube ssh \n# Last login: Tue May 16 10:41:40 2023 from 192.168.49.1\nsudo su\ntcpdump -i tunl0\n# tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n# listening on tunl0, link-type RAW (Raw IP), capture size 262144 bytes\n\n\n通过路由表找到对应的网卡\n\nip r |grep 10.244.120.67\n10.244.120.67 dev cali00c313c8253 scope link \n\n\n抓包目标的网卡\n\ntcpdump -i cali00c313c8253\n# tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n# listening on cali00c313c8253, link-type EN10MB (Ethernet), capture size 262144 bytes\n# 10:47:39.884507 IP 10.244.120.68 > 10.244.120.67: ICMP echo request, id 19969, seq 141, length 64\n# 10:47:39.884649 IP 10.244.120.67 > 10.244.120.68: ICMP echo reply, id 19969, seq 141, length 64\n# 10:47:40.885829 IP 10.244.120.68 > 10.244.120.67: ICMP echo request, id 19969, seq 142, length 64\n# ...\n\n\n通过以上抓包可以发现并没有经过隧道，而是直接路由到了目标的网卡\n\n小结\nVXLAN模式开启vxlan模式\n修改ippool中VXLANMODE字段为Always,IPIPMODE改为Never,其中VXLANMODE中的也有CrossSubnet只在跨子网时使用\n\n修改calico-node的环境变量\n\n\n# 修改环境变量\nkubectl -n kube-system set env ds/calico-node -c calico-node CALICO_IPV4POOL_IPIP=Never\nkubectl -n kube-system set env ds/calico-node -c calico-node CALICO_IPV4POOL_VXLAN=Always\n\n\n关闭bird\n\n# 将calico_backend修改为vxlan\n# calico_backend: vxlan\nkubectl edit cm -nkube-system calico-config\n\n\n关闭bird的健康检查\n\nkubectl -n kube-system edit ds calico-node\n\nlivenessProbe:\n  exec:\n    command:\n    - /bin/calico-node\n    - -felix-live\n   # - -bird-live\nreadinessProbe:\n  exec:\n    command:\n    - /bin/calico-node\n    # - -bird-ready\n    - -felix-ready\n\nvxlan路径分析\n依然使用之前的nginx来做测试\n\n节点路由\n\n\nip r\n# default via 192.168.49.1 dev eth0 \n# 10.244.0.192/26 via 10.244.0.192 dev vxlan.calico onlink \n# blackhole 10.244.120.64/26 proto 80 \n# 10.244.120.65 dev califc4f8273134 scope link \n# 10.244.120.66 dev cali54e305c20b5 scope link \n# 10.244.120.68 dev cali1143a22bb0c scope link \n# 10.244.205.192/26 via 10.244.0.192 dev vxlan.calico onlink  之前是ipip的tunl0网卡变更为vxlan的网卡\n# 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown \n# 192.168.49.0/24 dev eth0 proto kernel scope link src 192.168.49.2 \n\n\n查看vxlan.calico网卡信息\n\nip -s  addr show vxlan.calico\n#863: vxlan.calico: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65485 qdisc noqueue state UNKNOWN group default \n#    link/ether 66:66:b0:7b:5c:e1 brd ff:ff:ff:ff:ff:ff\n#    inet 10.244.120.70/32 scope global vxlan.calico\n#       valid_lft forever preferred_lft forever\n#    RX: bytes  packets  errors  dropped overrun mcast   \n#    23094      267      0       0       0       0       \n#    TX: bytes  packets  errors  dropped carrier collsns \n#    22374      268      0       0       0       0       \n\n\n搜下这个ip发现他作用为vxlan的网卡ip\n\ncalicoctl ipam show --ip=10.244.120.70\n#IP 10.244.120.70 is in use\n#Attributes:\n#  node: minikube\n#  type: vxlanTunnelAddress\n\nvxlan-pod到node\n从上面的网卡和路由信息可以看到calico只是修改了到其他节点pod的通讯方式，从ipip隧道改为vlxan隧道然后修改路由\n所以后pod到node的方式其实没有变化和ipip模式是一样的\n\nvxlan-pod到另一个node的pod\n依然开始长ping\n\n抓包vxlan网卡\n\n\ntcpdump -i vxlan.calico -eenn\n# tcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n# listening on vxlan.calico, link-type EN10MB (Ethernet), capture size 262144 bytes\n# 09:22:33.108257 66:66:b0:7b:5c:e1 > 66:9f:82:63:75:c1, ethertype IPv4 (0x0800), length 98: 10.244.120.68 > 10.244.205.196: ICMP echo request, id 30721, seq 33, length 64\n# 09:22:33.108388 66:9f:82:63:75:c1 > 66:66:b0:7b:5c:e1, ethertype IPv4 (0x0800), length 98: 10.244.205.196 > 10.244.120.68: ICMP echo reply, id 30721, seq 33, length 64\n# 09:22:34.109579 66:66:b0:7b:5c:e1 > 66:9f:82:63:75:c1, ethertype IPv4 (0x0800), length 98: 10.244.120.68 > 10.244.205.196: ICMP echo request, id 30721, seq 34, length 64\n# ...\n\n\n可以看到到vxlan网卡有我们长ping的数据包，可以确定不同node上的pod是通过vxlan来通讯\n66:66:b0:7b:5c:e1为源头pod所在node的vxlan网卡mac\n66:9f:82:63:75:c1为目标pod所在node的vxlan网卡mac\n\nvxlan-小结\nvxlan模式下知识变换了从一个node到另一个node的方式，从之前的ipip变为vxlan\npod到node没有变化\n\n\nBGP模式(full mesh)\nBGP是一个使用广泛的路由协议，分为2种一种是不同as号的ebgp已经同as号的ibgp，这里使用的是ibgp\n\n注意此模式节点只能在同一个子网中进行，如果节点不在同一个子网请参考IPIP&#x2F;VXLAN跨子网模式\n\n\n开启BGP模式\n开启bgp模式基本思路就是将ipip和vxlan模式都给关闭了\n\n\n修改ippool中VXLANMODE字段为Never,IPIPMODE改为Never\n\n修改calico-node的环境变量\n\n\n# 修改环境变量\nkubectl -n kube-system set env ds/calico-node -c calico-node CALICO_IPV4POOL_IPIP=Never\nkubectl -n kube-system set env ds/calico-node -c calico-node CALICO_IPV4POOL_VXLAN=Never\n\n\n如果关闭bird需要开启bird\n\n# 将calico_backend修改为bird\n# calico_backend: bird\nkubectl edit cm -nkube-system calico-config\n\n\n如果关闭了bird的健康检查则需要开启bird的健康检查\n\nkubectl -n kube-system edit ds calico-node\n\nlivenessProbe:\n  exec:\n    command:\n    - /bin/calico-node\n    - -felix-live\n    - -bird-live # 打开\nreadinessProbe:\n  exec:\n    command:\n    - /bin/calico-node\n    - -felix-ready\n    - -bird-ready # 打开\n\nBGP模式路径分析\n查看路由,可以发现跨节点通讯使用路由\n\nip r \n# default via 192.168.49.1 dev eth0 \n# 10.244.0.192/26 via 192.168.49.3 dev eth0 proto bird \n# blackhole 10.244.120.64/26 proto bird \n# 10.244.120.65 dev califc4f8273134 scope link \n# 10.244.120.66 dev cali54e305c20b5 scope link \n# 10.244.120.68 dev cali1143a22bb0c scope link \n# 10.244.205.192/26 via 192.168.49.3 dev eth0 proto bird # 这里已经变成了bird了\n# 172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown \n# 192.168.49.0/24 dev eth0 proto kernel scope link src 192.168.49.2 \n\n\n查看网卡,可以发现没有vxlan的网卡\n\nip addr\n# 1: lo: &lt;LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n#     inet 127.0.0.1/8 scope host lo\n#        valid_lft forever preferred_lft forever\n# 2: tunl0@NONE: &lt;NOARP,UP,LOWER_UP> mtu 65515 qdisc noqueue state UNKNOWN group default qlen 1000\n#     link/ipip 0.0.0.0 brd 0.0.0.0\n# 3: ip6tnl0@NONE: &lt;NOARP> mtu 1452 qdisc noop state DOWN group default qlen 1000\n#     link/tunnel6 :: brd ::\n# 4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n#     link/ether 02:42:7a:58:36:29 brd ff:ff:ff:ff:ff:ff\n#     inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n#        valid_lft forever preferred_lft forever\n# 5: califc4f8273134@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n#     link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1\n# 6: cali54e305c20b5@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default \n#     link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 2\n# 10: cali1143a22bb0c@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65515 qdisc noqueue state UP group default \n#     link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 4\n# 16: eth0@if17: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 65535 qdisc noqueue state UP group default \n#     link/ether 02:42:c0:a8:31:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n#     inet 192.168.49.2/24 brd 192.168.49.255 scope global eth0\n#        valid_lft forever preferred_lft forever\n\n\n通过birdcl查看路由表\n\nkubectl -n kube-system exec -it calico-node-9pwnj -c calico-node -- /bin/bash\n\nbirdcl\nshow route\n\n# bird> show route\n# 0.0.0.0/0          via 192.168.49.1 on eth0 [kernel1 07:27:41] * (10)\n# 10.244.205.192/26  via 192.168.49.3 on eth0 [Mesh_192_168_49_3 07:27:41] * (100/0) [i]\n# 192.168.49.0/24    dev eth0 [direct1 07:27:40] * (240)\n# 10.244.120.64/26   blackhole [static1 07:27:40] * (200)\n# 10.244.120.65/32   dev califc4f8273134 [kernel1 07:27:41] * (10)\n# 10.244.120.66/32   dev cali54e305c20b5 [kernel1 07:27:41] * (10)\n# 10.244.120.68/32   dev cali1143a22bb0c [kernel1 07:27:41] * (10)\n# 10.244.0.192/26    via 192.168.49.3 on eth0 [Mesh_192_168_49_3 07:27:41] * (100/0) [i]\n# 172.17.0.0/16      dev docker0 [direct1 07:27:40] * (240)\n# bird> \n\n\n通过birdcl查bgp邻居状态\n\nbirdcl show protocols\n# root@minikube /]# birdcl  show protocols\n# BIRD v0.3.3+birdv1.6.8 ready.\n# name     proto    table    state  since       info\n# static1  Static   master   up     07:27:40    \n# kernel1  Kernel   master   up     07:27:40    \n# device1  Device   master   up     07:27:40    \n# direct1  Direct   master   up     07:27:40    \n# Mesh_192_168_49_3 BGP      master   up     07:27:53    Established  bgp邻居状态\n\n\n查看bgp详细信息\n\nbirdcl show protocols all Mesh_192_168_49_2\n# root@minikube-m02 /]# birdcl show protocols all Mesh_192_168_49_2\n# BIRD v0.3.3+birdv1.6.8 ready.\n# name     proto    table    state  since       info\n# Mesh_192_168_49_2 BGP      master   up     07:27:53    Established   \n#   Description:    Connection to BGP peer\n#   Preference:     100\n#   Input filter:   ACCEPT\n#   Output filter:  calico_export_to_bgp_peers\n#   Routes:         1 imported, 2 exported, 1 preferred\n#   Route change stats:     received   rejected   filtered    ignored   accepted\n#     Import updates:              4          0          0          0          4\n#     Import withdraws:            3          0        ---          0          3\n#     Export updates:             17          4          8        ---          5\n#     Export withdraws:            8        ---        ---        ---          3\n#   BGP state:          Established\n#     Neighbor address: 192.168.49.2\n#     Neighbor AS:      64512\n#     Neighbor ID:      192.168.49.2\n#     Neighbor caps:    refresh enhanced-refresh restart-able llgr-aware AS4 add-path-rx add-path-tx\n#     Session:          internal multihop AS4 add-path-rx add-path-tx\n#     Source address:   192.168.49.3\n#     Hold timer:       157/240\n#     Keepalive timer:  67/80\n\n\n总体而言比较简单，直接通过路由到目标pod对应的节点\n\nbgp小结\n每个节点之间通过bgp宣告路由\n\n\n\nBGP-RR模式(Route reflectors)\n\n从节点中选取一部分节点作为bgp路由反射器以减少bgp对等体数量\n\n路由反射部署方案\n测试环境为4个节点的minikube集群\n\n\n部署路由反射器有很多方法:\n\n\n在集群外的机器中部署bird\n\n\n\n在k8s中选择专门的节点\n\n\n\n部署专门的calico-node容器作为反射器\n\n\n\n\n很显然直接在集群中部署专门的节点作为反射器比较方便且容易管理\n\n选择2个节点作为反射器，这里我选择后2个\n\n\n确认现在的为bgp的mesh模式需要注意calicoctl node status这个需要再部署calico的节点上执行。。。。有点唐突\n\n\n\n查看当前bgp邻居,可以发现已经与另外三个节点建立了邻居关系\n\ncalicoctl node status\n# root@minikube:~# calicoctl node status\n# Calico process is running.\n# \n# IPv4 BGP status\n# +--------------+-------------------+-------+----------+-------------+\n# | PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |\n# +--------------+-------------------+-------+----------+-------------+\n# | 192.168.49.3 | node-to-node mesh | up    | 18:13:08 | Established |\n# | 192.168.49.4 | node-to-node mesh | up    | 19:37:12 | Established |\n# | 192.168.49.5 | node-to-node mesh | up    | 02:27:27 | Established |\n# +--------------+-------------------+-------+----------+-------------+\n\ncalicoctl get nodes -o wide \n# NAME           ASN       IPV4              IPV6   \n# minikube       (64512)   192.168.49.2/24          \n# minikube-m02   (64512)   192.168.49.3/24          \n# minikube-m03   (64512)   192.168.49.4/24   # 作为反射器       \n# minikube-m04   (64512)   192.168.49.5/24   # 作为反射器\n\n指定节点作为反射器\n导出calico/node资源\n\ncalicoctl get nodes minikube-m03 -o yaml > minikube-m03.yaml\ncalicoctl get nodes minikube-m04 -o yaml > minikube-m04.yaml\n\n\n在导出的文件中添加下面的字段\n\n\nrouteReflectorClusterID应该是为了防止路由环路\n\n# ...\n  bgp:\n    ipv4Address: 192.168.49.4/24\n    routeReflectorClusterID: 1.0.0.1\n# ...\n\n\n应用修改\n\ncalicoctl replace -f minikube-m03.yaml\n# Successfully replaced 1 'Node' resource(s)\ncalicoctl replace -f minikube-m04.yaml\n# Successfully replaced 1 'Node' resource(s)\n\n\n在指定的节点打上标签\n\nkubectl label node minikube-m04 minikube-m03 route-reflector=true\n\n配置bgp对等体kind: BGPPeer\napiVersion: crd.projectcalico.org/v1\nmetadata:\n  name: node-rr\nspec:\n  nodeSelector: all()\n  peerSelector: route-reflector == 'true'\n\n\n修改bgp配置，关闭mesh模式\n\napiVersion: projectcalico.org/v3\nkind: BGPConfiguration\nmetadata:\n  name: default\nspec:\n  logSeverityScreen: Info\n  nodeToNodeMeshEnabled: false\n  asNumber: 63400\n\n查看节点bgp状态# rr模式的节点上\nroot@minikube-m03:~# calicoctl node status\nCalico process is running.\n\nIPv4 BGP status\n+--------------+---------------+-------+----------+-------------+\n| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |\n+--------------+---------------+-------+----------+-------------+\n| 192.168.49.2 | node specific | up    | 03:17:41 | Established |\n| 192.168.49.3 | node specific | up    | 03:17:41 | Established |\n| 192.168.49.5 | node specific | up    | 03:17:41 | Established |\n+--------------+---------------+-------+----------+-------------+\n# 普通节点\nroot@minikube:~# calicoctl node status \nCalico process is running.\n\nIPv4 BGP status\n+--------------+---------------+-------+----------+-------------+\n| PEER ADDRESS |   PEER TYPE   | STATE |  SINCE   |    INFO     |\n+--------------+---------------+-------+----------+-------------+\n| 192.168.49.4 | node specific | up    | 03:17:41 | Established |\n| 192.168.49.5 | node specific | up    | 03:17:43 | Established |\n+--------------+---------------+-------+----------+-------------+\n\nBGP-TOR路由Top of Rack机架上面的交换机\n\n\n这个方案中所有的节点将bgp信息宣告给tor交换机由交换机负责bgp宣告\n需要硬件交换机和路由器中整体部署bgp网络，然后宣告给这个网络\n\nIPIP&#x2F;VXLAN跨子网模式\n当跨子网时使用ipip/vxlan来进行通讯\n将ippool中的IPIPMODE或VXLANMODE修改为CrossSubnet即可\n\n环境说明kubectl get no\n# [root@10-72-164-144 ~]# kubectl get no\n# NAME            STATUS   ROLES           AGE   VERSION\n# 10-72-137-177   Ready    control-plane   14d   v1.27.1\n# 10-72-164-144   Ready    control-plane   14d   v1.27.1\n# 10-72-164-145   Ready    control-plane   14d   v1.27.1\n\nkubectl get po -o wide\n# [root@10-72-164-144 ~]# kubectl get po -o wide\n# NAME             READY   STATUS    RESTARTS   AGE   IP              NODE            NOMINATED NODE   READINESS GATES\n# net-test-5g2hg   1/1     Running   0          14d   10.244.90.196   10-72-164-145   &lt;none>           &lt;none>\n# net-test-5j2kw   1/1     Running   0          14d   10.244.77.194   10-72-164-144   &lt;none>           &lt;none>\n# net-test-dzhhs   1/1     Running   0          14d   10.244.90.132   10-72-137-177   &lt;none>           &lt;none>\n\n\n10.72.137.177为一个子网，10.72.164.144,10.72.164.145为同一个子网\n\nIPIP\n修改ipip为CrossSubnet\n\n# 关闭vxlan如果开启了\ncalicoctl patch ippool default-ipv4-ippool  -p '&#123;\"spec\":&#123;\"vxlanMode\": \"Never\"&#125;&#125;'\ncalicoctl patch ippool default-ipv4-ippool  -p '&#123;\"spec\":&#123;\"ipipMode\": \"CrossSubnet\"&#125;&#125;'\n\n# 确认修改\ncalicoctl get ippool default-ipv4-ippool -o wide\n# [root@10-72-164-144 ~]# calicoctl get ippool default-ipv4-ippool -o wide\n# NAME                  CIDR            NAT    IPIPMODE      VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR\n# default-ipv4-ippool   10.244.0.0/16   true   CrossSubnet   Never       false      false              all()\n\n\n查看路由\n\nip r\n# [root@10-72-164-144 ~]# ip r\n# default via 10.72.164.1 dev eth0\n# 10.72.164.0/24 dev eth0 proto kernel scope link src 10.72.164.144\n# blackhole 10.244.77.192/26 proto bird\n# 10.244.77.194 dev cali97faa6acd6c scope link\n# 10.244.77.195 dev cali8192f92cc2f scope link\n# 10.244.90.128/26 via 10.72.137.177 dev tunl0 proto bird onlink 跨子网使用了tunlo这个网卡\n# 10.244.90.192/26 via 10.72.164.145 dev eth0 proto bird\n\n\n确认是ipip网卡\n\nip addr show  tunl0\n# [root@10-72-164-144 ~]# ip addr show  tunl0\n# 9: tunl0@NONE: &lt;NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN qlen 1000\n#     link/ipip 0.0.0.0 brd 0.0.0.0\n#     inet 10.244.77.199/32 scope global tunl0\n\n\n\n通过路由可以看到ipip在CrossSubnet下同子网直接通过bgp(bird)获取的路由发送出去,跨子网则使用了ipip隧道\n\nVXLAN\n修改VXLANMODE为CrossSubnet\n\ncalicoctl patch ippool default-ipv4-ippool  -p '&#123;\"spec\":&#123;\"ipipMode\": \"Never\"&#125;&#125;'\ncalicoctl patch ippool default-ipv4-ippool  -p '&#123;\"spec\":&#123;\"vxlanMode\": \"CrossSubnet\"&#125;&#125;'\n\n# 确认下\ncalicoctl get ippool default-ipv4-ippool -o wide\n# [root@10-72-164-144 ~]# calicoctl get ippool default-ipv4-ippool -o wide\n# NAME                  CIDR            NAT    IPIPMODE   VXLANMODE     DISABLED   DISABLEBGPEXPORT   SELECTOR\n# default-ipv4-ippool   10.244.0.0/16   true   Never      CrossSubnet   false      false              all()\n\n\n查看路由\n\nip r\n# [root@10-72-164-144 ~]# ip r\n# default via 10.72.164.1 dev eth0\n# 10.72.164.0/24 dev eth0 proto kernel scope link src 10.72.164.144\n# 10.244.0.192/26 via 10.72.164.145 dev eth0 proto 80 onlink\n# blackhole 10.244.77.192/26 proto 80\n# 10.244.77.194 dev cali97faa6acd6c scope link\n# 10.244.77.195 dev cali8192f92cc2f scope link\n# 10.244.90.128/26 via 10.244.90.135 dev vxlan.calico onlink 跨子网使用了vxlan\n# 10.244.90.192/26 via 10.72.164.145 dev eth0 proto 80 onlink 同子网其他节点\n\n\n查看vxlan网卡\n\nip addr show vxlan.calico\n# [root@10-72-164-144 ~]# ip addr show vxlan.calico\n# 43: vxlan.calico: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN\n#     link/ether 66:80:c0:e8:b1:d7 brd ff:ff:ff:ff:ff:ff\n#     inet 10.244.77.200/32 scope global vxlan.calico\n#        valid_lft forever preferred_lft forever\n#     inet6 fe80::6480:c0ff:fee8:b1d7/64 scope link\n#        valid_lft forever preferred_lft forever\n\n\n\n结论通过路由可以看到vxlan在CrossSubnet下同子网直接通过eth0发送出去,跨子网则使用了vxlan\n\n跨子网bgp full mesh模式下路由\n查看路由\n\n# [root@10-72-164-144 ~]# calicoctl get ippool default-ipv4-ippool -o wide\n# NAME                  CIDR            NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR\n# default-ipv4-ippool   10.244.0.0/16   true   Never      Never       false      false              all()\n\nip r\n# [root@10-72-164-144 ~]# ip r\n# default via 10.72.164.1 dev eth0\n# 10.72.164.0/24 dev eth0 proto kernel scope link src 10.72.164.144\n# blackhole 10.244.77.192/26 proto bird\n# 10.244.77.194 dev cali97faa6acd6c scope link\n# 10.244.77.195 dev cali8192f92cc2f scope link\n# 10.244.90.128/26 via 10.72.164.1 dev eth0 proto bird\n# 10.244.90.192/26 via 10.72.164.145 dev eth0 proto bird\n\n\n可以看到在跨子网时,路由指向了当前节点的网关，但我们的网关却不知道10.244.90.128&#x2F;26路由到哪里，所以10.244.90.128&#x2F;26的地址不通\n\nk get po -o wide\n# [root@10-72-164-144 ~]# k get po -o wide\n# NAME             READY   STATUS    RESTARTS   AGE   IP              NODE            NOMINATED NODE   READINESS GATES\n# net-test-5g2hg   1/1     Running   0          14d   10.244.90.196   10-72-164-145   &lt;none>           &lt;none>\n# net-test-5j2kw   1/1     Running   0          14d   10.244.77.194   10-72-164-144   &lt;none>           &lt;none>\n# net-test-dzhhs   1/1     Running   0          14d   10.244.90.132   10-72-137-177   &lt;none>           &lt;none>\nk exec -it net-test-5g2hg -- ping 10.244.90.132\n# [root@10-72-164-144 ~]# k exec -it net-test-5g2hg -- ping 10.244.90.132\n# PING 10.244.90.132 (10.244.90.132): 56 data bytes\n\n\n\n符合预期\n\n将ipip和vxlan全部设置为CrossSubnet\n不行\n\n[root@10-72-164-144 ~]# calicoctl patch ippool default-ipv4-ippool  -p '&#123;\"spec\":&#123;\"vxlanMode\": \"CrossSubnet\"&#125;&#125;'\nSuccessfully patched 1 'IPPool' resource\n[root@10-72-164-144 ~]# calicoctl patch ippool default-ipv4-ippool  -p '&#123;\"spec\":&#123;\"ipipMode\": \"CrossSubnet\"&#125;&#125;'\nHit error: updating existing resource: error with field IPPool.Spec.VXLANMode = 'CrossSubnet' (Cannot enable both VXLAN and IPIP on the same IPPool)\n\nEBPF\nepbf是一个内核虚拟机\n\nEBPF部署环境验证uname -rv\n# docker@minikube:~$ uname -rv\n# 5.15.49-linuxkit #1 SMP PREEMPT Tue Sep 13 07:51:32 UTC 2022\n\n\nebpf对内核版本要求比较高，内核版本最高5.0往上，红帽4.8往上也行\n\n修改api-server地址\ncalico默认使用的是kube-proxy提供的api-server的svc地址需要改为api-server负载均衡的地址\n可以通过kubelet的配置文件来查看，\n创建配置文件\n\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kubernetes-services-endpoint\n  namespace: kube-system\ndata:\n  KUBERNETES_SERVICE_HOST: \"192.168.49.2\" # &lt;API server host>\n  KUBERNETES_SERVICE_PORT: \"8443\"         # &lt;API server port>\n\n\n重启calico组件\n\nkubectl -n kube-system rollout restart deployment calico-kube-controllers\nkubectl -n kube-system rollout restart ds calico-node\n\n开启ebpf\n配置kube-proxy\n\nkubectl patch ds -n kube-system kube-proxy -p '&#123;\"spec\":&#123;\"template\":&#123;\"spec\":&#123;\"nodeSelector\":&#123;\"non-calico\": \"true\"&#125;&#125;&#125;&#125;&#125;'\n\n\n开启ebpf\n\ncalicoctl patch felixconfiguration default --patch='&#123;\"spec\": &#123;\"bpfEnabled\": true&#125;&#125;'\n# Successfully patched 1 'FelixConfiguration' resource\n\n开启dsr\ndsr可以保留客户端ip\n\n\n主要是修改BPFExternalServiceMode这个变量\n\n开启dsr\n\n\ncalicoctl patch felixconfiguration default --patch='&#123;\"spec\": &#123;\"bpfExternalServiceMode\": \"DSR\"&#125;&#125;'\n\n\n回滚\n\ncalicoctl patch felixconfiguration default --patch='&#123;\"spec\": &#123;\"bpfExternalServiceMode\": \"Tunnel\"&#125;&#125;'\n\nIP地址管理静态ip\n一般来说pod不需要pod的ip是静态的，而是已通过service来访问，但是在安全等领域可能需要pod的ip是静态或者一小段范围\n\n使用ipam里面的ip\n\n\nannotations:\n  'cni.projectcalico.org/ipAddrs': '[\"192.168.0.1\"]'\n\n不使用ipam里面的ip\n此功能需要cni开启特性才行\n\nkubectl -n kube-system edit cm calico-config\n\n&#123;\n  \"name\": \"any_name\",\n  \"cniVersion\": \"0.1.0\",\n  \"type\": \"calico\",\n  \"ipam\": &#123;\n    \"type\": \"calico-ipam\"\n  &#125;,\n  \"feature_control\": &#123;\n    \"ip_addrs_no_ipam\": true\n  &#125;\n&#125;\n\n\n然后重启calico的agent\n\nkubectl -n kube-system rollout restart ds calico-node\n\n\n在pod上添加下面的注释\n\nannotations:\n  'cni.projectcalico.org/ipAddrsNoIpam': '[\"10.0.0.1\"]'\n\n\n此时pod的ip则会设置为10.0.0.1,但是这个ip只能在你在pod所在的node上ping通过，路由等需要自己手动处理\n\nFloating IP(浮动ip)\nFloating IP（浮动IP）是一种IP地址分配技术，它能够将一个IP地址从一台主机转移到另一台主机。 浮动IP通常用于云计算环境中，因为在这种环境下，虚拟资源（如虚拟机）可能随时在不同的物理主机之间移动。\n\n只能在BGP模式下使用\n\n\n\n也需要打开特性和上面noipam的类似方法不做赘述\n\n&#123;\n  \"name\": \"any_name\",\n  \"cniVersion\": \"0.1.0\",\n  \"type\": \"calico\",\n  \"ipam\": &#123;\n    \"type\": \"calico-ipam\"\n  &#125;,\n  \"feature_control\": &#123;\n    \"floating_ips\": true\n  &#125;\n&#125;\n\nannotations:\n  'cni.projectcalico.org/floatingIPs': '[\"10.0.0.1\"]'\n\nIP预留\n顾名思义保留的ip不会分配给pod\n\napiVersion: crd.projectcalico.org/v1\nkind: IPReservation\nmetadata:\n  name: my-ipreservation-1\nspec:\n  reservedCIDRs:\n    - 192.168.2.3\n    - 10.0.2.3/32\n    - cafe:f00d::/123\n\n优先级pod的注释&gt;pod所在的ns的注释&gt;ippool\n带宽限制\ncni配置文件需要设置如下参数\n\n&#123;\n  \"type\": \"bandwidth\",\n  \"capabilities\": &#123; \"bandwidth\": true &#125;\n&#125;\n\n\n在pod上配置带宽\n\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    kubernetes.io/ingress-bandwidth: 1M # 进入\n    kubernetes.io/egress-bandwidth: 1M  # 出口\n\n\n实际是调用了bandwidth这个cni插件\n\n指定MAC地址annotations:\n  \"cni.projectcalico.org/hwAddr\": \"1c:0c:0a:c0:ff:ee\"\n\n\n重启pod生效\n\n开启ipv6支持\nipv6需要k8s同步开启双栈\n\n修改cni配置文件kubectl -n kube-system edit cm calico-config\n\n\"ipam\": &#123;\n    \"type\": \"calico-ipam\",\n    \"assign_ipv4\": \"true\",\n    \"assign_ipv6\": \"true\"\n&#125;,\n\n修改DS环境变量\n\n\n环境变量\n值\n\n\n\nIP6\nautodetect\n\n\nFELIX_IPV6SUPPORT\ntrue\n\n\n# 修改环境变量\nkubectl -n kube-system set env ds/calico-node -c calico-node IP6=autodetect\nkubectl -n kube-system set env ds/calico-node -c calico-node FELIX_IPV6SUPPORT=true\n\n参考资料https://docs.tigera.io/calico/latest/about\n","tags":["k8s","cni","网络"]},{"title":"在k8s集群中部署tidb","url":"/2023/11/09/%E5%9C%A8k8s%E9%9B%86%E7%BE%A4%E4%B8%AD%E9%83%A8%E7%BD%B2tidb/","content":"tidb是pingcap开发的开源数据库\n\n\n其底层使用rust上层使用go开发，兼容mysql协议\n\n部署\n部署crd\n\nkubectl create -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/crd.yaml\n\n\n部署operator\n\nhelm repo add pingcap https://charts.pingcap.org/\n\nkubectl create namespace tidb-admin\n\nhelm install --namespace tidb-admin tidb-operator pingcap/tidb-operator --version v1.5.1\n\n\n部署一个官方示例的集群\n\nkubectl create namespace tidb-cluster &amp;&amp; \\\n    kubectl -n tidb-cluster apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/basic/tidb-cluster.yaml\n\n\n查看集群\n\nk get tc\n# NAME    READY   PD                  STORAGE   READY   DESIRE   TIKV                  STORAGE   READY   DESIRE   TIDB                  READY   DESIRE   AGE\n# basic   True    pingcap/pd:v7.1.1   1Gi       1       1        pingcap/tikv:v7.1.1   1Gi       1       1        pingcap/tidb:v7.1.1   1       1        45m\n\n\n部署dashboard\n\nkubectl -n tidb-cluster apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/basic/tidb-dashboard.yaml\n\n\n部署监控\n\nkubectl -n tidb-cluster apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/examples/basic/tidb-monitor.yaml\n\n连接数据库\n转发svc到本地，使用使用直接写对应的svc的地址和端口就好\n\nkubectl port-forward -n tidb-cluster svc/basic-tidb 14000:4000 > pf14000.out &amp;\n\n\nmysql连接\n\nmysql --comments -h 127.0.0.1 -P 14000 -u root\n# Welcome to the MariaDB monitor.  Commands end with ; or \\g.\n# Your MySQL connection id is 677\n# Server version: 5.7.25-TiDB-v7.1.1 TiDB Server (Apache License 2.0) Community Edition, MySQL 5.7 compatible\n# \n# Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n# \n# Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n# \n# MySQL [(none)]>\n\n\n创建一个表\n\nuse test;\ncreate table hello_world (id int unsigned not null auto_increment primary key, v varchar(32));\n\n\n查询版本号\n\nselect tidb_version()\\G\n\n\n查询存储状态\n\nselect * from information_schema.tikv_store_status\\G\n\n\n查询集群状态\n\nselect * from information_schema.cluster_info\\G\n\n\n查看监控的grafana，转发下grafana的svc然后打开浏览默认密码admin&#x2F;admin,也可以将svc改成nodeport模式\n\n# 转发\nkubectl port-forward -n tidb-cluster svc/basic-grafana 3000 > pf12333.out &amp;\n\n\ntidb的ui,默认没密码\n\nkubectl port-forward -n tidb-cluster svc/basic-tidb-dashboard-exposed 12333 > pf12333.out &amp;\n\n升级集群kubectl patch tc basic -n tidb-cluster --type merge -p '&#123;\"spec\": &#123;\"version\": \"&lt;版本号>\"&#125; &#125;'\n\n扩缩容kubectl patch -n $&#123;namespace&#125; tc $&#123;cluster_name&#125; --type merge --patch '&#123;\"spec\":&#123;\"pd\":&#123;\"replicas\":&lt;副本数>&#125;&#125;&#125;'\n\n参考资料https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/get-started\n","tags":["k8s","tikv","数据库"]},{"title":"将时间机器备份到smb上","url":"/2022/06/03/%E5%B0%86%E6%97%B6%E9%97%B4%E6%9C%BA%E5%99%A8%E5%A4%87%E4%BB%BD%E5%88%B0smb%E4%B8%8A/","content":"macos的时间机器备份很方便\n\n\n\n如果只是备份到优盘或者固态硬盘等设备上有的时候不够灵活，通过网络直接备份到smb文件服务器上\n\n创建smb共享文件夹需要给读写权限\n创建磁盘映像文件打开磁盘工具文件–&gt;创建空白映像\n这里注意格式大小和容量\n\n将创建好的文件拷贝到共享文件夹里\n挂载时间机器1.双击共享文件夹里的创建的磁盘挂载2.命令行查看挂载路径df -h\n\n3.设置时间机器备份磁盘sudo tmutil setdestination &lt;挂载路径>\n\n4.这个时候打开时间机器就可以看到我们创建的磁盘了","tags":["备份"]},{"title":"本地大模型运行平台-ollama","url":"/2024/06/14/%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%BF%90%E8%A1%8C%E5%B9%B3%E5%8F%B0-ollama/","content":"ollama是一个用go写的本地大模型运行框架,支持多种大大模型，支持多平台\n\n\n安装\ndocker\n\ndocker pull ollama/ollama\n\n\n脚本安装\n\ncurl -fsSL https://ollama.com/install.sh | sh\n\n基本使用\n这就像docker一样,运行一个llama3大模型，启动完成之后会弹出一个对话框，就可以像chartGPT那样对话了\nollama run类似docker run如果本地没有模型则下载模型并运行\n\nollama run llama3\n# pulling manifest\n# pulling 6a0746a1ec1a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB\n# pulling 4fa551d4f938... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  12 KB\n# pulling 8ab4849b038c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  254 B\n# pulling 577073ffcc6c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  110 B\n# pulling 3f8eb4da87fa... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  485 B\n# verifying sha256 digest\n# writing manifest\n# removing any unused layers\n# success\n# >>> Send a message (/? for help)\n\n\n除了可以使用对话框以外还可以使命令行\n\nollama run llama3:latest \"地球为什么是圆的\"\n\n\n下载模型\n\nollama pull &lt;模型>\n\n\n查看模型\n\nollama list\n# NAME            ID              SIZE    MODIFIED\n# llama3:latest   365c0bd3c000    4.7 GB  10 minutes ago\n\n\n查看正在运行的模型\n\nollama ps\n# NAME            ID              SIZE    PROCESSOR       UNTIL\n# llama3:latest   365c0bd3c000    4.9 GB  100% CPU        About a minute from now\n\n\n删除模型\n\nollama rm &lt;模型>\n\n\nhttps://ollama.com/library可以查看ollama的模型仓库，这个和docker hub类似\n\nmodelfile\nmodelfile类似dockerfile可以在基础模型之上添加一些修改\n\n\n添加参数\n\n编写modelfile，modelfile相关文档\n\n\nFROM qwen2:7b\n\nPARAMETER temperature 1\n\n# 设置系统提示词\nSYSTEM \"\"\"\n我是玉皇大帝\n\"\"\"\n\n\n创建新的model\n\nollama create test2 -f ./Modelfile\n# transferring model data\n# using existing layer sha256:43f7a214e5329f672bb05404cfba1913cbb70fdaa1a17497224e1925046b0ed5\n# using existing layer sha256:62fbfd9ed093d6e5ac83190c86eec5369317919f4b149598d2dbb38900e9faef\n# using existing layer sha256:c156170b718ec29139d3653d40ed1986fd92fb7e0959b5c71f3c48f62e6636f4\n# creating new layer sha256:51b98d7e95aee52247df320b4c76c526d614ce6df6ed4bd20e27d49f06ff4695\n# using existing layer sha256:b1c932e03beb32c4ab61bb50b2fa06ab1f2ea4e99ee6495670bbe23834dc7d62\n# creating new layer sha256:6e63a33de34e7673b5e8e9ea0ef5e00020ca427e5a3e017009028a45ef2c2ccd\n# writing manifest\n# success\n\n\n使用新创建的model\n\nollama run test2\n# >>> 我是谁\n# 您提及的身份是“我”，在哲学和心理学中，这通常指的是个体意识中的主体性。在不同的语境或情境下，“我”可以指代具体的人、事物或者抽象的概念。如果您是在询问您的个人身份，比如名字、角色等信息，请提供更多的上下文或背景，以便我能更准确地回答您的\n# 问题。\n# \n# 若您是指“玉皇大帝”，那么在古代中国的神话体系中，玉皇大帝是宇宙间最高的神祇，掌管着一切的最高主宰。他是天界之王，代表了至高的权威和智慧。如果您自称为“玉皇大帝”，则是在扮演或想象自己为这一神祇的角色。\n# \n# 无论您是在探讨哲学问题还是讨论神话角色，您的身份都是独一无二且具有特定内涵的存在。\n# \n# >>>\n\n图形化界面\n上面这是命令行的对话，部署一个界面则可以像chartGPT一样\n\n\n使用open-webui,一件安装open-webui和ollama\n\ndocker run -d -p 3000:8080 --gpus=all -v ./ollama:/root/.ollama -v ./open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n\n\n访问你的ip:3000,注册个账号，第一个是管理员\n\n\n\n设置模型\n\n\n\n自定义文档\n部署的时候dokcer映射了一个open-webui目录，将文档放在此目录下的docs目录\n\n\n\n在设置中设置\n\n\n\n聊天框中通过#来选择文档(@可以选择模型)\n\n\n参考资料https://github.com/ollama/ollamahttps://docs.openwebui.com/https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n","tags":["ai"]},{"title":"测试工具","url":"/2022/10/11/%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/","content":"整理一些linux上常用测试工具\n\n\n硬盘测试fiofio安装yum install -y fio\n\nfio参数说明\n\n\n参数\n说明\n\n\n\ndirect&#x3D;1\n忽略缓存，直接写入磁盘\n\n\niodepth&#x3D;128\n请求的IO队列深度\n\n\nrw&#x3D;write\n读写策略，可选值randread(随机读)，randwrite(随机写)，read(顺序读)，write(顺序写)，randrw(混合随机读写)\n\n\nioengine&#x3D;libaio\nIO引擎配置，建议使用libaio\n\n\nbs&#x3D;4k\n块大小配置，可以使用4k，8k，16k等\n\n\nsize&#x3D;200G\n测试生成文件的大小\n\n\nnumjobs&#x3D;1\n线程数配置\n\n\nruntime&#x3D;1000\n测试运行时长，单位秒\n\n\ngroup_reporting\n测试结果汇总展示\n\n\nname&#x3D;test\n测试任务名称\n\n\nfilename&#x3D;&#x2F;data&#x2F;test\n测试输出的路径与文件名\n\n\n测试使用\n读时延\n\nfio -direct=1 -iodepth=1 -rw=read -ioengine=libaio -bs=4k -size=200G -numjobs=1 -runtime=1000 -group_reporting -name=test -filename=/data/test\n\n\n写时延\n\nfio -direct=1 -iodepth=1 -rw=write -ioengine=libaio -bs=4k -size=200G -numjobs=1 -runtime=1000 -group_reporting -name=test -filename=/data/test\n\n\n读带宽\n\nfio -direct=1 -iodepth=32 -rw=read -ioengine=libaio -bs=256k -size=200G -numjobs=4 -runtime=1000 -group_reporting -name=test -filename=/data/test  \n\n\n写带宽\n\nfio -direct=1 -iodepth=32 -rw=write -ioengine=libaio -bs=256k -size=200G -numjobs=4 -runtime=1000 -group_reporting -name=test -filename=/data/test\n\n\n读IOPS\n\nfio -direct=1 -iodepth=32 -rw=randread  -ioengine=libaio -bs=4k -size=200G -numjobs=4 -runtime=1000 -group_reporting -name=test -filename=/data/test\n\n\n写IOPS\n\nfio -direct=1 -iodepth=32 -rw=randwrite -ioengine=libaio -bs=4k -size=200G -numjobs=4 -runtime=1000 -group_reporting -name=test -filename=/data/test\n\n网络iperf3安装yum -y install iperf3\n\n下载地址:https://iperf.fr/iperf-download.php\niperf3参数说明\n\n\n参数\n说明\n\n\n\n-c\n客户端模式\n\n\n-s\n服务端模式\n\n\n-p\n指定端口号\n\n\n-d\n打印调试信息\n\n\n-P\n多线程\n\n\n-u\n使用udb\n\n\n-t\n测试时间\n\n\n-b\n带宽\n\n\n-R\n双向测试\n\n\n使用\n基本使用\n\n# 服务端\niperf3 -s\n# 客户端\niperf3 -c 10.23.219.49\n\n\n在tcp在100m带宽3个线程下双向测试10秒\n\niperf3 -c 10.23.219.49 -i -d -P 3 -R -t 10 -b 100M\n\n\ntcp测试\n\niperf3 -c 10.23.219.49 -R -P 10\n\n\nudp测试\n\n侧重于看丢包\niperf3 -c 10.23.219.49 -u -b 10M\n\nethr微软出的一个综合网络测试工具https://github.com/Microsoft/Ethr\n参数\n\n\n参数\n说明\n\n\n\n-m\nx 表示Ethr作为外部客户端模式与共它服务端对接\n\n\n-c\n表示作为客户端模式连接服务端与iperf类似\n\n\n-t\n表示测试类型 如TCP新建:c、TCP带宽:b、TCP延迟:l\n\n\n-n\n表示并发会话&#x2F;线程数据0表示采用当前CPU数量\n\n\n-d\n表示测试时长默认10s、0表示不限时长测试\n\n\nerhr使用# 服务端\nethr -s -ui -port 9999\n# 客户端\n\nethr -c 192.168.1.1\n\n-ui 开启一个文本界面\n\ntcp带宽测试\n\nethr -c 10.23.219.49 -t b -p tcp -n 0 -d 0\n\n\ntcp新建测试\n\nethr -c 10.23.219.49 -t c -p tcp -n 0 -d 0\n\n\ntcp 延迟测试\n\nethr -c 10.23.219.49 -t l -p tcp -n 0 -d 0\n\n\nudp pps测试\n\nethr -c 10.23.219.49 -t p -p udp -n 0 -d 0\n\nweb服务测试ab(apachebench)ab是apachebench的缩写最开始用来测试apache httpd的也可以用来测试其他web服务器\nab安装yum -y install httpd-tools\n\n\n测试\n\nab -c 10 -n 10 http://www.baidu.com/\n\n\n-c 次数\n\n-n 并发数\n\n\n综合测试sysbenchsysbench是个综合测试工具包括cpu硬盘数据等\n安装sysbenchyum install -y sysbench\n\n使用sysbench\n测试cpu\n\nsysbench --test=cpu --cpu-max-prime=20000 run\n\n\n测试内存\n\nsysbench --test=memory run\n\n\n测试io\n\nsysbench --test=fileio --file-test-mode=seqwr run\n\n\n测试mysql\n\n# 准备数据\nsysbench /usr/share/sysbench/oltp_read_only.lua --mysql-host=localhost --mysql-port=3306 --mysql-user=root --mysql-db=test --db-driver=mysql --tables=10 --table-size=1000 --time=10 prepare\n\n# 执行测试\nsysbench /usr/share/sysbench/oltp_read_only.lua --mysql-host=localhost --mysql-port=3306 --mysql-user=root --mysql-db=test --db-driver=mysql --tables=10 --table-size=1000 --time=10 run\n\n# 清理\nsysbench /usr/share/sysbench/oltp_read_only.lua --mysql-host=localhost --mysql-port=3306 --mysql-user=root --mysql-db=test --db-driver=mysql --tables=10 --table-size=1000 --time=10 cleanup\n\n\n&#x2F;usr&#x2F;share&#x2F;sysbench&#x2F;下有很多测试脚本,根据测试需要填写，可以使用help命令查看参数,–help查看测试项的参数\n\nk8s\nkubemark\ntestkube\n\n参考资料https://mp.weixin.qq.com/s?src=11&amp;timestamp=1665469357&amp;ver=4097&amp;signature=UHpleVGTVCM4lwywMQ3B9xfQXA0H5tOi8Ju3nw9YxPQNHo9KazyAmpeLLBvXOFJh9O5OPzrsxoVkF5SCso0kIu9HlAD5EuZ9tffPaTfbVBn0QLf8h0PURQoTCtHOxjkK&amp;new=1\nhttps://linuxhint.com/use-sysbench-for-linux-performance-testing/\n","tags":["测试"]},{"title":"用k8s资源方式创建k8s-clusterapi","url":"/2022/12/19/%E7%94%A8k8s%E8%B5%84%E6%BA%90%E6%96%B9%E5%BC%8F%E5%88%9B%E5%BB%BAk8s-clusterapi/","content":"cluster-api是k8s的一个子项目隶属于SIG Cluster Lifecycle,主要使用类似k8s风格的资源对象来管理k8s集群的生命周期\n\n\nk8s的部署比较复杂，且每个发行版本稍微有些不应，cluster api则致力于通过k8s得资源对象来创建，管理k8s集群\n安装\nclustar api的命令工具为clusterctl\n\n# m1 macos\ncurl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.3.1/clusterctl-darwin-arm64 -o clusterctl\n\n初始化管理集群服务端\ninfrastructure参数指定基础架构供应商\n\nclusterctl init --infrastructure vcluster\n\n使用clusterctl部署集群export HELM_VALUES=\"service:\\n  type: NodePort\"\n\nkubectl create namespace $&#123;CLUSTER_NAMESPACE&#125;\n\n# 生成cluster-api的cr并应用\nclusterctl generate cluster $&#123;CLUSTER_NAME&#125; \\\n    --infrastructure vcluster \\\n    --kubernetes-version $&#123;KUBERNETES_VERSION&#125; \\\n    --target-namespace $&#123;CLUSTER_NAMESPACE&#125; | kubectl apply -f -\n\n\n查看集群发现已经部署好了\n\n❯ vcluster list        \n NAME        NAMESPACE         STATUS    CONNECTED   CREATED                         AGE     CONTEXT   \n capi-test   clusterapi-test   Running               2022-12-19 15:40:46 +0800 CST   3m22s   minikube  \n\n\n查看集群详情\n\nclusterctl describe cluster  capi-test\n\n\n获取创建的集群的kube-config\n\nclusterctl get kubeconfig capi-test\n\n\n删除管理集群\n\n# 删除 供应商 创建的命名空间和crd\nclusterctl delete --infrastructure aws --include-namespace --include-crd\n\n# 删除所有\nclusterctl delete --all\n\n命令补全\nzsh\n\n# 已经有了此配置可以忽略\necho \"autoload -U compinit; compinit\" >> ~/.zshrc\n\nclusterctl completion zsh > \"$&#123;fpath[1]&#125;/_clusterctl\"\n\n参考资料https://cluster-api.sigs.k8s.io/introduction.html\n","tags":["k8s","部署"]},{"title":"记一次k8s节点网络重传排查","url":"/2024/03/05/%E8%AE%B0%E4%B8%80%E6%AC%A1k8s%E8%8A%82%E7%82%B9%E7%BD%91%E7%BB%9C%E9%87%8D%E4%BC%A0%E6%8E%92%E6%9F%A5/","content":"值班的时候有用户报障他们的一个定时job有超时，他们自己抓包发现网络有重传认为我们的k8s网络有问题\n看了下历史记录问，题是去年提出的，之前同事已经测试过长ping，且抓包未发现异常，他们发现访问的服务端和客户端的pod都在同一个节点上，怀疑用户业务自身问题\n\n\n经过年后来了之后，用户依然出现超时问题，查看用户集群有ingress询问用户是否经过ingress过到服务端，且用户反应出现此类问题的不只是这一个业务以及测试集群也有出现\n从现象来看此时我也认为是客户自身业务导致，但是客户坚持认为和业务没关系\n客户提供了一个是集群让我们测试，以及出问题的pod\n于是就在客户的测试集群中使用ab测试出问题的那些pod的网络情况，并没有发现问题,因为ab没有按时间探测的功能所有把包给加大了，测试了同node和不同node\nab -c 10 -t 1000 -n 10000000 http://&lt;ip>/\n\n在测试3天之后发现ab中偶有超时最长是1000ms+，此时怀疑可能网络有问题了但不是那么的确定，因为重传有很多原因\n同时登录上客户的机器看了cpu，内存等情况，发现客户的内存的free很少，网卡有丢包统计但是不是很多ethtool -S etho等命令查看队列等，没有发现太多相关性\n客户表示出问题已经几个月且没有啥变更，同时我们和技术支持还有部门其他同事一块开了个会议，其他同事也进行了排查也没发现问题所在，且怀疑svc导致的得到的答复并没有svc，是pod和pod直接连接\n同时我们提出试试重启pod来尝试下，我又用ab压测的同时抓包，且约了客户会议\n抓包的结果有大量的rst，改下之后rst减少所以我认为这个属于压测问题\n和客户会议得知客户的服务调用链和我们之前了解的略有出入,此时心中感觉有些希望\n整个调用链中多了个ingress，我觉得可能是ingress导致的，查看了下ingress日志也没发现一些异常，询问用户是否可以排除ingress直接访问得到了否定的结果\n\n和客户会议后的第二天客户表示问题依然出现，客户部署了监控之后我们看监控也是正常的\n此时想抓出问题的时候的ingress包，则在客户的生产环境的node上执行了一个抓包，同时查看tcp重传相关\nnohup  tcpdump -i any src host &lt;src_ip> and dst host &lt;dest_ip> -w test.pcap &amp;\n\n经过一夜抓包之后使用wireshark分析发现有大量的重传和重复确认的包，在分析包中的时候顺吧看了下系统日志,发现里面有一些oom且内存free较少，将oom和内存的情况反馈给客户\n\n同时让客户看了下服务端的资源情况也没发现异常，客户反馈oom的是logstash并问是否有关系，我回复关系不大\n此时将抓包的截图发给网络的同事让帮看看，网络的同事一看说你抓包有问题，因为在tcpdump中-i参数使用了any，机器内的包经过了2个网卡就会出现wireshark中有大量的重传和重复确认包\n恍然大悟于是改进了抓包机制，只抓pod网卡的包摒弃了-i any参数\n又抓了一夜的包。。。\n不如意外故障依旧，但这次抓包中就没有满屏的重传和重复确认包，但是依然有重传且和客户超时的时间前后脚\n\n同时将截图发给了网络的同事帮确认下，他们怀疑丢包，但是需要2端同时抓包来确认丢包\n已经连续抓包了好几次这次想彻底一点，让网络的同事从底层抓包将所有的客户端和server都进行抓包\n中途上厕所遇到了负责内核的同事，他也参加了和客户的会议，我将我这几天的情况告诉他，他说等会他看下\n他登录到机器之后和我们之前一样查了下cpu，内存等资源，最后他在看日志的时候发现oom了，他说这个可能会导致网络超时，\n我问他为啥 他说内核在执行oom的时候会放下其他工作专注杀死oom的进程，但是杀死的进程时间太长了就可能导致网络收发包出问题\n\n没错就是那个oom的logstash！！！\n于是让客户将这个logstash加大内存，客户加了内存之后k8s将其调度到其他地方\n处理了oom第二天之后询问客户没有发现超时问题，为了防止被打脸就延长了网络的抓包时间\n新的一周开始问客户是否还发生，结果没发生。\n总结\n应该及时和客户沟通获取业务的调用链\n\nwireshark分析会被-i any干扰\n\n内核oom的时候会停止网络收发包等工作导致网络超时，至于oom事件过长是因为logstash有大约几百个线程\n\n\n参考资料https://cloud.tencent.com/developer/article/1404089\n","tags":["网络","故障处理"]},{"title":"部署Rancher","url":"/2020/11/02/%E9%83%A8%E7%BD%B2Rancher/","content":"rancer是一个多k8s集群管理工具\n\n\n在k8s集群里安装\n直接部署在k8s中具备高可用 中文官方https://www.rancher.cn英文官网https://rancher.com\n\nrancher需要cert-manager# 添加helm仓库\nhelm repo add jetstack https://charts.jetstack.io\n\n# 更新仓库\nhelm repo update\n\n# 使用helm安装cert-manager\nhelm install \\\n cert-manager jetstack/cert-manager \\\n --namespace cert-manager \\\n --set installCRDs=true \\\n --create-namespace\n\n使用helm安装rancher# 添加rancher的repo仓库，这里是用latest，生产环境推荐使用stable，尝鲜使用alpha\nhelm repo add rancher-latest http://rancher-mirror.oss-cn-beijing.aliyuncs.com/server-charts/latest\n\n# 安装rancher\nhelm install rancher rancher-latest/rancher \\\n --namespace cattle-system \\\n --set hostname=&lt;域名> \\\n --create-namespace\n\n单节点安装\n作为一个单独的程序管理其他的集群\n\ndocker run -d --restart=unless-stopped \\\n  -p 80:80 -p 443:443 \\\n  -v /data/rancher:/var/lib/rancher/ \\\n  -v /data/rancher/log/auditlog:/var/log/auditlog \\\n  -e CATTLE_SYSTEM_CATALOG=bundled \\\n  -e AUDIT_LEVEL=3 \\\n rancher/rancher:stable --no-cacerts\n","tags":["k8s","rancher"]},{"title":"配置docker支持gpu","url":"/2024/06/14/%E9%85%8D%E7%BD%AEdocker%E6%94%AF%E6%8C%81gpu/","content":"现在很多ai相关的程序会跑在docker当中，默认docker是不支持gpu的，所以需要使其支持gpu\n\n\n安装驱动\n打开nvidia官网的驱动下载界面根据显卡和操作系统的类型下载对应的驱动\n\n安装驱动,可能会报错缺少一些依赖，把缺少的安装即可\n\ncentos7可能会遇到类似找不到kernel-source类似的报错需要手动添加--kernel-source-path /usr/src/kernels/3.10.0-1160.119.1.el7.x86_64/来配置内核源码路径\n\n\n# 安装需要gcc支持\nyum install gcc\nyum install kernel-devel\n\nbash NVIDIA-Linux-x86_64-440.95.01.run -a -s -Z -X \n\n\n验证安装\n\nnvidia-smi\n# Fri Jun 14 16:55:37 2024\n# +-----------------------------------------------------------------------------+\n# | NVIDIA-SMI 440.95.01    Driver Version: 440.95.01    CUDA Version: 10.2     |\n# |-------------------------------+----------------------+----------------------+\n# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n# |===============================+======================+======================|\n# |   0  Tesla V100-PCIE...  Off  | 00000000:00:03.0 Off |                    0 |\n# | N/A   32C    P0    36W / 250W |      0MiB / 16160MiB |      0%      Default |\n# +-------------------------------+----------------------+----------------------+\n# \n# +-----------------------------------------------------------------------------+\n# | Processes:                                                       GPU Memory |\n# |  GPU       PID   Type   Process name                             Usage      |\n# |=============================================================================|\n# |  No running processes found                                                 |\n# +-----------------------------------------------------------------------------+\n\n安装docker\n使用官方一件脚本安装\n\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n\n\n启动\n\nsystemctl start  docker\nsystemctl enable docker\n\n\n验证安装\n\ndocker ps\n# CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n\n安装nvidia-toolkit\n配置yaml仓库\n\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \\\n  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\n\nyum-config-manager --enable nvidia-container-toolkit-experimental\n\n\n安装toolkit\n\nyum install -y nvidia-container-toolkit\n\n\n配置\n\nnvidia-ctk runtime configure --runtime=docker\nsystemctl restart docker\n\n\n测试,出现下面则表示安装成功\n\ndocker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n# Fri Jun 14 09:24:04 2024\n# +-----------------------------------------------------------------------------+\n# | NVIDIA-SMI 440.95.01    Driver Version: 440.95.01    CUDA Version: 10.2     |\n# |-------------------------------+----------------------+----------------------+\n# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n# |===============================+======================+======================|\n# |   0  Tesla V100-PCIE...  Off  | 00000000:00:03.0 Off |                    0 |\n# | N/A   36C    P0    35W / 250W |      0MiB / 16160MiB |      0%      Default |\n# +-------------------------------+----------------------+----------------------+\n# \n# +-----------------------------------------------------------------------------+\n# | Processes:                                                       GPU Memory |\n# |  GPU       PID   Type   Process name                             Usage      |\n# |=============================================================================|\n# |  No running processes found                                                 |\n# +-----------------------------------------------------------------------------+\n\nk8s containerd配置sudo nvidia-ctk runtime configure --runtime=containerd\nsudo systemctl restart containerd\n\n参考资料https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html\n","tags":["gpu"]},{"title":"404","url":"//404.html","content":"██╗  ██╗ ██████╗ ██╗  ██╗    ███╗   ██╗ ██████╗ ████████╗\n██║  ██║██╔═████╗██║  ██║    ████╗  ██║██╔═══██╗╚══██╔══╝\n███████║██║██╔██║███████║    ██╔██╗ ██║██║   ██║   ██║\n╚════██║████╔╝██║╚════██║    ██║╚██╗██║██║   ██║   ██║\n     ██║╚██████╔╝     ██║    ██║ ╚████║╚██████╔╝   ██║\n     ╚═╝ ╚═════╝      ╚═╝    ╚═╝  ╚═══╝ ╚═════╝    ╚═╝\n\n    ███████╗ ██████╗ ██╗   ██╗███╗   ██╗██████╗\n    ██╔════╝██╔═══██╗██║   ██║████╗  ██║██╔══██╗\n    █████╗  ██║   ██║██║   ██║██╔██╗ ██║██║  ██║\n    ██╔══╝  ██║   ██║██║   ██║██║╚██╗██║██║  ██║\n    ██║     ╚██████╔╝╚██████╔╝██║ ╚████║██████╔╝\n    ╚═╝      ╚═════╝  ╚═════╝ ╚═╝  ╚═══╝╚═════╝\n"},{"title":"about","url":"/about/index.html","content":"随着年纪的增长和技术解除的越来越多有些东西记不太住遂用此博客记录解决问题的过程和方法\n"},{"title":"tags","url":"/tags/index.html","content":""}]